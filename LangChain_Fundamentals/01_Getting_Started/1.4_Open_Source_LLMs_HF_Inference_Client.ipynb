{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d779e870",
      "metadata": {},
      "source": [
        "## Using LLMs via Hugging Face Inference Client\n",
        "\n",
        "### What is the Hugging Face Inference Client?\n",
        "\n",
        "The **Hugging Face Inference Client** is a powerful Python library that allows you to interact with Large Language Models (LLMs) hosted on Hugging Face's servers ‚Äî **without needing to download or run the models locally**.\n",
        "\n",
        "### Why Use It?\n",
        "\n",
        "| Advantage | Description |\n",
        "|-----------|-------------|\n",
        "| **Free Tier Available** | HuggingFace offers a [free inference API](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client) with basic rate limits |\n",
        "| **No Infrastructure Needed** | Access 150,000+ models without GPU/hardware requirements |\n",
        "| **Easy to Use** | Simple Python API similar to OpenAI's client |\n",
        "| **Wide Model Selection** | Access to latest open-source models like Llama, Mistral, etc. |\n",
        "\n",
        "### Prerequisites\n",
        "- A Hugging Face account (free)\n",
        "- A Hugging Face API token (get it from [Settings > Access Tokens](https://huggingface.co/settings/tokens))\n",
        "- `huggingface_hub` library installed (`pip install huggingface_hub`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d5abacc3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface_hub version: 0.36.0\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 1: Import the Hugging Face Hub Library\n",
        "# =============================================================================\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "# Print the version to ensure compatibility\n",
        "# IMPORTANT: Version should be >= 0.36.0 for Inference Providers to work properly\n",
        "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
        "\n",
        "# Import the InferenceClient class\n",
        "# This is the main class we'll use to interact with HuggingFace's hosted models\n",
        "from huggingface_hub import InferenceClient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897019eb",
      "metadata": {},
      "source": [
        "### Step 2: Setting Up Authentication\n",
        "\n",
        "To use the Inference API, you need to authenticate with your Hugging Face API token. We'll load it securely from environment variables using the `python-dotenv` library.\n",
        "\n",
        "> üîê **Security Best Practice**: Never hardcode your API tokens directly in code. Always use environment variables or secret management tools.\n",
        "\n",
        "üìö **Documentation**: Feel free to refer to the [official InferenceClient documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) for more details on available methods and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d2a05cf4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ HuggingFace API token loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 2: Load API Token from Environment Variables\n",
        "# =============================================================================\n",
        "\n",
        "from dotenv import load_dotenv  # Library to load variables from .env file\n",
        "import os  # Standard library for OS operations\n",
        "\n",
        "# Load environment variables from a .env file in the project root\n",
        "# Your .env file should contain: HF_TOKEN=your_huggingface_api_token_here\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the Hugging Face API token from environment variables\n",
        "# This token authenticates your requests to the HuggingFace Inference API\n",
        "hf_key = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "# Optional: Verify the token was loaded (don't print the actual token!)\n",
        "if hf_key:\n",
        "    print(\"‚úÖ HuggingFace API token loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Warning: HF_TOKEN not found. Please check your .env file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd6aa82",
      "metadata": {},
      "source": [
        "### Step 3: Making Your First API Call\n",
        "\n",
        "Now let's use the `InferenceClient` to interact with a Large Language Model. We'll use **Meta's Llama 3.1 8B Instruct** model, which is:\n",
        "- An open-source model available for free\n",
        "- Instruction-tuned (optimized to follow instructions)\n",
        "- 8 billion parameters (good balance between quality and speed)\n",
        "\n",
        "#### Key Concepts:\n",
        "- **Chat Completion**: A conversation-style API where you send messages with roles (user, assistant, system)\n",
        "- **Messages Format**: A list of dictionaries with `role` and `content` keys\n",
        "- **max_tokens**: Controls the maximum length of the generated response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7901175c",
      "metadata": {},
      "outputs": [
        {
          "ename": "HfHubHTTPError",
          "evalue": "401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/HuggingFaceTB/SmolLM3-3B/v1/chat/completions (Request ID: Root=1-697c6e30-1b7acd773d8a06a474da36a7;de3b34e0-0feb-4fad-95f1-3ee1e5a0ebf8)\n\nInvalid username or password.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/HuggingFaceTB/SmolLM3-3B/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     24\u001b[39m chat = [\n\u001b[32m     25\u001b[39m     {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mExplain what is Generative AI in 2 bullet points\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m     },\n\u001b[32m     29\u001b[39m ]\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Make the API call using chat_completion()\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Parameters:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#   - messages: The conversation history (our 'chat' list)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#   - model: Which model to use for generation\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#   - max_tokens: Maximum number of tokens in the response (1 token ‚âà 4 characters)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Print the full response object to see its structure\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull API Response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:915\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n\u001b[32m    908\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    909\u001b[39m     inputs=messages,\n\u001b[32m    910\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    914\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:275\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:475\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/HuggingFaceTB/SmolLM3-3B/v1/chat/completions (Request ID: Root=1-697c6e30-1b7acd773d8a06a474da36a7;de3b34e0-0feb-4fad-95f1-3ee1e5a0ebf8)\n\nInvalid username or password."
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 3: Create the Inference Client and Make a Chat Completion Request\n",
        "# =============================================================================\n",
        "\n",
        "# Define the model to use\n",
        "# Format: \"organization/model-name\"\n",
        "# Note: Only models with \"warm\" inference status work with the free API\n",
        "# You can find available models at: https://huggingface.co/models?inference=warm\n",
        "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "\n",
        "# Initialize the InferenceClient with your API token\n",
        "# This client handles all communication with HuggingFace's servers\n",
        "client = InferenceClient(token=hf_key)\n",
        "\n",
        "# Define the conversation as a list of messages\n",
        "# Each message has:\n",
        "#   - \"role\": Who is speaking (\"system\", \"user\", or \"assistant\")\n",
        "#   - \"content\": The actual message text\n",
        "# \n",
        "# Common roles:\n",
        "#   - \"system\": Sets the behavior/personality of the AI (optional)\n",
        "#   - \"user\": Messages from the human user\n",
        "#   - \"assistant\": Previous responses from the AI (for multi-turn conversations)\n",
        "chat = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain what is Generative AI in 2 bullet points\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Make the API call using chat_completion()\n",
        "# Parameters:\n",
        "#   - messages: The conversation history (our 'chat' list)\n",
        "#   - model: Which model to use for generation\n",
        "#   - max_tokens: Maximum number of tokens in the response (1 token ‚âà 4 characters)\n",
        "response = client.chat_completion(chat, model=model_name, max_tokens=1000)\n",
        "\n",
        "# Print the full response object to see its structure\n",
        "print(\"Full API Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1fc18309",
      "metadata": {},
      "outputs": [
        {
          "ename": "HFValidationError",
          "evalue": "Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'HuggingFaceTB/SmolLM3-3B:hf-inference'.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[32m      4\u001b[39m client = InferenceClient(\n\u001b[32m      5\u001b[39m     api_key=os.environ[\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceTB/SmolLM3-3B:hf-inference\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion.choices[\u001b[32m0\u001b[39m].message)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:878\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    875\u001b[39m payload_model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# Get the provider helper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversational\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# Prepare the payload\u001b[39;00m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/__init__.py:216\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     provider_mapping = \u001b[43m_fetch_inference_provider_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     provider = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(provider_mapping)).provider\n\u001b[32m    219\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/_common.py:307\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03mFetch provider mappings for a model from the Hub.\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m info = \u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferenceProviderMapping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m provider_mapping = info.inference_provider_mapping\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have -- or .. in repo_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'HuggingFaceTB/SmolLM3-3B:hf-inference'."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"HuggingFaceTB/SmolLM3-3B:hf-inference\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c8736a",
      "metadata": {},
      "source": [
        "### Step 4: Extracting the Response\n",
        "\n",
        "The API returns a `ChatCompletionOutput` object with several fields:\n",
        "- `choices`: List of generated responses (usually just one)\n",
        "- `id`: Unique identifier for this request\n",
        "- `model`: The model that was used\n",
        "- `usage`: Token usage statistics (prompt_tokens, completion_tokens, total_tokens)\n",
        "\n",
        "To get just the text response, we need to navigate: `response.choices[0].message.content`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc33d94",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are 2 bullet points explaining what Generative AI is:\n",
            "\n",
            "‚Ä¢ **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\n",
            "\n",
            "‚Ä¢ **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 4: Extract the Generated Text from the Response\n",
        "# =============================================================================\n",
        "\n",
        "# The response structure is:\n",
        "# response\n",
        "#   ‚îî‚îÄ‚îÄ choices (list of completions)\n",
        "#       ‚îî‚îÄ‚îÄ [0] (first/only choice)\n",
        "#           ‚îî‚îÄ‚îÄ message\n",
        "#               ‚îî‚îÄ‚îÄ content (the actual generated text)\n",
        "\n",
        "# Extract just the text content\n",
        "generated_text = response.choices[0].message.content\n",
        "print(\"Generated Response:\")\n",
        "print(\"-\" * 50)\n",
        "print(generated_text)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Bonus: Let's also look at the token usage\n",
        "print(f\"\\nüìä Token Usage Statistics:\")\n",
        "print(f\"   - Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"   - Completion tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"   - Total tokens: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a7cd26",
      "metadata": {},
      "source": [
        "### Bonus: Advanced Usage with System Prompt\n",
        "\n",
        "You can customize the AI's behavior using a **system prompt**. This is especially useful for:\n",
        "- Setting a specific persona or role\n",
        "- Defining output format requirements\n",
        "- Establishing constraints or guidelines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "18f076af",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response with System Prompt:\n",
            "--------------------------------------------------\n",
            "Imagine you're at a restaurant and you want to order food. You can't just walk into the kitchen and start making your own food, right? You need to tell the waiter what you want, and they'll order it for you.\n",
            "\n",
            "An API (Application Programming Interface) is like the waiter. You give the waiter (API) instructions (requests), and they go to the kitchen (server) to get what you need (data). The waiter then brings back the data (response) to you.\n",
            "\n",
            "In code, you send a request to the API, and it returns data that you can use in your program. APIs help different apps and systems talk to each other and share data.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Bonus: Using System Prompts to Customize AI Behavior\n",
        "# =============================================================================\n",
        "\n",
        "# Define a conversation with a system prompt\n",
        "# The system prompt sets the AI's persona and behavior rules\n",
        "chat_with_system = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful coding tutor. Explain concepts simply and use analogies. Keep responses concise.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is an API?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Make the request with the system prompt\n",
        "response_with_system = client.chat_completion(\n",
        "    chat_with_system,\n",
        "    model=model_name,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response with System Prompt:\")\n",
        "print(\"-\" * 50)\n",
        "print(response_with_system.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c5fa08",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "In this notebook, you learned how to:\n",
        "\n",
        "1. **Set up** the Hugging Face Inference Client\n",
        "2. **Authenticate** using API tokens stored in environment variables\n",
        "3. **Make API calls** to open-source LLMs hosted on HuggingFace\n",
        "4. **Parse responses** to extract the generated text\n",
        "5. **Use system prompts** to customize AI behavior\n",
        "\n",
        "## üîó Additional Resources\n",
        "\n",
        "- [HuggingFace Inference Client Documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n",
        "- [Available Models for Inference](https://huggingface.co/models?inference=warm)\n",
        "- [Chat Completion API Reference](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient.chat_completion)\n",
        "\n",
        "## üí° Try It Yourself\n",
        "\n",
        "Experiment with:\n",
        "- Different models (e.g., `mistralai/Mistral-7B-Instruct-v0.2`)\n",
        "- Different `max_tokens` values\n",
        "- Adding multi-turn conversations\n",
        "- Using different system prompts to change the AI's personality\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
