{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kiH8lf1y4sD"
      },
      "source": [
        "# Build a Multi-User Conversational Tool-Calling Agentic AI Research Assistant with LangChain\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "1. **Multi-User Session Management**: How to store and retrieve conversation history per user session\n",
        "2. **SQL-based Memory Storage**: Using `SQLChatMessageHistory` to persist conversations in a database\n",
        "3. **Conversational AI Agents**: Building agents that maintain context across multiple interactions\n",
        "4. **Tool Integration**: Combining web search and weather tools with session-based memory\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Completion of M2 notebook (Tool-Calling Agentic AI Research Assistant)\n",
        "- Basic understanding of LangChain agents and tools\n",
        "- API keys: OpenAI, Tavily Search, WeatherAPI (OpenWeatherMap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYBpZTjLnEXb"
      },
      "source": [
        "## üìö Conceptual Overview\n",
        "\n",
        "This demo builds on the previous Tool-Calling Agent by adding **multi-user session management**. The key enhancement is the ability to:\n",
        "\n",
        "- **Store conversation history** in a SQLite database\n",
        "- **Retrieve context** for each user based on their session ID\n",
        "- **Enable contextual follow-up questions** like \"What about Intel?\" or \"Which city is hotter?\"\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    Multi-User Conversational Agent                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  User 1: john001          ‚îÇ  User 2: bond007                        ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ\n",
        "‚îÇ  ‚îÇ \"Nvidia earnings?\" ‚îÇ    ‚îÇ  ‚îÇ \"Weather in BLR?\"  ‚îÇ                  ‚îÇ\n",
        "‚îÇ  ‚îÇ \"What about Intel?\"‚îÇ    ‚îÇ  ‚îÇ \"What about Mumbai?\"‚îÇ                  ‚îÇ\n",
        "‚îÇ  ‚îÇ \"Which is better?\" ‚îÇ    ‚îÇ  ‚îÇ \"Which is hotter?\"  ‚îÇ                  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
        "‚îÇ           ‚îÇ               ‚îÇ           ‚îÇ                             ‚îÇ\n",
        "‚îÇ           ‚ñº               ‚îÇ           ‚ñº                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  ‚îÇ                 SQLite Memory Database                     ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  session_id: john001 ‚Üí [message1, message2, message3]     ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  session_id: bond007 ‚Üí [message1, message2, message3]     ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "![](https://i.imgur.com/lHWqaT9.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "---\n",
        "## üîß Section 1: Environment Setup and Dependencies\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2evPp14fy258"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSTALL DEPENDENCIES (Run once, then comment out)\n",
        "# =============================================================================\n",
        "# Uncomment these lines if you need to install the packages:\n",
        "\n",
        "# !pip install langchain==0.3.14\n",
        "# !pip install langchain-openai==0.3.0\n",
        "# !pip install langchain-community==0.3.14\n",
        "# !pip install markitdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD ENVIRONMENT VARIABLES\n",
        "# =============================================================================\n",
        "# This loads API keys from a .env file in the project directory.\n",
        "# Your .env file should contain:\n",
        "#   OPENAI_API_KEY=your_openai_key\n",
        "#   TAVILY_API_KEY=your_tavily_key  \n",
        "#   WEATHER_API_KEY=your_openweathermap_key\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# find_dotenv() searches for .env file in current and parent directories\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howf-v0ARWbv"
      },
      "source": [
        "---\n",
        "## üõ†Ô∏è Section 2: Create Custom Tools\n",
        "\n",
        "We will create two custom tools that the agent can use:\n",
        "\n",
        "1. **Web Search Tool** (`search_web_extract_info`): Searches the web using Tavily API and extracts content from search results\n",
        "2. **Weather Tool** (`get_weather`): Fetches real-time weather data using OpenWeatherMap API\n",
        "\n",
        "### Key Concepts:\n",
        "- The `@tool` decorator converts a Python function into a LangChain tool\n",
        "- Tools must have clear docstrings that describe their purpose (used by the LLM for tool selection)\n",
        "\n",
        "![](https://i.imgur.com/TyPAYXE.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue8xgu9WpuPi"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TOOL CREATION: Web Search and Weather Tools\n",
        "# =============================================================================\n",
        "# In LangChain, \"tools\" are functions that agents can invoke to interact with\n",
        "# external systems (APIs, databases, web search, etc.).\n",
        "#\n",
        "# The @tool decorator converts a regular Python function into a LangChain-\n",
        "# compatible tool. The docstring becomes the tool description that the LLM\n",
        "# uses to decide WHEN to call this tool.\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from markitdown import MarkItDown\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
        "import requests\n",
        "import os\n",
        "from warnings import filterwarnings\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TOOL 1: Web Search and Information Extraction\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize Tavily search with advanced settings for better quality results\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,           # Return top 5 search results\n",
        "    search_depth='advanced', # Use advanced search for better relevance\n",
        "    include_answer=False,    # Don't include AI-generated summary\n",
        "    include_raw_content=True # Include raw HTML content from pages\n",
        ")\n",
        "\n",
        "# Configure HTTP session with browser-like headers to avoid bot detection\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "})\n",
        "\n",
        "# MarkItDown converts web pages to readable markdown text\n",
        "md = MarkItDown(requests_session=session)\n",
        "\n",
        "@tool\n",
        "def search_web_extract_info(query: str) -> list:\n",
        "    \"\"\"\n",
        "    Search the web for a query and extract useful information from the search links.\n",
        "    \n",
        "    This tool performs two main operations:\n",
        "    1. Searches the web using Tavily API to find relevant URLs\n",
        "    2. Extracts and converts content from each URL to readable text\n",
        "    \n",
        "    Args:\n",
        "        query (str): The search query to find information about\n",
        "        \n",
        "    Returns:\n",
        "        list: A list of extracted text content from relevant web pages\n",
        "    \"\"\"\n",
        "    print('Calling web search tool')\n",
        "    results = tavily_tool.invoke(query)\n",
        "    docs = []\n",
        "\n",
        "    def extract_content(url):\n",
        "        \"\"\"Helper function to extract content from a URL.\"\"\"\n",
        "        extracted_info = md.convert(url)\n",
        "        text_title = extracted_info.title.strip()\n",
        "        text_content = extracted_info.text_content.strip()\n",
        "        return text_title + '\\n' + text_content\n",
        "\n",
        "    # Process URLs in parallel for faster execution\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for result in tqdm(results):\n",
        "            try:\n",
        "                future = executor.submit(extract_content, result['url'])\n",
        "                content = future.result(timeout=60)  # 60-second timeout\n",
        "                docs.append(content)\n",
        "            except TimeoutError:\n",
        "                print(f\"Extraction timed out for url: {result['url']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting from url: {result['url']} - {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TOOL 2: Weather Information Tool  \n",
        "# -----------------------------------------------------------------------------\n",
        "# This tool fetches real-time weather data using OpenWeatherMap API\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetch current weather data for a specified location using OpenWeatherMap API.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The city name to get weather information for\n",
        "        \n",
        "    Returns:\n",
        "        dict: Weather data including temperature, humidity, wind speed, etc.\n",
        "              Returns \"Weather Data Not Found\" if the location is invalid\n",
        "    \"\"\"\n",
        "    # Construct API URL with city name and API key\n",
        "    # Note: The ',IN' suffix restricts search to India - modify as needed\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={query},IN&appid={os.getenv('WEATHER_API_KEY')}&units=metric\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    \n",
        "    # Validate response by checking if 'name' field exists\n",
        "    if data.get(\"name\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7kvwG3SmREW"
      },
      "source": [
        "---\n",
        "## ü§ñ Section 3: Build the AI Agent\n",
        "\n",
        "Now we'll create the agent with a **ReAct-style prompt** that guides the agent through:\n",
        "- **Thought**: What should I do to answer this question?\n",
        "- **Action**: Which tool should I use?\n",
        "- **Observation**: What did the tool return?\n",
        "- **Final Answer**: The complete response to the user\n",
        "\n",
        "### Key Components:\n",
        "1. **System Prompt** - Defines agent behavior and available tools\n",
        "2. **Message Placeholders** - For conversation history and agent working memory\n",
        "3. **Agent** - The decision-making component\n",
        "4. **AgentExecutor** - The runtime that executes agent decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grNq1I6_5dxC"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE THE AGENT PROMPT TEMPLATE (ReAct Pattern)\n",
        "# =============================================================================\n",
        "# The prompt template defines how the agent should think and behave.\n",
        "# We use the ReAct (Reasoning + Acting) pattern which structures the agent's\n",
        "# thought process into: Thought ‚Üí Action ‚Üí Observation ‚Üí (repeat) ‚Üí Answer\n",
        "#\n",
        "# KEY ADDITION: The 'history' placeholder allows us to inject previous\n",
        "# conversation messages, enabling contextual follow-up questions!\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant.\n",
        "                You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "                At the end of the loop, you output an Answer.\n",
        "                Use Thought to describe your thoughts about the question you have been asked.\n",
        "                Use Action to run one of the actions available to you - then return PAUSE.\n",
        "                Observation will be the result of running those actions.\n",
        "                Repeat till you get to the answer for the given user query.\n",
        "\n",
        "                Use the following workflow format:\n",
        "                  Question: the input task you must solve\n",
        "                  Thought: you should always think about what to do\n",
        "                  Action: the action to take which can be any of the following:\n",
        "                            - break it into smaller steps if needed\n",
        "                            - see if you can answer the given task with your trained knowledge\n",
        "                            - call the most relevant tools at your disposal mentioned below in case you need more information\n",
        "                  Action Input: the input to the action\n",
        "                  Observation: the result of the action\n",
        "                  ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "                  Thought: I now know the final answer\n",
        "                  Final Answer: the final answer to the original input question\n",
        "\n",
        "                Tools at your disposal to perform tasks as needed:\n",
        "                  - get_weather: whenever user asks get the weather of a place.\n",
        "                  - search_web_extract_info: whenever user asks for specific information or if you don't know the answer.\n",
        "             \"\"\"\n",
        "\n",
        "# Build the prompt template with multiple components:\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),                                    # System instructions\n",
        "        MessagesPlaceholder(variable_name=\"history\", optional=True), # üîë Chat history for memory!\n",
        "        (\"human\", \"{query}\"),                                      # User's current query\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),     # Agent's working memory\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Prompt template messages:\")\n",
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4NMA82HpueH"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE THE AGENT AND AGENT EXECUTOR\n",
        "# =============================================================================\n",
        "# The Agent is the \"brain\" that decides what actions to take.\n",
        "# The AgentExecutor is the \"body\" that executes those actions.\n",
        "#\n",
        "# IMPORTANT: We pass 'chatgpt' (not 'chatgpt_with_tools') because\n",
        "# create_tool_calling_agent handles .bind_tools() internally.\n",
        "# =============================================================================\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the LLM with temperature=0 for deterministic responses\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Define the tools available to the agent\n",
        "tools = [search_web_extract_info, get_weather]\n",
        "\n",
        "# Create the agent (LLM + Tools + Prompt)\n",
        "agent = create_tool_calling_agent(chatgpt, tools, prompt_template)\n",
        "\n",
        "# Create the AgentExecutor (runtime that executes agent decisions)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    early_stopping_method='force',  # Force stop if max iterations reached\n",
        "    max_iterations=10               # Prevent infinite loops\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Agent and AgentExecutor created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9mxPskx734"
      },
      "source": [
        "---\n",
        "## üß† Section 4: Add Multi-User Conversation Memory\n",
        "\n",
        "This is the **key differentiator** from the basic agent! We use `SQLChatMessageHistory` to:\n",
        "\n",
        "1. **Store** each conversation in a SQLite database\n",
        "2. **Retrieve** conversation history based on user/session ID\n",
        "3. **Enable** contextual follow-up questions\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "```python\n",
        "# Each user gets their own conversation history:\n",
        "session_id = 'john001'  # User identifier\n",
        "\n",
        "# When user asks: \"Summarize Nvidia's Q4 2024 earnings\"\n",
        "# ‚Üí Agent searches web, returns summary, stores in DB under 'john001'\n",
        "\n",
        "# Later, when user asks: \"What about Intel?\"\n",
        "# ‚Üí Agent retrieves history, sees context about earnings, searches for Intel\n",
        "\n",
        "# Even later: \"Which company is doing better?\"\n",
        "# ‚Üí Agent uses full context to compare Nvidia vs Intel\n",
        "```\n",
        "\n",
        "### Why SQLite?\n",
        "- **Persistence**: Conversations survive between sessions\n",
        "- **Scalability**: Can handle multiple users simultaneously\n",
        "- **Simplicity**: No external database server needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUG09xD5zd2N"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# OPTIONAL: Clear Previous Memory (Run only when you want a fresh start)\n",
        "# =============================================================================\n",
        "# This removes the SQLite database file that stores all conversation histories.\n",
        "# Only run this if you want to reset ALL user conversations!\n",
        "\n",
        "# !rm memory.db  # On Linux/Mac\n",
        "# # Or on Windows: !del memory.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XSzUrlyZyFF"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE MULTI-USER CONVERSATIONAL AGENT WITH SQL MEMORY\n",
        "# =============================================================================\n",
        "# This is where the magic happens! We wrap the AgentExecutor with:\n",
        "# 1. SQLChatMessageHistory - Stores messages in SQLite database\n",
        "# 2. RunnableWithMessageHistory - Automatically injects history into the agent\n",
        "#\n",
        "# Each user/session is identified by a unique 'session_id', and their\n",
        "# conversations are stored separately in the database.\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_session_history_db(session_id):\n",
        "    \"\"\"\n",
        "    Retrieve conversation history from the SQLite database for a specific session.\n",
        "    \n",
        "    This function is called automatically by RunnableWithMessageHistory to:\n",
        "    1. Load previous messages when starting a new conversation turn\n",
        "    2. Save new messages after each interaction\n",
        "    \n",
        "    Args:\n",
        "        session_id (str): Unique identifier for the user/session (e.g., 'john001')\n",
        "        \n",
        "    Returns:\n",
        "        SQLChatMessageHistory: Object that manages message storage/retrieval\n",
        "    \"\"\"\n",
        "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
        "\n",
        "\n",
        "# Wrap the agent executor with message history capability\n",
        "agentic_chatbot = RunnableWithMessageHistory(\n",
        "    agent_executor,                 # The agent to wrap\n",
        "    get_session_history_db,         # Function to get/create session history\n",
        "    input_messages_key=\"query\",     # Key for user input in the prompt\n",
        "    history_messages_key=\"history\", # Key for history in the prompt template\n",
        ")\n",
        "\n",
        "\n",
        "def chat_with_agent(prompt: str, session_id: str):\n",
        "    \"\"\"\n",
        "    Send a message to the conversational agent and display the response.\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): The user's question or request\n",
        "        session_id (str): Unique identifier for this user's conversation\n",
        "    \"\"\"\n",
        "    response = agentic_chatbot.invoke(\n",
        "        {\"query\": prompt},\n",
        "        {'configurable': {'session_id': session_id}}  # Pass session ID for memory\n",
        "    )\n",
        "    display(Markdown(response['output']))\n",
        "\n",
        "\n",
        "print(\"‚úÖ Multi-user conversational agent ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYTNL_iJ6ibC"
      },
      "source": [
        "---\n",
        "## üß™ Section 5: Test Multi-User Conversations\n",
        "\n",
        "Now let's test the agent with **two different users** to demonstrate:\n",
        "1. Each user maintains their own conversation context\n",
        "2. Follow-up questions work correctly within each session\n",
        "3. Users don't see each other's conversation history\n",
        "\n",
        "### Test Scenario 1: User 'john001' - Financial Research\n",
        "We'll ask about company earnings and test contextual follow-ups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irMU68Ds0NTm"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 1: john001 - First Question (Nvidia Earnings)\n",
        "# =============================================================================\n",
        "# This is the initial question - no context exists yet.\n",
        "# The agent will search the web and store this interaction in the database.\n",
        "\n",
        "user_id = 'john001'\n",
        "prompt = \"Summarize the key points discussed in Nvidia's Q4 2024 earnings call\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4vtvku0TBW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 1: john001 - Contextual Follow-up (Intel Earnings)\n",
        "# =============================================================================\n",
        "# üîë KEY POINT: The question \"What about Intel?\" only makes sense in context!\n",
        "# Without memory, the agent wouldn't know we're asking about earnings.\n",
        "# With memory, the agent understands we want Intel's Q4 2024 earnings.\n",
        "\n",
        "prompt = \"What about Intel?\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-sz15g8YGNA"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 1: john001 - Comparative Analysis Using Context\n",
        "# =============================================================================\n",
        "# üîë This question requires understanding BOTH previous responses!\n",
        "# The agent uses stored context about Nvidia and Intel to make a comparison.\n",
        "# No new web search is needed - the information is already in memory.\n",
        "\n",
        "prompt = \"Which company seems to be doing better?\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4zIlISy6m-x"
      },
      "source": [
        "### Test Scenario 2: User 'bond007' - Weather Research\n",
        "A different user with completely separate context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta1RUF_81RxX"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 2: bond007 - Different User, Different Context (Weather)\n",
        "# =============================================================================\n",
        "# This is a completely different user with their own conversation history.\n",
        "# They're asking about weather - unrelated to john001's financial queries.\n",
        "\n",
        "user_id = 'bond007'\n",
        "prompt = \"how is the weather in Bangalore today? Show detailed statistics\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfn_Wxxg42rZ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 2: bond007 - Contextual Follow-up (Mumbai Weather)\n",
        "# =============================================================================\n",
        "# Again, \"what about Mumbai?\" only makes sense in context.\n",
        "# The agent knows to fetch Mumbai's weather because of the previous query.\n",
        "\n",
        "user_id = 'bond007'\n",
        "prompt = \"what about Mumbai?\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7jjxtmz6Qzi"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# USER 2: bond007 - Comparative Question Using Context\n",
        "# =============================================================================\n",
        "# Without memory, the agent wouldn't know which cities to compare!\n",
        "# With memory, it understands we're comparing Bangalore vs Mumbai temperatures.\n",
        "\n",
        "user_id = 'bond007'\n",
        "prompt = \"which city is hotter?\"\n",
        "chat_with_agent(prompt, user_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìù Summary and Key Takeaways\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **Session-Based Memory**: Using `SQLChatMessageHistory` to store conversations per user\n",
        "2. **Contextual Understanding**: Agents can answer follow-up questions that reference previous context\n",
        "3. **Multi-User Support**: Each user/session has isolated conversation history\n",
        "4. **RunnableWithMessageHistory**: LangChain's way to add memory to any runnable\n",
        "\n",
        "### Comparison: With vs Without Memory\n",
        "\n",
        "| Question | Without Memory | With Memory |\n",
        "|----------|---------------|-------------|\n",
        "| \"What about Intel?\" | ‚ùå \"Please clarify what you're asking about Intel\" | ‚úÖ Searches for Intel Q4 2024 earnings |\n",
        "| \"Which is better?\" | ‚ùå \"Which companies are you comparing?\" | ‚úÖ Compares Nvidia vs Intel based on context |\n",
        "| \"Which city is hotter?\" | ‚ùå \"Please specify the cities\" | ‚úÖ Compares Bangalore vs Mumbai |\n",
        "\n",
        "### Next Steps:\n",
        "- Explore **LangGraph** for more advanced agent architectures with stateful graphs\n",
        "- Add **Redis** or **PostgreSQL** for production-scale memory storage\n",
        "- Implement **conversation summarization** for very long conversations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
