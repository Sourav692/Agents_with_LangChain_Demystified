{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-header",
      "metadata": {},
      "source": [
        "# ðŸ¤– Tool-Calling Agents in LangChain\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, you will learn:\n",
        "1. **What is an Agent?** - An LLM that can autonomously decide which tools to use\n",
        "2. **Basic Tool Calling** - Binding tools to an LLM and processing tool calls\n",
        "3. **Complete Agents** - Using `AgentExecutor` for automatic tool execution\n",
        "4. **Pydantic Schemas** - Structured input validation for tools\n",
        "5. **Parallel Tool Calling** - Handling multiple tool calls in a single request\n",
        "\n",
        "## Key Concepts\n",
        "- **Tool Binding**: Attaching tools to an LLM so it knows what's available\n",
        "- **Tool Calls**: The LLM's request to execute a specific tool with arguments\n",
        "- **Agent Loop**: The cycle of LLM thinking â†’ tool execution â†’ processing results\n",
        "- **AgentExecutor**: LangChain's built-in agent runner\n",
        "\n",
        "## Prerequisites\n",
        "- Completed the previous notebook on Tools in LangChain\n",
        "- Basic understanding of LLMs and prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-section",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸ”§ Step 1: Environment Setup\n",
        "\n",
        "Let's set up our LLM using helper functions that manage API keys and configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25fd369",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Import LLM Helper Functions & Initialize LLM\n",
        "# ============================================================================\n",
        "# We use helper functions to create LLM instances with proper configuration.\n",
        "# These functions handle API key loading from .env and model configuration.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "sys.path.append(os.path.abspath(\"../..\"))\n",
        "\n",
        "# Import our LLM factory functions\n",
        "# - get_groq_llm(): Creates a Groq-hosted LLM (fast inference with open-source models)\n",
        "# - get_openai_llm(): Creates an OpenAI GPT model\n",
        "# - get_databricks_llm(): Creates a Databricks-hosted LLM\n",
        "from helpers.utils import get_groq_llm, get_openai_llm, get_databricks_llm\n",
        "\n",
        "print(\"âœ… LLM helpers imported successfully!\")\n",
        "print(f\"ðŸ“ Running on: {platform.system()}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize the LLM based on platform or preference\n",
        "# The choice of LLM affects tool calling capabilities and speed\n",
        "# -----------------------------------------------------------------------------\n",
        "if sys.platform == \"win32\":\n",
        "    # Windows: Use Groq for fast inference\n",
        "    llm = get_groq_llm()\n",
        "elif sys.platform == \"darwin\":\n",
        "    # macOS: Use Databricks-hosted Gemini\n",
        "    llm = get_databricks_llm(\"databricks-gemini-2-5-pro\")  \n",
        "else:\n",
        "    # Linux: Default to Groq\n",
        "    llm = get_groq_llm()\n",
        "\n",
        "# Print which LLM we're using\n",
        "if hasattr(llm, 'model_name'):\n",
        "    print(f\"ðŸ¤– LLM initialized: {llm.model_name}\")\n",
        "elif hasattr(llm, 'model'):\n",
        "    print(f\"ðŸ¤– LLM initialized: {llm.model} (Databricks)\")\n",
        "else:\n",
        "    print(\"ðŸ¤– LLM initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c860320d",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸ”— Part 1: Basic Tool Calling\n",
        "\n",
        "### How Tool Calling Works\n",
        "1. **Define Tools**: Create functions with descriptive docstrings\n",
        "2. **Bind Tools to LLM**: Use `llm.bind_tools()` to attach tools\n",
        "3. **Invoke LLM**: Send a query that might need tool usage\n",
        "4. **Process Tool Calls**: LLM returns `tool_calls` instead of (or alongside) text\n",
        "\n",
        "> **Important**: The LLM doesn't execute tools - it only generates the tool call request!\n",
        "> Your code is responsible for actually running the tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d5f82f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BASIC TOOL CALLING: Define and Bind Tools\n",
        "# ============================================================================\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Define Custom Tools\n",
        "# Note: Docstrings are CRITICAL - the LLM uses them to understand when to use each tool\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the current weather for a given city.\n",
        "    \n",
        "    Args:\n",
        "        city: Name of the city (e.g., 'Bangalore', 'Mumbai', 'Delhi')\n",
        "    \n",
        "    Returns:\n",
        "        A string with temperature and weather conditions\n",
        "    \"\"\"\n",
        "    # This is simulated data - in production, you'd call a real weather API\n",
        "    weather_data = {\n",
        "        \"bangalore\": \"28Â°C, Partly Cloudy\",\n",
        "        \"mumbai\": \"32Â°C, Humid\",\n",
        "        \"delhi\": \"25Â°C, Sunny\"\n",
        "    }\n",
        "    return weather_data.get(city.lower(), \"Weather data not available for this city\")\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression and return the result.\n",
        "    \n",
        "    Args:\n",
        "        expression: A valid Python mathematical expression (e.g., '25 * 4', '100 / 5 + 10')\n",
        "    \n",
        "    Returns:\n",
        "        The calculated result as a string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Bind Tools to the LLM\n",
        "# This creates a new LLM instance that knows about our tools\n",
        "# -----------------------------------------------------------------------------\n",
        "tools = [get_weather, calculate]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"âœ… Tools bound to LLM:\")\n",
        "for t in tools:\n",
        "    print(f\"   ðŸ“Œ {t.name}: {t.description[:60]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tool-call-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BASIC TOOL CALLING: Demonstration\n",
        "# ============================================================================\n",
        "# When we invoke the LLM with a query that needs a tool, it returns tool_calls\n",
        "\n",
        "# Ask a question that requires the weather tool\n",
        "response = llm_with_tools.invoke(\"What's the weather in Bangalore?\")\n",
        "\n",
        "print(\"ðŸ¤– LLM Response Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nðŸ“„ Content (if any): {response.content or '(No text content - tool call generated)'}\") \n",
        "print(f\"\\nðŸ”§ Tool Calls:\")\n",
        "for tc in response.tool_calls:\n",
        "    print(f\"   - Tool: {tc['name']}\")\n",
        "    print(f\"   - Arguments: {tc['args']}\")\n",
        "    print(f\"   - ID: {tc['id']}\")\n",
        "\n",
        "# Note: At this point, the tool hasn't been executed yet!\n",
        "# We need to execute it ourselves using the arguments provided"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4e19f4",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸš€ Part 2: Complete Agent with AgentExecutor\n",
        "\n",
        "While manually processing tool calls works, it's tedious. LangChain provides **AgentExecutor** which:\n",
        "1. Automatically executes tool calls\n",
        "2. Feeds results back to the LLM\n",
        "3. Continues until the LLM provides a final answer\n",
        "4. Handles multi-step reasoning\n",
        "\n",
        "### The Agent Loop\n",
        "```\n",
        "User Query â†’ LLM â†’ Tool Call? â†’ Execute Tool â†’ Feed Result â†’ LLM â†’ ... â†’ Final Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4c2adc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE AGENT: Multi-Tool Agent with AgentExecutor\n",
        "# ============================================================================\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Define a Set of Diverse Tools\n",
        "# This demonstrates how an agent can work with multiple tools\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "@tool\n",
        "def search_database(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Search the internal database for information about a topic.\n",
        "    Use this for internal company data, product information, or customer records.\n",
        "    \"\"\"\n",
        "    # Simulated database search\n",
        "    return f\"Found 3 results for: {query}\"\n",
        "\n",
        "@tool\n",
        "def get_stock_price(symbol: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the current stock price for a given stock symbol.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock ticker symbol (e.g., 'GOOGL', 'MSFT', 'AAPL')\n",
        "    \"\"\"\n",
        "    # Simulated stock prices (in production, use a real API)\n",
        "    prices = {\"GOOGL\": 175.50, \"MSFT\": 420.30, \"AAPL\": 195.80}\n",
        "    price = prices.get(symbol.upper(), \"Not found\")\n",
        "    return f\"{symbol.upper()}: ${price}\"\n",
        "\n",
        "@tool\n",
        "def send_email(to: str, subject: str, body: str) -> str:\n",
        "    \"\"\"\n",
        "    Send an email to the specified recipient.\n",
        "    \n",
        "    Args:\n",
        "        to: Email address of the recipient\n",
        "        subject: Subject line of the email\n",
        "        body: Body content of the email\n",
        "    \"\"\"\n",
        "    # Simulated email sending (in production, use SMTP or email API)\n",
        "    return f\"Email sent to {to} with subject: {subject}\"\n",
        "\n",
        "tools = [search_database, get_stock_price, send_email]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Create the Prompt Template\n",
        "# The placeholder {agent_scratchpad} is where LangChain inserts the agent's\n",
        "# thinking and tool call results during the agent loop\n",
        "# -----------------------------------------------------------------------------\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use the available tools to help answer the user's questions. Always provide helpful and complete responses.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")  # Critical: holds intermediate steps\n",
        "])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Create and Run the Agent\n",
        "# -----------------------------------------------------------------------------\n",
        "# create_tool_calling_agent: Creates an agent that uses native tool calling\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "# AgentExecutor: Runs the agent loop automatically\n",
        "# - verbose=True: Shows the thinking process\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, \n",
        "    tools=tools, \n",
        "    verbose=True  # Set to False in production\n",
        ")\n",
        "\n",
        "# Run a complex multi-step task\n",
        "print(\"ðŸš€ Running Agent with Multi-Step Task:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"What's the stock price of Google? Once you get the stock price, send an email to sourav.banerjee@gmail.com with the information.\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“¤ Final Output:\")\n",
        "print(result[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373dcc32",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸ“‹ Part 3: Structured Tools with Pydantic\n",
        "\n",
        "For production use, you should define your tool inputs using **Pydantic models**. This provides:\n",
        "- **Type Safety**: Inputs are validated before tool execution\n",
        "- **Clear Documentation**: Each field has a description\n",
        "- **Default Values**: Optional parameters with sensible defaults\n",
        "- **Better LLM Understanding**: Rich schema helps the LLM generate correct arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea323267",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STRUCTURED TOOLS: Using Pydantic for Input Validation\n",
        "# ============================================================================\n",
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Define the Input Schema with Pydantic\n",
        "# This provides type checking, validation, and rich descriptions\n",
        "# -----------------------------------------------------------------------------\n",
        "class DataQueryInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Input schema for querying data from a database table.\n",
        "    All fields must be provided to execute the query.\n",
        "    \"\"\"\n",
        "    table_name: str = Field(\n",
        "        description=\"Name of the table to query (e.g., 'customers', 'sales', 'products')\"\n",
        "    )\n",
        "    filters: dict = Field(\n",
        "        description=\"Filter conditions as key-value pairs (e.g., {'region': 'Asia', 'status': 'active'})\"\n",
        "    )\n",
        "    limit: int = Field(\n",
        "        default=10, \n",
        "        description=\"Maximum number of rows to return (default: 10)\"\n",
        "    )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Create a Tool with the Pydantic Schema\n",
        "# The args_schema parameter links the tool to our input model\n",
        "# -----------------------------------------------------------------------------\n",
        "@tool(args_schema=DataQueryInput)\n",
        "def query_data(table_name: str, filters: dict, limit: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Query data from a specified database table with filters.\n",
        "    \n",
        "    Returns matching records up to the specified limit.\n",
        "    \"\"\"\n",
        "    # Simulated query (in production, execute actual SQL)\n",
        "    return f\"Queried {table_name} with filters {filters}, returning up to {limit} rows\"\n",
        "\n",
        "print(\"âœ… Structured tool created!\")\n",
        "print(f\"ðŸ“‹ Tool Name: {query_data.name}\")\n",
        "print(f\"ðŸ“ Tool Description: {query_data.description}\")\n",
        "print(f\"ðŸ“¦ Tool Arguments Schema: {query_data.args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "structured-tool-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STRUCTURED TOOLS: Demonstration\n",
        "# ============================================================================\n",
        "\n",
        "# Bind the structured tool to LLM\n",
        "llm_with_structured_tools = llm.bind_tools([query_data])\n",
        "\n",
        "# Ask a natural language question that should trigger the tool\n",
        "response = llm_with_structured_tools.invoke(\n",
        "    \"Get the top 5 customers from the sales table where region is Asia\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ¤– LLM Generated Tool Call:\")\n",
        "print(\"=\" * 60)\n",
        "if response.tool_calls:\n",
        "    tc = response.tool_calls[0]\n",
        "    print(f\"ðŸ“Œ Tool: {tc['name']}\")\n",
        "    print(f\"ðŸ“¦ Arguments:\")\n",
        "    for key, value in tc['args'].items():\n",
        "        print(f\"   - {key}: {value}\")\n",
        "    \n",
        "    # Execute the tool with the LLM-generated arguments\n",
        "    print(f\"\\nðŸ”§ Tool Execution Result:\")\n",
        "    result = query_data.invoke(tc[\"args\"])\n",
        "    print(f\"   {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afecf39",
      "metadata": {},
      "source": [
        "---\n",
        "## âš¡ Part 4: Parallel Tool Calling\n",
        "\n",
        "Many modern LLMs can call **multiple tools in parallel** when they're independent. This is much faster than sequential tool calling for queries like:\n",
        "- \"Get weather in Bangalore AND Mumbai\" (2 weather calls)\n",
        "- \"Get weather in Delhi and calculate 25 * 4\" (different tools)\n",
        "\n",
        "The LLM will return multiple tool calls in a single response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ded122",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PARALLEL TOOL CALLING: Setup\n",
        "# ============================================================================\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# Use the weather and calculate tools from earlier\n",
        "tools = [get_weather, calculate]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"âœ… LLM ready for parallel tool calling\")\n",
        "print(f\"ðŸ“Œ Available tools: {[t.name for t in tools]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9070f813",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PARALLEL TOOL CALLING: Multiple Tools in One Query\n",
        "# ============================================================================\n",
        "\n",
        "# Ask a question that requires multiple tools\n",
        "query = \"Get weather in Bangalore and Mumbai, also calculate 25 * 4\"\n",
        "\n",
        "response = llm_with_tools.invoke(query)\n",
        "\n",
        "print(f\"ðŸ“ Query: {query}\")\n",
        "print(\"\\nðŸ”§ LLM generated {0} parallel tool calls:\".format(len(response.tool_calls)))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, tool_call in enumerate(response.tool_calls, 1):\n",
        "    print(f\"\\n{i}. Tool: {tool_call['name']}\")\n",
        "    print(f\"   Arguments: {tool_call['args']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parallel-agent-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PARALLEL TOOL CALLING: Complete Agent Execution\n",
        "# ============================================================================\n",
        "# AgentExecutor handles parallel tool calls automatically\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a prompt that encourages parallel tool use\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Call ALL tools needed to fully answer the user's request. When possible, call multiple tools in parallel.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])\n",
        "\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "print(\"ðŸš€ Running Agent with Parallel Tool Calls:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"Get weather in Bangalore and Mumbai, also calculate 25 * 4\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“¤ Final Output:\")\n",
        "print(result[\"output\"] if \"output\" in result else result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary-section",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸ“ Summary\n",
        "\n",
        "In this notebook, we learned how to build **Tool-Calling Agents** in LangChain:\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Tool Binding** | `llm.bind_tools(tools)` - Attaches tools to an LLM |\n",
        "| **Tool Calls** | LLM's request to execute a tool (name + arguments) |\n",
        "| **AgentExecutor** | Automates the agent loop (tool execution + result handling) |\n",
        "| **Pydantic Schemas** | Type-safe tool inputs with validation |\n",
        "| **Parallel Calling** | Multiple independent tools called simultaneously |\n",
        "\n",
        "### Code Patterns\n",
        "\n",
        "```python\n",
        "# 1. Define a tool\n",
        "@tool\n",
        "def my_tool(arg: str) -> str:\n",
        "    \"\"\"Description for LLM to understand when to use this tool.\"\"\"\n",
        "    return \"result\"\n",
        "\n",
        "# 2. Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([my_tool])\n",
        "\n",
        "# 3. Create and run agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "executor = AgentExecutor(agent=agent, tools=tools)\n",
        "result = executor.invoke({\"input\": \"...\"})\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "- Explore **LangGraph** for more complex agent workflows\n",
        "- Learn about **Memory** to build agents that remember context\n",
        "- Build **Custom Agents** with specialized behaviors"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}