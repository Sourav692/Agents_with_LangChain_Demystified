{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CVPAiNAy9MH"
      },
      "source": [
        "# üõ†Ô∏è Exploring Tools in LangChain\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, you will learn:\n",
        "1. **What are Tools?** - Interfaces that enable LLMs to interact with external systems\n",
        "2. **Built-in Tools** - How to use pre-built tools like Wikipedia and Tavily Search\n",
        "3. **Custom Tools** - How to create your own tools with proper type validation\n",
        "4. **Tool Calling** - How LLMs automatically select and invoke the right tools\n",
        "\n",
        "## Prerequisites\n",
        "- Basic understanding of LangChain\n",
        "- Familiarity with Python decorators and type hints\n",
        "- API keys for OpenAI, Tavily, and WeatherAPI (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "---\n",
        "## üì¶ Step 1: Environment Setup\n",
        "\n",
        "First, let's set up our environment by importing necessary libraries and suppressing warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXgPP-n3lDy6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP: Suppress Warnings for Cleaner Output\n",
        "# ============================================================================\n",
        "# We suppress warnings to keep our notebook output clean and focused\n",
        "# In production, you may want to review warnings for debugging\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Warnings suppressed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2evPp14fy258"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# OPTIONAL: Install Required Packages\n",
        "# ============================================================================\n",
        "# Uncomment and run these lines if you haven't installed the packages yet\n",
        "# These packages are required for LangChain functionality\n",
        "\n",
        "# Core LangChain packages\n",
        "# !pip install langchain==0.3.14\n",
        "# !pip install langchain-openai==0.3.0\n",
        "# !pip install langchain-community==0.3.14\n",
        "\n",
        "# Data extraction and utility packages\n",
        "# !pip install wikipedia==1.4.0    # For Wikipedia tool\n",
        "# !pip install markitdown           # For extracting content from URLs\n",
        "# !pip install rich                 # For pretty-printing JSON output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T0s0um5Svfa"
      },
      "source": [
        "---\n",
        "## üîë Step 2: Configure LLM and API Keys\n",
        "\n",
        "We'll initialize our LLM using helper functions that handle API key management.\n",
        "\n",
        "> **Note**: Make sure you have a `.env` file with your API keys:\n",
        "> - `OPENAI_API_KEY` - For OpenAI models\n",
        "> - `GROQ_API_KEY` - For Groq models\n",
        "> - `TAVILY_API_KEY` - For Tavily Search ([Get free key](https://tavily.com/#api))\n",
        "> - `WEATHER_API_KEY` - For WeatherAPI ([Get free key](https://www.weatherapi.com/signup.aspx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Import LLM Helper Functions & Initialize LLM\n",
        "# ============================================================================\n",
        "# We use helper functions to create LLM instances with proper configuration\n",
        "# These functions handle API key loading from .env and model configuration\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "sys.path.append(os.path.abspath(\"../..\"))\n",
        "\n",
        "# Import our LLM factory functions\n",
        "# - get_groq_llm(): Creates a Groq-hosted LLM (fast inference with open-source models)\n",
        "# - get_openai_llm(): Creates an OpenAI GPT model\n",
        "# - get_databricks_llm(): Creates a Databricks-hosted LLM\n",
        "from helpers.utils import get_groq_llm, get_openai_llm, get_databricks_llm\n",
        "\n",
        "print(\"‚úÖ LLM helpers imported successfully!\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize the LLM\n",
        "# Choose your preferred LLM provider by uncommenting the appropriate line\n",
        "# -----------------------------------------------------------------------------\n",
        "llm = get_databricks_llm(\"databricks-gemini-2-5-pro\")  # Databricks-hosted Gemini\n",
        "# llm = get_groq_llm()        # Fast, open-source models hosted by Groq\n",
        "# llm = get_openai_llm()      # OpenAI's GPT models\n",
        "\n",
        "# Print which LLM we're using\n",
        "if hasattr(llm, 'model_name'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model_name}\")\n",
        "elif hasattr(llm, 'model'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model} (Databricks)\")\n",
        "else:\n",
        "    print(\"ü§ñ LLM initialized: Unknown model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65C3PellZGYf"
      },
      "source": [
        "---\n",
        "## üîß Part 1: Exploring Built-in Tools\n",
        "\n",
        "LangChain provides several pre-built tools that you can use out-of-the-box. These tools wrap common APIs and services, making it easy to give your LLM access to external capabilities.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Tool**: An interface that an agent/LLM can use to interact with the world\n",
        "- **API Wrapper**: Handles the low-level API calls\n",
        "- **Tool Attributes**: Each tool has `name`, `description`, and `args`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howf-v0ARWbv"
      },
      "source": [
        "### 1.1 üìö Wikipedia Tool\n",
        "\n",
        "The Wikipedia tool enables you to tap into Wikipedia's vast knowledge base through their API. This is useful for retrieving factual information about entities, concepts, and events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2CMhK9Rjk2t"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# WIKIPEDIA TOOL: Setup and Configuration\n",
        "# ============================================================================\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# Configure the Wikipedia API wrapper\n",
        "# - top_k_results: Number of Wikipedia pages to return (default: 3)\n",
        "# - doc_content_chars_max: Maximum characters to return per document (default: 4000)\n",
        "wiki_api_wrapper = WikipediaAPIWrapper(\n",
        "    top_k_results=3,\n",
        "    doc_content_chars_max=8000\n",
        ")\n",
        "\n",
        "# Create the Wikipedia tool by wrapping the API\n",
        "wiki_tool = WikipediaQueryRun(\n",
        "    api_wrapper=wiki_api_wrapper, \n",
        "    features=\"lxml\"  # Use lxml parser for HTML parsing\n",
        ")\n",
        "\n",
        "# Inspect the tool's attributes - these are what the LLM uses to understand the tool\n",
        "print(\"üìã Tool Name:\", wiki_tool.name)\n",
        "print(\"üìù Tool Description:\", wiki_tool.description)\n",
        "print(\"üì¶ Tool Arguments:\", wiki_tool.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luhjlzeSkgUq"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# WIKIPEDIA TOOL: Demonstration\n",
        "# ============================================================================\n",
        "# Let's test the Wikipedia tool by searching for information about Microsoft\n",
        "\n",
        "result = wiki_tool.invoke({\"query\": \"Microsoft\"})\n",
        "print(\"üîç Wikipedia Search Result for 'Microsoft':\")\n",
        "print(\"=\" * 60)\n",
        "print(result[:2000])  # Print first 2000 characters for readability\n",
        "print(\"\\n... [truncated for display]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnfMeoXJVn-i"
      },
      "source": [
        "#### üé® Customizing Built-in Tools\n",
        "\n",
        "You can customize any built-in tool by wrapping it with your own name, description, and behavior. This is useful when you want to:\n",
        "- Provide a more specific description for your use case\n",
        "- Rename the tool for clarity\n",
        "- Add pre/post-processing logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tO5g9Q1jk7x"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOMIZING TOOLS: Creating a Custom Wikipedia Tool\n",
        "# ============================================================================\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "# Create a custom version of the Wikipedia tool with our own name and description\n",
        "wiki_tool_custom = Tool(\n",
        "    name=\"Wikipedia\",\n",
        "    func=wiki_api_wrapper.run,  # Use the same underlying function\n",
        "    description=\"Useful when you need a detailed answer about general knowledge, \"\n",
        "                \"historical facts, famous people, places, or scientific concepts.\"\n",
        ")\n",
        "\n",
        "# Compare the custom tool attributes\n",
        "print(\"üìã Custom Tool Name:\", wiki_tool_custom.name)\n",
        "print(\"üìù Custom Tool Description:\", wiki_tool_custom.description)\n",
        "print(\"üì¶ Custom Tool Arguments:\", wiki_tool_custom.args)\n",
        "\n",
        "# Test the custom tool (note: uses 'tool_input' instead of 'query')\n",
        "print(\"\\nüîç Testing custom tool with 'AI':\")\n",
        "print(wiki_tool_custom.invoke({\"tool_input\": \"AI\"})[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnQfnkQeV7Hp"
      },
      "source": [
        "### 1.2 üîç Tavily Search Tool\n",
        "\n",
        "**Tavily Search API** is a search engine optimized for LLMs and RAG (Retrieval-Augmented Generation). It provides:\n",
        "- Real-time web search results\n",
        "- Clean, structured output\n",
        "- Advanced search capabilities\n",
        "- Raw content extraction from web pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjWM95p5pB4k"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TAVILY SEARCH TOOL: Setup and Configuration\n",
        "# ============================================================================\n",
        "# Note: Requires TAVILY_API_KEY in your environment variables\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Configure the Tavily search tool\n",
        "# - max_results: Maximum number of search results to return\n",
        "# - search_depth: 'basic' or 'advanced' (advanced provides more detailed results)\n",
        "# - include_raw_content: Whether to include the raw HTML content\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth='advanced',\n",
        "    include_raw_content=True\n",
        ")\n",
        "\n",
        "# Inspect tool attributes\n",
        "print(\"üìã Tool Name:\", tavily_tool.name)\n",
        "print(\"üìù Tool Description:\", tavily_tool.description)\n",
        "print(\"üì¶ Tool Arguments:\", tavily_tool.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khqWQVcvrnG9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TAVILY SEARCH TOOL: Demonstration\n",
        "# ============================================================================\n",
        "# Let's search for information about Microsoft\n",
        "\n",
        "results = tavily_tool.invoke(\"Tell me about Microsoft\")\n",
        "\n",
        "print(\"üîç Tavily Search Results:\")\n",
        "print(\"=\" * 60)\n",
        "for i, result in enumerate(results[:3], 1):  # Show first 3 results\n",
        "    print(f\"\\nüìÑ Result {i}:\")\n",
        "    print(f\"   URL: {result.get('url', 'N/A')}\")\n",
        "    print(f\"   Content: {result.get('content', 'N/A')[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GNP2J9Dd_nj"
      },
      "source": [
        "---\n",
        "## üî® Part 2: Building Custom Tools\n",
        "\n",
        "While built-in tools are useful, you'll often need to create custom tools for your specific use cases. LangChain provides several ways to create tools.\n",
        "\n",
        "### Key Components of a Tool:\n",
        "1. **Name** - A unique identifier for the tool\n",
        "2. **Description** - Explains what the tool does (LLM uses this to decide when to use it)\n",
        "3. **Args Schema** - JSON schema defining the input parameters\n",
        "4. **Function** - The actual code that runs when the tool is invoked\n",
        "5. **Return Direct** - Whether to return the result directly to the user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtoArDYfheYD"
      },
      "source": [
        "### 2.1 ‚ûï Building a Simple Math Tool\n",
        "\n",
        "The simplest way to create a tool is using the `@tool` decorator. Let's create a basic multiplication tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa0mztbQ8Ae3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM TOOLS: Simple Tool with @tool Decorator\n",
        "# ============================================================================\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a, b):\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Inspect the automatically generated tool attributes\n",
        "print(\"üìã Tool Name:\", multiply.name)\n",
        "print(\"üìù Tool Description:\", multiply.description)\n",
        "print(\"üì¶ Tool Arguments:\", multiply.args)\n",
        "print(\"üîß Tool Type:\", type(multiply))\n",
        "\n",
        "# Test the tool with different inputs\n",
        "print(\"\\nüßÆ Testing multiply tool:\")\n",
        "print(f\"   2 √ó 3 = {multiply.invoke({'a': 2, 'b': 3})}\")\n",
        "print(f\"   2.1 √ó 3.2 = {multiply.invoke({'a': 2.1, 'b': 3.2})}\")\n",
        "\n",
        "# Note: Without type hints, the tool accepts any type!\n",
        "print(f\"   2 √ó 'abc' = {multiply.invoke({'a': 2, 'b': 'abc'})} (string repeated!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XED8giGYeS_I"
      },
      "source": [
        "### 2.2 üîí Building a Type-Safe Tool with Pydantic\n",
        "\n",
        "The simple `@tool` decorator doesn't enforce type checking. For production use, you should use **Pydantic** schemas to validate inputs. This approach:\n",
        "- Enforces type safety\n",
        "- Provides better descriptions for each argument\n",
        "- Generates proper JSON schemas for LLM understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sKF2zqQ8Aih"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM TOOLS: Type-Safe Tool with Pydantic Schema\n",
        "# ============================================================================\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "# Define the input schema using Pydantic\n",
        "# This provides type validation and rich descriptions\n",
        "class CalculatorInput(BaseModel):\n",
        "    \"\"\"Input schema for calculator operations.\"\"\"\n",
        "    a: float = Field(description=\"The first number to multiply\")\n",
        "    b: float = Field(description=\"The second number to multiply\")\n",
        "\n",
        "# Define the function with proper type hints\n",
        "def multiply_safe(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers safely.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Create a StructuredTool with the Pydantic schema\n",
        "multiply = StructuredTool.from_function(\n",
        "    func=multiply_safe,\n",
        "    name=\"multiply\",\n",
        "    description=\"Use this tool to multiply two numbers together. \"\n",
        "                \"Both inputs must be valid numbers (integers or decimals).\",\n",
        "    args_schema=CalculatorInput,\n",
        "    return_direct=True  # Return result directly without further processing\n",
        ")\n",
        "\n",
        "# Inspect the tool - note the improved argument schema\n",
        "print(\"üìã Tool Name:\", multiply.name)\n",
        "print(\"üìù Tool Description:\", multiply.description)\n",
        "print(\"üì¶ Tool Arguments:\", multiply.args)\n",
        "\n",
        "# Test with valid input\n",
        "print(\"\\n‚úÖ Valid input (2 √ó 3):\", multiply.invoke({\"a\": 2, \"b\": 3}))\n",
        "\n",
        "# Test with invalid input - this will raise a validation error!\n",
        "print(\"\\n‚ùå Invalid input (2 √ó 'abc'):\")\n",
        "try:\n",
        "    multiply.invoke({\"a\": 2, \"b\": 'abc'})\n",
        "except Exception as e:\n",
        "    print(f\"   Validation Error: {type(e).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMsvq9sVhjuL"
      },
      "source": [
        "### 2.3 üåê Building a Web Search & Information Extraction Tool\n",
        "\n",
        "Let's create a more sophisticated tool that:\n",
        "1. Searches the web using Tavily\n",
        "2. Extracts content from the found URLs using MarkItDown\n",
        "3. Returns clean, structured information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G6V3Kv1jwU0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM TOOLS: Advanced Web Search & Content Extraction Tool\n",
        "# ============================================================================\n",
        "from markitdown import MarkItDown\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "# Initialize the search tool and markdown converter\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth='advanced',\n",
        "    include_answer=False,\n",
        "    include_raw_content=True\n",
        ")\n",
        "md = MarkItDown()\n",
        "\n",
        "@tool\n",
        "def search_web_extract_info(query: str) -> list:\n",
        "    \"\"\"\n",
        "    Search the web for a query and extract useful information from the search results.\n",
        "    \n",
        "    This tool:\n",
        "    1. Searches the web using Tavily Search API\n",
        "    2. Visits each result URL\n",
        "    3. Extracts and cleans the text content\n",
        "    4. Returns a list of extracted documents\n",
        "    \n",
        "    Args:\n",
        "        query: The search query to look up on the web\n",
        "        \n",
        "    Returns:\n",
        "        list: A list of extracted text content from web pages\n",
        "    \"\"\"\n",
        "    # Step 1: Search the web\n",
        "    results = tavily_tool.invoke(query)\n",
        "    docs = []\n",
        "    \n",
        "    # Step 2: Extract content from each URL\n",
        "    for result in tqdm(results, desc=\"Extracting content\"):\n",
        "        try:\n",
        "            extracted_info = md.convert(result['url'])\n",
        "            text_title = extracted_info.title.strip()\n",
        "            text_content = extracted_info.text_content.strip()\n",
        "            docs.append(text_title + '\\n' + text_content)\n",
        "        except Exception as e:\n",
        "            print(f'‚ö†Ô∏è Extraction blocked for url: {result[\"url\"]}')\n",
        "            pass\n",
        "    \n",
        "    return docs\n",
        "\n",
        "print(\"‚úÖ Web search tool created successfully!\")\n",
        "print(\"üìã Tool Name:\", search_web_extract_info.name)\n",
        "print(\"üìù Tool Description:\", search_web_extract_info.description[:100] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qNB0fSmlhLQ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# WEB SEARCH TOOL: Demonstration\n",
        "# ============================================================================\n",
        "# Test the web search tool (this may take a few seconds)\n",
        "\n",
        "docs = search_web_extract_info.invoke('OpenAI GPT-4o')\n",
        "\n",
        "print(f\"\\nüìö Extracted {len(docs)} documents\")\n",
        "if docs:\n",
        "    print(\"\\nüìÑ First document preview:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(docs[0][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3km3-7WcnYk6"
      },
      "source": [
        "### 2.4 üå§Ô∏è Building a Weather Tool\n",
        "\n",
        "Let's create a tool that fetches real-time weather data using the OpenWeatherMap API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM8R-JgOnXdN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CUSTOM TOOLS: Weather API Tool\n",
        "# ============================================================================\n",
        "import requests\n",
        "import rich\n",
        "\n",
        "# Get the Weather API key from environment\n",
        "WEATHER_API_KEY = os.getenv('WEATHER_API_KEY')\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get the current weather for a city using OpenWeatherMap API.\n",
        "    \n",
        "    Args:\n",
        "        query: The name of the city to get weather for (e.g., 'Bangalore', 'Mumbai')\n",
        "        \n",
        "    Returns:\n",
        "        dict: Weather data including temperature, humidity, and conditions,\n",
        "              or an error message if the city is not found\n",
        "    \"\"\"\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={query},IN&appid={WEATHER_API_KEY}&units=metric\"\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    \n",
        "    if data.get(\"name\"):\n",
        "        return data\n",
        "    else:\n",
        "        return {\"error\": \"Weather Data Not Found\", \"city\": query}\n",
        "\n",
        "# Test the weather tool\n",
        "print(\"üå§Ô∏è Testing Weather Tool:\")\n",
        "result = get_weather.invoke(\"Bangalore\")\n",
        "rich.print_json(data=result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIOhB430gpW9"
      },
      "source": [
        "---\n",
        "## ü§ñ Part 3: LLM Tool Calling\n",
        "\n",
        "Now comes the exciting part! **Tool calling** (also known as function calling) is the ability for an LLM to:\n",
        "1. Understand available tools from their descriptions\n",
        "2. Decide which tool(s) to use based on user input\n",
        "3. Generate the correct arguments for the tool\n",
        "4. Execute the tool and incorporate results into its response\n",
        "\n",
        "### Key Insight:\n",
        "> The LLM doesn't actually execute the tools - it generates the tool calls (name + arguments). \n",
        "> Your code is responsible for actually running the tools!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y26Ohn3P54j"
      },
      "source": [
        "### 3.1 üîó Native Tool Calling (Recommended)\n",
        "\n",
        "Most modern LLMs (OpenAI, Anthropic, Gemini, etc.) have native support for tool calling. This is the recommended approach as it's more reliable and efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjKWxFNgB2t_"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOL CALLING: Binding Tools to an LLM\n",
        "# ============================================================================\n",
        "# We create an LLM with tools \"bound\" to it\n",
        "# This tells the LLM what tools are available and how to use them\n",
        "\n",
        "tools = [multiply, search_web_extract_info, get_weather]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"‚úÖ LLM bound with tools:\")\n",
        "for t in tools:\n",
        "    print(f\"   - {t.name}: {t.description[:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ271K_tB9K7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOL CALLING: Let the LLM Decide Which Tools to Use\n",
        "# ============================================================================\n",
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "from pprint import pprint\n",
        "\n",
        "# Create a prompt that requires multiple tools\n",
        "prompt = \"\"\"\n",
        "Given only the tools at your disposal, mention tool calls for the following tasks:\n",
        "Do not change the query given for any search tasks\n",
        "1. What is 2.1 times 3.5\n",
        "2. What is the current weather in Bangalore today\n",
        "3. What are the 4 major Agentic AI Design Patterns\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the LLM - it will return tool calls instead of a text response\n",
        "results = llm_with_tools.invoke(prompt)\n",
        "\n",
        "print(\"ü§ñ LLM decided to call these tools:\")\n",
        "print(\"=\" * 60)\n",
        "pprint(results.tool_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POvGp_xZCpSg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOL CALLING: Execute the Tool Calls\n",
        "# ============================================================================\n",
        "# Now we actually run the tools that the LLM requested\n",
        "\n",
        "# Create a mapping of tool names to tool functions\n",
        "toolkit = {\n",
        "    \"multiply\": multiply,\n",
        "    \"search_web_extract_info\": search_web_extract_info,\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "print(\"üîß Executing tool calls:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for tool_call in results.tool_calls:\n",
        "    tool_name = tool_call[\"name\"].lower()\n",
        "    selected_tool = toolkit[tool_name]\n",
        "    \n",
        "    print(f\"\\nüìû Calling tool: {tool_call['name']}\")\n",
        "    print(f\"   Arguments: {tool_call['args']}\")\n",
        "    \n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    \n",
        "    # Pretty print the output (truncate if too long)\n",
        "    output_str = str(tool_output)\n",
        "    if len(output_str) > 200:\n",
        "        print(f\"   Result: {output_str[:200]}...\")\n",
        "    else:\n",
        "        print(f\"   Result: {tool_output}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Yc8d9MQDqa"
      },
      "source": [
        "### 3.2 üìù Prompt-Based Tool Calling (For LLMs Without Native Support)\n",
        "\n",
        "Some older or open-source LLMs don't have native tool calling support. For these models, we can use a **prompt engineering** approach to get the LLM to output tool calls in a structured format (like JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr2Wt-iuHEuw"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROMPT-BASED TOOL CALLING: Setup\n",
        "# ============================================================================\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import render_text_description\n",
        "\n",
        "# Render tool descriptions as plain text for the prompt\n",
        "rendered_tools = render_text_description(tools)\n",
        "print(\"üìã Tool descriptions for the prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(rendered_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO8AT_uiK3zV"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROMPT-BASED TOOL CALLING: Create the Prompt Template\n",
        "# ============================================================================\n",
        "# This prompt instructs the LLM to output tool calls as JSON\n",
        "\n",
        "system_prompt = f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user instructions, for each instruction do the following:\n",
        " - Return the name and input of the tool to use.\n",
        " - Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        " - The `arguments` should be a dictionary, with keys corresponding\n",
        "   to the argument names and the values corresponding to the requested values.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Prompt template created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJWWhI0NHE3J"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROMPT-BASED TOOL CALLING: Create the Chain and Execute\n",
        "# ============================================================================\n",
        "\n",
        "# Create a chain: Prompt ‚Üí LLM ‚Üí JSON Parser\n",
        "chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "# Define our instructions (each will result in a tool call)\n",
        "instructions = [\n",
        "    {\"input\": \"What is 2.1 times 3.5\"},\n",
        "    {\"input\": \"What is the current weather in Greenland\"},\n",
        "    {\"input\": \"Tell me about the current state of Agentic AI in the industry\"}\n",
        "]\n",
        "\n",
        "# Run all instructions in parallel using map()\n",
        "responses = chain.map().invoke(instructions)\n",
        "\n",
        "print(\"ü§ñ LLM generated these tool calls:\")\n",
        "print(\"=\" * 60)\n",
        "for i, resp in enumerate(responses, 1):\n",
        "    print(f\"\\n{i}. Tool: {resp['name']}\")\n",
        "    print(f\"   Args: {resp['arguments']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLsWGvOkM5Qe"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROMPT-BASED TOOL CALLING: Execute the Tools\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîß Executing tool calls:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for tool_call in responses:\n",
        "    tool_name = tool_call[\"name\"].lower()\n",
        "    selected_tool = toolkit[tool_name]\n",
        "    \n",
        "    print(f\"\\nüìû Calling tool: {tool_call['name']}\")\n",
        "    tool_output = selected_tool.invoke(tool_call[\"arguments\"])\n",
        "    \n",
        "    # Pretty print the output (truncate if too long)\n",
        "    output_str = str(tool_output)\n",
        "    if len(output_str) > 300:\n",
        "        print(f\"   Result: {output_str[:300]}...\")\n",
        "    else:\n",
        "        print(f\"   Result: {tool_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìù Summary\n",
        "\n",
        "In this notebook, we learned:\n",
        "\n",
        "### 1. Built-in Tools\n",
        "- **WikipediaQueryRun**: Query Wikipedia for information\n",
        "- **TavilySearchResults**: Advanced web search optimized for LLMs\n",
        "- Tools can be customized with your own name and description\n",
        "\n",
        "### 2. Custom Tools\n",
        "- Use `@tool` decorator for simple tools\n",
        "- Use `StructuredTool` with Pydantic for type-safe tools\n",
        "- Tools should have clear descriptions for LLM understanding\n",
        "\n",
        "### 3. Tool Calling\n",
        "- **Native**: Use `llm.bind_tools()` for LLMs with built-in support\n",
        "- **Prompt-based**: Use prompt engineering for other LLMs\n",
        "- The LLM decides which tools to use and generates arguments\n",
        "- Your code is responsible for executing the actual tools\n",
        "\n",
        "### Next Steps\n",
        "- Move on to the next notebook to see how to build complete **Tool-Calling Agents**\n",
        "- Learn about the **Agent Loop** and how agents handle multi-step tasks"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}