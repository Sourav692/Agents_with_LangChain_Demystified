{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d779e870",
   "metadata": {},
   "source": [
    "## Using LLMs via Hugging Face Inference Client\n",
    "\n",
    "### What is the Hugging Face Inference Client?\n",
    "\n",
    "The **Hugging Face Inference Client** is a powerful Python library that allows you to interact with Large Language Models (LLMs) hosted on Hugging Face's servers ‚Äî **without needing to download or run the models locally**.\n",
    "\n",
    "### Why Use It?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Free Tier Available** | HuggingFace offers a [free inference API](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client) with basic rate limits |\n",
    "| **No Infrastructure Needed** | Access 150,000+ models without GPU/hardware requirements |\n",
    "| **Easy to Use** | Simple Python API similar to OpenAI's client |\n",
    "| **Wide Model Selection** | Access to latest open-source models like Llama, Mistral, etc. |\n",
    "\n",
    "### Prerequisites\n",
    "- A Hugging Face account (free)\n",
    "- A Hugging Face API token (get it from [Settings > Access Tokens](https://huggingface.co/settings/tokens))\n",
    "- `huggingface_hub` library installed (`pip install huggingface_hub`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5abacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Import the Hugging Face Hub Library\n",
    "# =============================================================================\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "# Print the version to ensure compatibility\n",
    "# IMPORTANT: Version should be >= 0.36.0 for Inference Providers to work properly\n",
    "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "\n",
    "# Import the InferenceClient class\n",
    "# This is the main class we'll use to interact with HuggingFace's hosted models\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897019eb",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up Authentication\n",
    "\n",
    "To use the Inference API, you need to authenticate with your Hugging Face API token. We'll load it securely from environment variables using the `python-dotenv` library.\n",
    "\n",
    "> üîê **Security Best Practice**: Never hardcode your API tokens directly in code. Always use environment variables or secret management tools.\n",
    "\n",
    "üìö **Documentation**: Feel free to refer to the [official InferenceClient documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) for more details on available methods and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a05cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace API token loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Load API Token from Environment Variables\n",
    "# =============================================================================\n",
    "\n",
    "from dotenv import load_dotenv  # Library to load variables from .env file\n",
    "import os  # Standard library for OS operations\n",
    "\n",
    "# Load environment variables from a .env file in the project root\n",
    "# Your .env file should contain: HF_TOKEN=your_huggingface_api_token_here\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Hugging Face API token from environment variables\n",
    "# This token authenticates your requests to the HuggingFace Inference API\n",
    "hf_key = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Optional: Verify the token was loaded (don't print the actual token!)\n",
    "if hf_key:\n",
    "    print(\"‚úÖ HuggingFace API token loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: HF_TOKEN not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6aa82",
   "metadata": {},
   "source": [
    "### Step 3: Making Your First API Call\n",
    "\n",
    "Now let's use the `InferenceClient` to interact with a Large Language Model. We'll use **Meta's Llama 3.1 8B Instruct** model, which is:\n",
    "- An open-source model available for free\n",
    "- Instruction-tuned (optimized to follow instructions)\n",
    "- 8 billion parameters (good balance between quality and speed)\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Chat Completion**: A conversation-style API where you send messages with roles (user, assistant, system)\n",
    "- **Messages Format**: A list of dictionaries with `role` and `content` keys\n",
    "- **max_tokens**: Controls the maximum length of the generated response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7901175c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-697c68c5-7670d4d84ef09fed5a7d8d0e;38a22e06-670e-4c7f-8987-e5b7fa731d8f)\n\nRepository Not Found for url: https://huggingface.co/api/models/meta-llama/Llama-3.1-8B-Instruct?expand=inferenceProviderMapping.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nUser Access Token \"token\" is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/meta-llama/Llama-3.1-8B-Instruct?expand=inferenceProviderMapping",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     24\u001b[39m chat = [\n\u001b[32m     25\u001b[39m     {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mExplain what is Generative AI in 2 bullet points\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m     },\n\u001b[32m     29\u001b[39m ]\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Make the API call using chat_completion()\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Parameters:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#   - messages: The conversation history (our 'chat' list)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#   - model: Which model to use for generation\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#   - max_tokens: Maximum number of tokens in the response (1 token ‚âà 4 characters)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Print the full response object to see its structure\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull API Response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:878\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    875\u001b[39m payload_model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# Get the provider helper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversational\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# Prepare the payload\u001b[39;00m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/__init__.py:216\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     provider_mapping = \u001b[43m_fetch_inference_provider_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     provider = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(provider_mapping)).provider\n\u001b[32m    219\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/_common.py:307\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03mFetch provider mappings for a model from the Hub.\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m info = \u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferenceProviderMapping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m provider_mapping = info.inference_provider_mapping\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:2661\u001b[39m, in \u001b[36mHfApi.model_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2659\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m] = expand\n\u001b[32m   2660\u001b[39m r = get_session().get(path, headers=headers, timeout=timeout, params=params)\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2662\u001b[39m data = r.json()\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(**data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:452\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mRepoNotFound\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    432\u001b[39m     response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m    433\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m error_message != \u001b[33m\"\u001b[39m\u001b[33mInvalid credentials in Authorization header\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[32m    443\u001b[39m     message = (\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m    455\u001b[39m     message = (\n\u001b[32m    456\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m endpoint:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBad request:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-697c68c5-7670d4d84ef09fed5a7d8d0e;38a22e06-670e-4c7f-8987-e5b7fa731d8f)\n\nRepository Not Found for url: https://huggingface.co/api/models/meta-llama/Llama-3.1-8B-Instruct?expand=inferenceProviderMapping.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nUser Access Token \"token\" is expired"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Create the Inference Client and Make a Chat Completion Request\n",
    "# =============================================================================\n",
    "\n",
    "# Define the model to use\n",
    "# Format: \"organization/model-name\"\n",
    "# Note: Only models with \"warm\" inference status work with the free API\n",
    "# You can find available models at: https://huggingface.co/models?inference=warm\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Initialize the InferenceClient with your API token\n",
    "# This client handles all communication with HuggingFace's servers\n",
    "client = InferenceClient(token=hf_key)\n",
    "\n",
    "# Define the conversation as a list of messages\n",
    "# Each message has:\n",
    "#   - \"role\": Who is speaking (\"system\", \"user\", or \"assistant\")\n",
    "#   - \"content\": The actual message text\n",
    "# \n",
    "# Common roles:\n",
    "#   - \"system\": Sets the behavior/personality of the AI (optional)\n",
    "#   - \"user\": Messages from the human user\n",
    "#   - \"assistant\": Previous responses from the AI (for multi-turn conversations)\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain what is Generative AI in 2 bullet points\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Make the API call using chat_completion()\n",
    "# Parameters:\n",
    "#   - messages: The conversation history (our 'chat' list)\n",
    "#   - model: Which model to use for generation\n",
    "#   - max_tokens: Maximum number of tokens in the response (1 token ‚âà 4 characters)\n",
    "response = client.chat_completion(chat, model=model_name, max_tokens=1000)\n",
    "\n",
    "# Print the full response object to see its structure\n",
    "print(\"Full API Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8736a",
   "metadata": {},
   "source": [
    "### Step 4: Extracting the Response\n",
    "\n",
    "The API returns a `ChatCompletionOutput` object with several fields:\n",
    "- `choices`: List of generated responses (usually just one)\n",
    "- `id`: Unique identifier for this request\n",
    "- `model`: The model that was used\n",
    "- `usage`: Token usage statistics (prompt_tokens, completion_tokens, total_tokens)\n",
    "\n",
    "To get just the text response, we need to navigate: `response.choices[0].message.content`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc33d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 bullet points explaining what Generative AI is:\n",
      "\n",
      "‚Ä¢ **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\n",
      "\n",
      "‚Ä¢ **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Extract the Generated Text from the Response\n",
    "# =============================================================================\n",
    "\n",
    "# The response structure is:\n",
    "# response\n",
    "#   ‚îî‚îÄ‚îÄ choices (list of completions)\n",
    "#       ‚îî‚îÄ‚îÄ [0] (first/only choice)\n",
    "#           ‚îî‚îÄ‚îÄ message\n",
    "#               ‚îî‚îÄ‚îÄ content (the actual generated text)\n",
    "\n",
    "# Extract just the text content\n",
    "generated_text = response.choices[0].message.content\n",
    "print(\"Generated Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(generated_text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Bonus: Let's also look at the token usage\n",
    "print(f\"\\nüìä Token Usage Statistics:\")\n",
    "print(f\"   - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   - Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"   - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7cd26",
   "metadata": {},
   "source": [
    "### Bonus: Advanced Usage with System Prompt\n",
    "\n",
    "You can customize the AI's behavior using a **system prompt**. This is especially useful for:\n",
    "- Setting a specific persona or role\n",
    "- Defining output format requirements\n",
    "- Establishing constraints or guidelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f076af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with System Prompt:\n",
      "--------------------------------------------------\n",
      "Imagine you're at a restaurant and you want to order food. You can't just walk into the kitchen and start making your own food, right? You need to tell the waiter what you want, and they'll order it for you.\n",
      "\n",
      "An API (Application Programming Interface) is like the waiter. You give the waiter (API) instructions (requests), and they go to the kitchen (server) to get what you need (data). The waiter then brings back the data (response) to you.\n",
      "\n",
      "In code, you send a request to the API, and it returns data that you can use in your program. APIs help different apps and systems talk to each other and share data.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Bonus: Using System Prompts to Customize AI Behavior\n",
    "# =============================================================================\n",
    "\n",
    "# Define a conversation with a system prompt\n",
    "# The system prompt sets the AI's persona and behavior rules\n",
    "chat_with_system = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful coding tutor. Explain concepts simply and use analogies. Keep responses concise.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is an API?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make the request with the system prompt\n",
    "response_with_system = client.chat_completion(\n",
    "    chat_with_system,\n",
    "    model=model_name,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"Response with System Prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_with_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5fa08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Set up** the Hugging Face Inference Client\n",
    "2. **Authenticate** using API tokens stored in environment variables\n",
    "3. **Make API calls** to open-source LLMs hosted on HuggingFace\n",
    "4. **Parse responses** to extract the generated text\n",
    "5. **Use system prompts** to customize AI behavior\n",
    "\n",
    "## üîó Additional Resources\n",
    "\n",
    "- [HuggingFace Inference Client Documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n",
    "- [Available Models for Inference](https://huggingface.co/models?inference=warm)\n",
    "- [Chat Completion API Reference](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient.chat_completion)\n",
    "\n",
    "## üí° Try It Yourself\n",
    "\n",
    "Experiment with:\n",
    "- Different models (e.g., `mistralai/Mistral-7B-Instruct-v0.2`)\n",
    "- Different `max_tokens` values\n",
    "- Adding multi-turn conversations\n",
    "- Using different system prompts to change the AI's personality\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
