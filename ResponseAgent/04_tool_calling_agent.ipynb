{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling Agent with ResponseAgent\n",
    "\n",
    "This notebook demonstrates building a production-ready tool-calling agent using MLflow ResponseAgent.\n",
    "\n",
    "## Table of Contents\n",
    "1. Understanding Tool Calling\n",
    "2. Building a Tool-Calling Agent\n",
    "3. Complex Tool Workflow\n",
    "4. Logging and Deployment\n",
    "5. Testing with Real Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Callable, Generator\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import mlflow\n",
    "import openai\n",
    "import backoff\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from openai import OpenAI\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Set OPENAI_API_KEY in .env\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"Tool_Calling_Agent\")\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Tool Calling\n",
    "\n",
    "### What is Tool Calling?\n",
    "\n",
    "Tool calling (also known as function calling) allows LLMs to:\n",
    "1. Recognize when they need external information\n",
    "2. Generate structured function calls\n",
    "3. Receive and process function results\n",
    "4. Generate final responses\n",
    "\n",
    "### Tool Calling Flow:\n",
    "\n",
    "```\n",
    "User: \"What's the weather in Boston?\"\n",
    "   ↓\n",
    "LLM: Generates function call → get_weather(location=\"Boston\")\n",
    "   ↓\n",
    "Tool: Executes → Returns \"72°F, Sunny\"\n",
    "   ↓\n",
    "LLM: Processes result → \"The weather in Boston is 72°F and sunny.\"\n",
    "```\n",
    "\n",
    "### ResponseAgent Tool Calling Advantages:\n",
    "\n",
    "✅ Returns intermediate tool calls (for transparency)\n",
    "\n",
    "✅ Structured tool execution tracking\n",
    "\n",
    "✅ Built-in retry logic support\n",
    "\n",
    "✅ Full tracing of tool invocations\n",
    "\n",
    "✅ OpenAI-compatible tool specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a tool that the agent can use.\n",
    "    \n",
    "    Attributes:\n",
    "        name: Tool identifier\n",
    "        spec: OpenAI-compatible tool specification\n",
    "        exec_fn: Python function that implements the tool\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "\n",
    "# Define actual tool implementations\n",
    "def get_weather(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"\n",
    "    Mock weather API - in production, this would call a real API.\n",
    "    \"\"\"\n",
    "    # Simulate API call\n",
    "    return {\n",
    "        \"temperature\": 72,\n",
    "        \"unit\": \"fahrenheit\",\n",
    "        \"condition\": \"sunny\",\n",
    "        \"location\": f\"({latitude}, {longitude})\"\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate(operation: str, x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Perform mathematical operations.\n",
    "    \"\"\"\n",
    "    operations = {\n",
    "        \"add\": lambda a, b: a + b,\n",
    "        \"subtract\": lambda a, b: a - b,\n",
    "        \"multiply\": lambda a, b: a * b,\n",
    "        \"divide\": lambda a, b: a / b if b != 0 else \"Error: Division by zero\",\n",
    "    }\n",
    "    \n",
    "    if operation not in operations:\n",
    "        return f\"Error: Unknown operation {operation}\"\n",
    "    \n",
    "    return operations[operation](x, y)\n",
    "\n",
    "\n",
    "# Create tool specifications\n",
    "TOOLS = [\n",
    "    ToolInfo(\n",
    "        name=\"get_weather\",\n",
    "        spec={\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current temperature and weather conditions for coordinates.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Latitude coordinate\"\n",
    "                    },\n",
    "                    \"longitude\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Longitude coordinate\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "            \"strict\": True,\n",
    "        },\n",
    "        exec_fn=get_weather,\n",
    "    ),\n",
    "    ToolInfo(\n",
    "        name=\"calculate\",\n",
    "        spec={\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Perform mathematical calculations: add, subtract, multiply, divide.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"operation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                        \"description\": \"Mathematical operation to perform\"\n",
    "                    },\n",
    "                    \"x\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"First number\"\n",
    "                    },\n",
    "                    \"y\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Second number\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"operation\", \"x\", \"y\"],\n",
    "            },\n",
    "        },\n",
    "        exec_fn=calculate,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Defined {len(TOOLS)} tools: {[t.name for t in TOOLS]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Tool-Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Production-ready tool-calling agent with ResponseAgent interface.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic tool execution\n",
    "    - Iterative tool calling (agent can call multiple tools)\n",
    "    - Full tracing of tool invocations\n",
    "    - Retry logic with exponential backoff\n",
    "    - Structured error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, tools: list[ToolInfo], system_prompt: str = None):\n",
    "        \"\"\"Initialize agent with model and tools.\"\"\"\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "        self.system_prompt = system_prompt or \"You are a helpful assistant with access to tools.\"\n",
    "    \n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        \"\"\"Return tool specifications for OpenAI API.\"\"\"\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        \"\"\"\n",
    "        Execute a tool with given arguments.\n",
    "        \n",
    "        This is traced separately for observability.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self._tools_dict[tool_name].exec_fn(**args)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error executing tool: {str(e)}\"\n",
    "    \n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    @mlflow.trace(span_type=SpanType.LLM)\n",
    "    def call_llm(self, input_messages) -> dict:\n",
    "        \"\"\"\n",
    "        Call LLM with retry logic.\n",
    "        \n",
    "        Uses exponential backoff for rate limits.\n",
    "        \"\"\"\n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=input_messages,\n",
    "            tools=self.get_tool_specs(),\n",
    "        )\n",
    "        return response.output[0].model_dump(exclude_none=True)\n",
    "    \n",
    "    def handle_tool_call(self, tool_call: dict[str, Any]) -> ResponsesAgentStreamEvent:\n",
    "        \"\"\"\n",
    "        Execute a tool call and return the result as a stream event.\n",
    "        \"\"\"\n",
    "        # Parse arguments\n",
    "        args = json.loads(tool_call[\"arguments\"])\n",
    "        \n",
    "        # Execute tool\n",
    "        result = self.execute_tool(tool_name=tool_call[\"name\"], args=args)\n",
    "        \n",
    "        # Convert to string for LLM\n",
    "        result_str = json.dumps(result) if isinstance(result, dict) else str(result)\n",
    "        \n",
    "        # Create tool output event\n",
    "        tool_call_output = {\n",
    "            \"type\": \"function_call_output\",\n",
    "            \"call_id\": tool_call[\"call_id\"],\n",
    "            \"output\": result_str,\n",
    "        }\n",
    "        \n",
    "        return ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=tool_call_output\n",
    "        )\n",
    "    \n",
    "    def call_and_run_tools(\n",
    "        self,\n",
    "        input_messages: list,\n",
    "        max_iter: int = 10,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Iteratively call LLM and execute tools until completion.\n",
    "        \n",
    "        This implements the agent loop:\n",
    "        1. Call LLM\n",
    "        2. If tool call -> execute and continue\n",
    "        3. If text response -> done\n",
    "        4. Repeat until max iterations\n",
    "        \"\"\"\n",
    "        for iteration in range(max_iter):\n",
    "            last_msg = input_messages[-1]\n",
    "            \n",
    "            # Check if we're done (assistant message with text)\n",
    "            if (\n",
    "                last_msg.get(\"type\") == \"message\"\n",
    "                and last_msg.get(\"role\") == \"assistant\"\n",
    "            ):\n",
    "                return\n",
    "            \n",
    "            # Execute tool if last message is a tool call\n",
    "            if last_msg.get(\"type\") == \"function_call\":\n",
    "                tool_call_res = self.handle_tool_call(last_msg)\n",
    "                input_messages.append(tool_call_res.item)\n",
    "                yield tool_call_res\n",
    "            else:\n",
    "                # Call LLM\n",
    "                llm_output = self.call_llm(input_messages=input_messages)\n",
    "                input_messages.append(llm_output)\n",
    "                yield ResponsesAgentStreamEvent(\n",
    "                    type=\"response.output_item.done\",\n",
    "                    item=llm_output,\n",
    "                )\n",
    "        \n",
    "        # Max iterations reached\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item={\n",
    "                \"id\": str(uuid4()),\n",
    "                \"content\": [{\n",
    "                    \"type\": \"output_text\",\n",
    "                    \"text\": \"Maximum iterations reached. Task may be incomplete.\",\n",
    "                }],\n",
    "                \"role\": \"assistant\",\n",
    "                \"type\": \"message\",\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=request.custom_inputs\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction with tool execution.\"\"\"\n",
    "        # Prepare input messages\n",
    "        input_messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + [i.model_dump() for i in request.input]\n",
    "        \n",
    "        # Run agent loop\n",
    "        yield from self.call_and_run_tools(input_messages=input_messages)\n",
    "\n",
    "\n",
    "print(\"✅ ToolCallingAgent class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Tool-Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create agent\n",
    "agent = ToolCallingAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=TOOLS,\n",
    "    system_prompt=\"You are a helpful assistant with access to weather and calculator tools.\"\n",
    ")\n",
    "\n",
    "print(\"Testing tool-calling agent...\\n\")\n",
    "\n",
    "# Test 1: Weather tool\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Weather Query\")\n",
    "print(\"=\" * 60)\n",
    "response1 = agent.predict({\n",
    "    \"input\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather at coordinates 42.3601, -71.0589 (Boston)?\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(f\"\\nNumber of output items: {len(response1.output)}\")\n",
    "for i, item in enumerate(response1.output):\n",
    "    print(f\"\\nItem {i+1}: {item.get('type')}\")\n",
    "    if item.get('type') == 'function_call':\n",
    "        print(f\"  Tool: {item.get('name')}\")\n",
    "        print(f\"  Args: {item.get('arguments')}\")\n",
    "    elif item.get('type') == 'function_call_output':\n",
    "        print(f\"  Result: {item.get('output')}\")\n",
    "    elif item.get('type') == 'message':\n",
    "        print(f\"  Response: {item['content'][0]['text']}\")\n",
    "\n",
    "# Test 2: Calculator tool\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 2: Math Calculation\")\n",
    "print(\"=\" * 60)\n",
    "response2 = agent.predict({\n",
    "    \"input\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is 125 multiplied by 48?\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "for i, item in enumerate(response2.output):\n",
    "    if item.get('type') == 'message':\n",
    "        print(f\"\\nFinal Response: {item['content'][0]['text']}\")\n",
    "\n",
    "print(\"\\n✅ Tool-calling agent tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Deployable Agent File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tool_calling_agent.py\n",
    "\"\"\"Production tool-calling agent with ResponseAgent.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Any, Callable, Generator\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "import openai\n",
    "import backoff\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "\n",
    "class ToolInfo(BaseModel):\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "\n",
    "def get_weather(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Mock weather API.\"\"\"\n",
    "    return {\n",
    "        \"temperature\": 72,\n",
    "        \"unit\": \"fahrenheit\",\n",
    "        \"condition\": \"sunny\",\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate(operation: str, x: float, y: float) -> float:\n",
    "    \"\"\"Perform calculations.\"\"\"\n",
    "    ops = {\n",
    "        \"add\": lambda a, b: a + b,\n",
    "        \"subtract\": lambda a, b: a - b,\n",
    "        \"multiply\": lambda a, b: a * b,\n",
    "        \"divide\": lambda a, b: a / b if b != 0 else \"Error: Division by zero\",\n",
    "    }\n",
    "    return ops.get(operation, lambda a, b: \"Unknown operation\")(x, y)\n",
    "\n",
    "\n",
    "class ToolCallingAgent(ResponsesAgent):\n",
    "    def __init__(self, model: str, tools: list[ToolInfo], system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "        self.system_prompt = system_prompt or \"You are a helpful assistant.\"\n",
    "    \n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        try:\n",
    "            return self._tools_dict[tool_name].exec_fn(**args)\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    @mlflow.trace(span_type=SpanType.LLM)\n",
    "    def call_llm(self, input_messages) -> dict:\n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=input_messages,\n",
    "            tools=self.get_tool_specs(),\n",
    "        )\n",
    "        return response.output[0].model_dump(exclude_none=True)\n",
    "    \n",
    "    def handle_tool_call(self, tool_call: dict) -> ResponsesAgentStreamEvent:\n",
    "        args = json.loads(tool_call[\"arguments\"])\n",
    "        result = self.execute_tool(tool_call[\"name\"], args)\n",
    "        result_str = json.dumps(result) if isinstance(result, dict) else str(result)\n",
    "        \n",
    "        return ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item={\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": tool_call[\"call_id\"],\n",
    "                \"output\": result_str,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def call_and_run_tools(\n",
    "        self, input_messages: list, max_iter: int = 10\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        for _ in range(max_iter):\n",
    "            last_msg = input_messages[-1]\n",
    "            \n",
    "            if (\n",
    "                last_msg.get(\"type\") == \"message\"\n",
    "                and last_msg.get(\"role\") == \"assistant\"\n",
    "            ):\n",
    "                return\n",
    "            \n",
    "            if last_msg.get(\"type\") == \"function_call\":\n",
    "                tool_res = self.handle_tool_call(last_msg)\n",
    "                input_messages.append(tool_res.item)\n",
    "                yield tool_res\n",
    "            else:\n",
    "                llm_out = self.call_llm(input_messages)\n",
    "                input_messages.append(llm_out)\n",
    "                yield ResponsesAgentStreamEvent(\n",
    "                    type=\"response.output_item.done\", item=llm_out\n",
    "                )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs, custom_outputs=request.custom_inputs\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        input_messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + [i.model_dump() for i in request.input]\n",
    "        yield from self.call_and_run_tools(input_messages)\n",
    "\n",
    "\n",
    "# Define tools\n",
    "TOOLS = [\n",
    "    ToolInfo(\n",
    "        name=\"get_weather\",\n",
    "        spec={\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get weather for coordinates.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": {\"type\": \"number\"},\n",
    "                    \"longitude\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "            },\n",
    "        },\n",
    "        exec_fn=get_weather,\n",
    "    ),\n",
    "    ToolInfo(\n",
    "        name=\"calculate\",\n",
    "        spec={\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Perform math operations.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"operation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    },\n",
    "                    \"x\": {\"type\": \"number\"},\n",
    "                    \"y\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"operation\", \"x\", \"y\"],\n",
    "            },\n",
    "        },\n",
    "        exec_fn=calculate,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create and set model\n",
    "AGENT = ToolCallingAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=TOOLS,\n",
    "    system_prompt=\"You are a helpful assistant with weather and calculator tools.\"\n",
    ")\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logging and Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the tool-calling agent\n",
    "with mlflow.start_run(run_name=\"tool_calling_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"tool_calling_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "            \"backoff\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Tool-calling agent logged!\")\n",
    "    print(f\"Model URI: {model_info.model_uri}\")\n",
    "    print(f\"\\nTo serve: mlflow models serve -m {model_info.model_uri} -p 5001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "1. ✅ Tool specifications (OpenAI compatible)\n",
    "2. ✅ Tool execution framework\n",
    "3. ✅ Iterative agent loop (LLM → Tool → LLM)\n",
    "4. ✅ Full tracing and observability\n",
    "5. ✅ Production-ready deployment\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Tool Definition**: Spec + Implementation\n",
    "- **Agent Loop**: Iterate until completion\n",
    "- **Transparency**: Return all intermediate steps\n",
    "- **Tracing**: Track every tool execution\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- Add authentication for sensitive tools\n",
    "- Implement rate limiting\n",
    "- Add tool result validation\n",
    "- Monitor tool usage and costs\n",
    "- Handle tool failures gracefully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
