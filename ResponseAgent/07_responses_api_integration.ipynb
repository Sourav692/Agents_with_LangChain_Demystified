{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResponsesAgent with OpenAI Responses API\n",
    "\n",
    "This notebook demonstrates using OpenAI's Responses API with MLflow ResponsesAgent for the most seamless integration.\n",
    "\n",
    "## Table of Contents\n",
    "1. OpenAI Responses API Overview\n",
    "2. Direct Responses API Integration\n",
    "3. Streaming with Responses API\n",
    "4. Tool Calling with Responses API\n",
    "5. Complete Production Agent\n",
    "6. Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"ResponsesAPI_Integration\")\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenAI Responses API Overview\n",
    "\n",
    "OpenAI's Responses API is the newer, more powerful API for conversational AI.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Structured Output** | Rich output types (text, tool calls, reasoning) |\n",
    "| **Native Tools** | Built-in function calling support |\n",
    "| **Multi-modal** | Supports various input/output types |\n",
    "| **Streaming** | Real-time streaming with structured events |\n",
    "| **Annotations** | Link citations and references |\n",
    "\n",
    "### Why Use Responses API with ResponsesAgent?\n",
    "\n",
    "MLflow's ResponsesAgent is designed around the Responses API format, providing:\n",
    "\n",
    "1. **Native compatibility**: No format conversion needed\n",
    "2. **Direct passthrough**: Response can be returned as-is\n",
    "3. **Full feature support**: All Responses API features work naturally\n",
    "4. **Simpler code**: Less boilerplate for format handling\n",
    "\n",
    "### API Comparison:\n",
    "\n",
    "```python\n",
    "# ChatCompletions API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[...]\n",
    ")\n",
    "\n",
    "# Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[...]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Responses API Integration\n",
    "\n",
    "The simplest integration - direct passthrough of Responses API results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResponsesAPIAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Simplest ResponsesAgent using OpenAI Responses API.\n",
    "    \n",
    "    Key insight: ResponsesAgent format matches OpenAI Responses API,\n",
    "    so we can use direct passthrough with minimal conversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"\n",
    "        Non-streaming prediction using Responses API.\n",
    "        \n",
    "        The response from OpenAI Responses API can be converted\n",
    "        directly to ResponsesAgentResponse using to_dict().\n",
    "        \"\"\"\n",
    "        # Call Responses API directly\n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=request.input,  # Direct passthrough!\n",
    "        )\n",
    "        \n",
    "        # Convert to ResponsesAgentResponse\n",
    "        return ResponsesAgentResponse(**response.to_dict())\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Streaming prediction using Responses API.\n",
    "        \n",
    "        OpenAI Responses API streaming events can be converted\n",
    "        directly to ResponsesAgentStreamEvent.\n",
    "        \"\"\"\n",
    "        # Stream from Responses API\n",
    "        for event in self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=request.input,\n",
    "            stream=True,\n",
    "        ):\n",
    "            # Direct conversion to ResponsesAgentStreamEvent\n",
    "            yield ResponsesAgentStreamEvent(**event.to_dict())\n",
    "\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Test the agent\n",
    "print(\"Testing Simple Responses API Agent...\\n\")\n",
    "agent = SimpleResponsesAPIAgent(model=\"gpt-4o\")\n",
    "\n",
    "response = agent.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is MLflow in one sentence?\"}]\n",
    "})\n",
    "\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Output: {response.output}\")\n",
    "print(\"\\nâœ… Simple Responses API agent working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming with Responses API\n",
    "\n",
    "Streaming is particularly elegant with the Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Streaming agent using OpenAI Responses API.\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Direct streaming passthrough\n",
    "    - Event type handling\n",
    "    - System prompt integration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\", system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt\n",
    "    \n",
    "    def _prepare_input(self, request: ResponsesAgentRequest) -> list:\n",
    "        \"\"\"Prepare input with optional system prompt.\"\"\"\n",
    "        input_messages = []\n",
    "        \n",
    "        # Add system prompt if provided\n",
    "        if self.system_prompt:\n",
    "            input_messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.system_prompt\n",
    "            })\n",
    "        \n",
    "        # Add user messages\n",
    "        input_messages.extend([i.model_dump() for i in request.input])\n",
    "        \n",
    "        return input_messages\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        input_messages = self._prepare_input(request)\n",
    "        \n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=input_messages,\n",
    "        )\n",
    "        \n",
    "        return ResponsesAgentResponse(**response.to_dict())\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction with direct passthrough.\"\"\"\n",
    "        input_messages = self._prepare_input(request)\n",
    "        \n",
    "        for event in self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=input_messages,\n",
    "            stream=True,\n",
    "        ):\n",
    "            yield ResponsesAgentStreamEvent(**event.to_dict())\n",
    "\n",
    "\n",
    "# Test streaming\n",
    "print(\"Testing Streaming Responses API Agent...\\n\")\n",
    "streaming_agent = StreamingResponsesAgent(\n",
    "    model=\"gpt-4o\",\n",
    "    system_prompt=\"You are a concise assistant. Keep responses under 50 words.\"\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for event in streaming_agent.predict_stream({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Explain Python decorators.\"}]\n",
    "}):\n",
    "    # Handle different event types\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        if hasattr(event, 'delta') and event.delta:\n",
    "            print(event.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"response.output_item.done\":\n",
    "        print(\"\\n[Complete]\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nâœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tool Calling with Responses API\n",
    "\n",
    "The Responses API has native tool support that integrates seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "class ResponsesAPIToolAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Tool-calling agent using OpenAI Responses API.\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Native Responses API tool support\n",
    "    - Tool execution loop\n",
    "    - Streaming tool calls\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\", tools: list = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.tools = tools or []\n",
    "        self._tool_functions = {}\n",
    "    \n",
    "    def register_tool(self, name: str, spec: dict, func):\n",
    "        \"\"\"Register a tool with its specification and implementation.\"\"\"\n",
    "        self.tools.append(spec)\n",
    "        self._tool_functions[name] = func\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, name: str, arguments: str) -> str:\n",
    "        \"\"\"Execute a tool and return its result.\"\"\"\n",
    "        args = json.loads(arguments)\n",
    "        result = self._tool_functions[name](**args)\n",
    "        return json.dumps(result) if isinstance(result, dict) else str(result)\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction with tool calling.\"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=getattr(request, 'custom_inputs', None)\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming with tool execution loop.\"\"\"\n",
    "        input_messages = [i.model_dump() for i in request.input]\n",
    "        max_iterations = 10\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            # Call Responses API with tools\n",
    "            response = self.client.responses.create(\n",
    "                model=self.model,\n",
    "                input=input_messages,\n",
    "                tools=self.tools if self.tools else None,\n",
    "            )\n",
    "            \n",
    "            # Process each output item\n",
    "            has_tool_call = False\n",
    "            for output_item in response.output:\n",
    "                item_dict = output_item.model_dump(exclude_none=True)\n",
    "                \n",
    "                # Yield the output item\n",
    "                yield ResponsesAgentStreamEvent(\n",
    "                    type=\"response.output_item.done\",\n",
    "                    item=item_dict,\n",
    "                )\n",
    "                \n",
    "                # Check if it's a tool call\n",
    "                if output_item.type == \"function_call\":\n",
    "                    has_tool_call = True\n",
    "                    \n",
    "                    # Execute the tool\n",
    "                    result = self.execute_tool(\n",
    "                        output_item.name,\n",
    "                        output_item.arguments\n",
    "                    )\n",
    "                    \n",
    "                    # Create tool output\n",
    "                    tool_output = {\n",
    "                        \"type\": \"function_call_output\",\n",
    "                        \"call_id\": output_item.call_id,\n",
    "                        \"output\": result,\n",
    "                    }\n",
    "                    \n",
    "                    # Yield tool output\n",
    "                    yield ResponsesAgentStreamEvent(\n",
    "                        type=\"response.output_item.done\",\n",
    "                        item=tool_output,\n",
    "                    )\n",
    "                    \n",
    "                    # Add to conversation for next iteration\n",
    "                    input_messages.append(item_dict)\n",
    "                    input_messages.append(tool_output)\n",
    "            \n",
    "            # If no tool calls, we're done\n",
    "            if not has_tool_call:\n",
    "                break\n",
    "\n",
    "\n",
    "# Create agent with tools\n",
    "print(\"Creating Responses API Tool Agent...\\n\")\n",
    "tool_agent = ResponsesAPIToolAgent(model=\"gpt-4o\")\n",
    "\n",
    "# Register tools\n",
    "tool_agent.register_tool(\n",
    "    name=\"get_weather\",\n",
    "    spec={\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City name\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    func=lambda location: {\"temp\": 72, \"condition\": \"sunny\", \"location\": location}\n",
    ")\n",
    "\n",
    "tool_agent.register_tool(\n",
    "    name=\"calculate\",\n",
    "    spec={\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform a calculation.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Math expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    },\n",
    "    func=lambda expression: {\"result\": eval(expression)}\n",
    ")\n",
    "\n",
    "print(f\"Registered {len(tool_agent.tools)} tools\")\n",
    "print(\"âœ… Tool agent ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tool calling\n",
    "print(\"Testing tool calling...\\n\")\n",
    "print(\"Query: What's the weather in Boston and what is 15 * 23?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event in tool_agent.predict_stream({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What's the weather in Boston and what is 15 * 23?\"}]\n",
    "}):\n",
    "    if event.type == \"response.output_item.done\":\n",
    "        item = event.item\n",
    "        item_type = item.get(\"type\") if isinstance(item, dict) else getattr(item, \"type\", None)\n",
    "        \n",
    "        if item_type == \"function_call\":\n",
    "            name = item.get(\"name\") if isinstance(item, dict) else getattr(item, \"name\", \"\")\n",
    "            args = item.get(\"arguments\") if isinstance(item, dict) else getattr(item, \"arguments\", \"\")\n",
    "            print(f\"\\nðŸ”§ TOOL CALL: {name}\")\n",
    "            print(f\"   Args: {args}\")\n",
    "        elif item_type == \"function_call_output\":\n",
    "            output = item.get(\"output\") if isinstance(item, dict) else getattr(item, \"output\", \"\")\n",
    "            print(f\"ðŸ“¤ RESULT: {output}\")\n",
    "        elif item_type == \"message\":\n",
    "            content = item.get(\"content\") if isinstance(item, dict) else getattr(item, \"content\", [])\n",
    "            if content and len(content) > 0:\n",
    "                text = content[0].get(\"text\") if isinstance(content[0], dict) else getattr(content[0], \"text\", \"\")\n",
    "                print(f\"\\nðŸ’¬ RESPONSE: {text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Tool calling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Production Agent\n",
    "\n",
    "A fully-featured production-ready agent using the Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile responses_api_agent.py\n",
    "\"\"\"Production Responses API agent with full features.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class ProductionResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Production-ready agent using OpenAI Responses API.\n",
    "    \n",
    "    Features:\n",
    "    - Direct Responses API integration\n",
    "    - Streaming with passthrough\n",
    "    - Configurable model and parameters\n",
    "    - Full MLflow tracing\n",
    "    - Custom outputs support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4o\",\n",
    "        system_prompt: str = None,\n",
    "        temperature: float = 1.0,\n",
    "        max_output_tokens: int = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.temperature = temperature\n",
    "        self.max_output_tokens = max_output_tokens\n",
    "    \n",
    "    def _prepare_input(self, request: ResponsesAgentRequest) -> list:\n",
    "        \"\"\"Prepare input messages with optional system prompt.\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        if self.system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.system_prompt\n",
    "            })\n",
    "        \n",
    "        messages.extend([i.model_dump() for i in request.input])\n",
    "        return messages\n",
    "    \n",
    "    def _build_api_kwargs(self, input_messages: list, stream: bool = False) -> dict:\n",
    "        \"\"\"Build API call kwargs.\"\"\"\n",
    "        kwargs = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": input_messages,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        \n",
    "        if stream:\n",
    "            kwargs[\"stream\"] = True\n",
    "        \n",
    "        if self.max_output_tokens:\n",
    "            kwargs[\"max_output_tokens\"] = self.max_output_tokens\n",
    "        \n",
    "        return kwargs\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        input_messages = self._prepare_input(request)\n",
    "        api_kwargs = self._build_api_kwargs(input_messages)\n",
    "        \n",
    "        response = self.client.responses.create(**api_kwargs)\n",
    "        \n",
    "        # Convert response and add custom outputs\n",
    "        result = ResponsesAgentResponse(**response.to_dict())\n",
    "        \n",
    "        # Add custom outputs if present in request\n",
    "        if hasattr(request, 'custom_inputs') and request.custom_inputs:\n",
    "            result.custom_outputs = request.custom_inputs\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction with direct passthrough.\"\"\"\n",
    "        input_messages = self._prepare_input(request)\n",
    "        api_kwargs = self._build_api_kwargs(input_messages, stream=True)\n",
    "        \n",
    "        for event in self.client.responses.create(**api_kwargs):\n",
    "            yield ResponsesAgentStreamEvent(**event.to_dict())\n",
    "\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create and set model\n",
    "agent = ProductionResponsesAgent(\n",
    "    model=\"gpt-4o\",\n",
    "    system_prompt=\"You are a helpful, accurate, and concise AI assistant.\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the agent\n",
    "with mlflow.start_run(run_name=\"responses_api_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"responses_api_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Responses API agent logged!\")\n",
    "    print(f\"Model URI: {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the logged model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "response = loaded_model.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "})\n",
    "\n",
    "print(f\"Loaded model response:\")\n",
    "print(response)\n",
    "print(\"\\nâœ… Model loaded and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "### When to Use Responses API vs ChatCompletions\n",
    "\n",
    "| Use Case | Recommended API |\n",
    "|----------|----------------|\n",
    "| Simple chat | Either works |\n",
    "| Tool calling | Responses API |\n",
    "| Multi-agent | Responses API |\n",
    "| Annotations | Responses API |\n",
    "| Legacy integration | ChatCompletions |\n",
    "| Library compatibility | ChatCompletions |\n",
    "\n",
    "### Code Organization Tips\n",
    "\n",
    "1. **Use direct passthrough** when possible - less code, fewer bugs\n",
    "2. **Handle all event types** in streaming for robustness\n",
    "3. **Add custom_outputs** for metadata tracking\n",
    "4. **Enable autologging** for full observability\n",
    "\n",
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import backoff\n",
    "\n",
    "\n",
    "class RobustResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Agent with proper error handling and retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (openai.RateLimitError, openai.APITimeoutError),\n",
    "        max_tries=3\n",
    "    )\n",
    "    def _call_api(self, input_messages: list):\n",
    "        \"\"\"Call API with retry logic.\"\"\"\n",
    "        return self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=input_messages,\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        try:\n",
    "            response = self._call_api([i.model_dump() for i in request.input])\n",
    "            return ResponsesAgentResponse(**response.to_dict())\n",
    "        \n",
    "        except openai.AuthenticationError:\n",
    "            # Return error response\n",
    "            return ResponsesAgentResponse(\n",
    "                output=[self.create_text_output_item(\n",
    "                    text=\"Authentication failed. Please check your API key.\",\n",
    "                    id=\"error_1\"\n",
    "                )]\n",
    "            )\n",
    "        \n",
    "        except openai.APIError as e:\n",
    "            return ResponsesAgentResponse(\n",
    "                output=[self.create_text_output_item(\n",
    "                    text=f\"API error occurred: {str(e)}\",\n",
    "                    id=\"error_1\"\n",
    "                )]\n",
    "            )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        try:\n",
    "            for event in self.client.responses.create(\n",
    "                model=self.model,\n",
    "                input=[i.model_dump() for i in request.input],\n",
    "                stream=True,\n",
    "            ):\n",
    "                yield ResponsesAgentStreamEvent(**event.to_dict())\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Yield error as output item\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=self.create_text_output_item(\n",
    "                    text=f\"Error during streaming: {str(e)}\",\n",
    "                    id=\"error_1\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"âœ… Robust agent pattern demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. âœ… **Responses API Overview**: Newer, richer API from OpenAI\n",
    "2. âœ… **Direct Integration**: Minimal code with passthrough\n",
    "3. âœ… **Streaming**: Elegant streaming with direct conversion\n",
    "4. âœ… **Tool Calling**: Native tool support\n",
    "5. âœ… **Production Agent**: Complete deployable implementation\n",
    "6. âœ… **Best Practices**: Error handling and when to use\n",
    "\n",
    "### Key Advantages of Responses API:\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Native Format** | ResponsesAgent uses same format |\n",
    "| **Direct Passthrough** | `ResponsesAgentResponse(**response.to_dict())` |\n",
    "| **Rich Output** | Multiple output types supported |\n",
    "| **Tool Support** | Built-in function calling |\n",
    "\n",
    "### Next Steps:\n",
    "- Learn about deployment options\n",
    "- Explore multi-agent patterns\n",
    "- Build production applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
