{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with MLflow ResponsesAgent\n",
    "\n",
    "This notebook demonstrates streaming capabilities in MLflow ResponsesAgent for real-time response generation.\n",
    "\n",
    "## Table of Contents\n",
    "1. Why Use Streaming?\n",
    "2. Streaming Interface Overview\n",
    "3. Basic Text Streaming\n",
    "4. Streaming with Tool Calls\n",
    "5. Advanced Streaming Patterns\n",
    "6. Production Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"Streaming_ResponseAgent\")\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Use Streaming?\n",
    "\n",
    "### Benefits of Streaming:\n",
    "\n",
    "1. **Better User Experience**: Users see responses as they're generated\n",
    "2. **Reduced Perceived Latency**: First token appears quickly\n",
    "3. **Real-time Processing**: Handle long-running operations gracefully\n",
    "4. **Progress Visibility**: Show intermediate steps in agent workflows\n",
    "5. **Resource Efficiency**: Process data incrementally\n",
    "\n",
    "### Streaming vs Non-Streaming:\n",
    "\n",
    "| Aspect | Non-Streaming | Streaming |\n",
    "|--------|---------------|----------|\n",
    "| Response | Wait for complete response | Receive chunks incrementally |\n",
    "| Latency | High perceived latency | Low perceived latency |\n",
    "| Memory | Hold entire response | Process incrementally |\n",
    "| UI Updates | Single update | Continuous updates |\n",
    "| Tool Calls | See after completion | See as they happen |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming Interface Overview\n",
    "\n",
    "### Key Streaming Types:\n",
    "\n",
    "**ResponsesAgentStreamEvent**: The wrapper for all streaming events\n",
    "\n",
    "```python\n",
    "ResponsesAgentStreamEvent(\n",
    "    type=\"response.output_text.delta\",  # Event type\n",
    "    delta={...},                          # For delta events\n",
    "    item={...},                           # For done events\n",
    ")\n",
    "```\n",
    "\n",
    "### Event Types:\n",
    "\n",
    "| Event Type | Purpose |\n",
    "|------------|--------|\n",
    "| `response.output_text.delta` | Text chunk |\n",
    "| `response.output_item.done` | Complete output item |\n",
    "| `response.annotation.added` | Annotation event |\n",
    "\n",
    "### Helper Methods for Streaming:\n",
    "\n",
    "1. `create_text_delta(delta, item_id)` - Create text chunk event\n",
    "2. `create_annotation_added(...)` - Create annotation event\n",
    "3. `create_text_output_item(...)` - Create complete text output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Text Streaming\n",
    "\n",
    "Let's implement basic text streaming in a ResponsesAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicStreamingAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Demonstrates basic text streaming with ResponsesAgent.\n",
    "    \n",
    "    Key concepts:\n",
    "    - yield text deltas with same item_id\n",
    "    - yield final output_item.done to complete\n",
    "    \"\"\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"\n",
    "        Non-streaming prediction.\n",
    "        Collects stream events and returns final response.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Streaming prediction - yields text chunks.\n",
    "        \n",
    "        Pattern:\n",
    "        1. Yield text deltas with consistent item_id\n",
    "        2. Yield output_item.done with complete text\n",
    "        \"\"\"\n",
    "        # Simulate generating text in chunks\n",
    "        text_chunks = [\n",
    "            \"Hello! \",\n",
    "            \"I am a streaming \",\n",
    "            \"ResponsesAgent. \",\n",
    "            \"This demonstrates \",\n",
    "            \"real-time text generation!\"\n",
    "        ]\n",
    "        \n",
    "        full_text = \"\"\n",
    "        item_id = \"msg_stream_1\"\n",
    "        \n",
    "        # Yield each chunk as a delta event\n",
    "        for chunk in text_chunks:\n",
    "            full_text += chunk\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                **self.create_text_delta(delta=chunk, item_id=item_id)\n",
    "            )\n",
    "        \n",
    "        # Yield the complete output item\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\n",
    "                text=full_text,\n",
    "                id=item_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Test basic streaming\n",
    "print(\"Testing basic streaming agent...\\n\")\n",
    "agent = BasicStreamingAgent()\n",
    "\n",
    "# Stream the response\n",
    "print(\"Streaming output:\")\n",
    "print(\"-\" * 40)\n",
    "for event in agent.predict_stream({\"input\": [{\"role\": \"user\", \"content\": \"Hi!\"}]}):\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        print(f\"[DELTA] {event.delta}\", end=\"\", flush=True)\n",
    "    elif event.type == \"response.output_item.done\":\n",
    "        print(f\"\\n\\n[DONE] Complete item received\")\n",
    "        \n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"âœ… Basic streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming with OpenAI Integration\n",
    "\n",
    "Let's integrate real OpenAI streaming with ResponsesAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.types.responses import (\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "\n",
    "\n",
    "class OpenAIStreamingAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    ResponsesAgent that streams from OpenAI ChatCompletions API.\n",
    "    \n",
    "    Uses MLflow's conversion utilities to handle format translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def call_llm_stream(self, messages: list):\n",
    "        \"\"\"Stream responses from OpenAI.\"\"\"\n",
    "        for chunk in self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        ):\n",
    "            yield chunk.to_dict()\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=request.custom_inputs\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Stream from OpenAI and convert to ResponsesAgent format.\n",
    "        \n",
    "        Uses output_to_responses_items_stream for automatic conversion.\n",
    "        \"\"\"\n",
    "        # Convert input to ChatCompletions format\n",
    "        messages = to_chat_completions_input(\n",
    "            [i.model_dump() for i in request.input]\n",
    "        )\n",
    "        \n",
    "        # Stream and convert\n",
    "        yield from output_to_responses_items_stream(\n",
    "            self.call_llm_stream(messages)\n",
    "        )\n",
    "\n",
    "\n",
    "# Enable OpenAI autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Test OpenAI streaming\n",
    "print(\"Testing OpenAI streaming agent...\\n\")\n",
    "openai_agent = OpenAIStreamingAgent(model=\"gpt-4o-mini\")\n",
    "\n",
    "test_request = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Write a haiku about Python programming.\"}]\n",
    "}\n",
    "\n",
    "print(\"Streaming from OpenAI:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for event in openai_agent.predict_stream(test_request):\n",
    "    if hasattr(event, 'delta') and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"response.output_item.done\":\n",
    "        print(\"\\n\\n[Stream Complete]\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"âœ… OpenAI streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming with Tool Calls\n",
    "\n",
    "Streaming tool calls allows users to see agent actions in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class StreamingToolAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Demonstrates streaming with tool calls.\n",
    "    \n",
    "    Pattern:\n",
    "    1. Yield function_call as output_item.done\n",
    "    2. Yield function_call_output as output_item.done\n",
    "    3. Yield final text response\n",
    "    \"\"\"\n",
    "    \n",
    "    def execute_tool(self, name: str, args: dict) -> str:\n",
    "        \"\"\"Execute tool and return result.\"\"\"\n",
    "        if name == \"get_time\":\n",
    "            import datetime\n",
    "            return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        elif name == \"calculate\":\n",
    "            op = args.get(\"operation\")\n",
    "            x, y = args.get(\"x\", 0), args.get(\"y\", 0)\n",
    "            if op == \"add\":\n",
    "                return str(x + y)\n",
    "            elif op == \"multiply\":\n",
    "                return str(x * y)\n",
    "        return \"Unknown tool\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Stream tool calls and results.\n",
    "        \"\"\"\n",
    "        # Simulate: Agent decides to call a tool\n",
    "        tool_call = {\n",
    "            \"name\": \"calculate\",\n",
    "            \"arguments\": json.dumps({\"operation\": \"multiply\", \"x\": 7, \"y\": 6})\n",
    "        }\n",
    "        \n",
    "        # 1. Yield the function call\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_function_call_item(\n",
    "                id=\"fc_1\",\n",
    "                call_id=\"call_123\",\n",
    "                name=tool_call[\"name\"],\n",
    "                arguments=tool_call[\"arguments\"],\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # 2. Execute tool and yield result\n",
    "        args = json.loads(tool_call[\"arguments\"])\n",
    "        result = self.execute_tool(tool_call[\"name\"], args)\n",
    "        \n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_function_call_output_item(\n",
    "                call_id=\"call_123\",\n",
    "                output=result,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # 3. Stream the final text response\n",
    "        response_chunks = [\n",
    "            \"Based on the calculation, \",\n",
    "            \"7 multiplied by 6 \",\n",
    "            f\"equals {result}!\"\n",
    "        ]\n",
    "        \n",
    "        full_text = \"\"\n",
    "        for chunk in response_chunks:\n",
    "            full_text += chunk\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                **self.create_text_delta(delta=chunk, item_id=\"msg_1\")\n",
    "            )\n",
    "        \n",
    "        # 4. Yield complete text output\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\n",
    "                text=full_text,\n",
    "                id=\"msg_1\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Test streaming with tools\n",
    "print(\"Testing streaming with tool calls...\\n\")\n",
    "tool_agent = StreamingToolAgent()\n",
    "\n",
    "print(\"Streaming events:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event in tool_agent.predict_stream({\"input\": [{\"role\": \"user\", \"content\": \"What is 7 times 6?\"}]}):\n",
    "    if event.type == \"response.output_item.done\":\n",
    "        item = event.item\n",
    "        if isinstance(item, dict):\n",
    "            item_type = item.get(\"type\")\n",
    "        else:\n",
    "            item_type = getattr(item, \"type\", \"unknown\")\n",
    "        \n",
    "        if item_type == \"function_call\":\n",
    "            print(f\"\\nðŸ”§ TOOL CALL: {item.get('name', getattr(item, 'name', 'unknown'))}\")\n",
    "            print(f\"   Arguments: {item.get('arguments', getattr(item, 'arguments', ''))}\")\n",
    "        elif item_type == \"function_call_output\":\n",
    "            print(f\"\\nðŸ“¤ TOOL RESULT: {item.get('output', getattr(item, 'output', ''))}\")\n",
    "        elif item_type == \"message\":\n",
    "            print(f\"\\nðŸ’¬ FINAL MESSAGE (complete)\")\n",
    "    elif event.type == \"response.output_text.delta\":\n",
    "        print(f\"[delta] {event.delta}\", end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Tool streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reasoning Items in Streaming\n",
    "\n",
    "ResponsesAgent supports streaming chain-of-thought reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningStreamAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Demonstrates streaming with reasoning/chain-of-thought.\n",
    "    \n",
    "    Shows:\n",
    "    - Reasoning items for transparency\n",
    "    - Multi-step thinking\n",
    "    - Final answer after reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Stream reasoning followed by answer.\n",
    "        \"\"\"\n",
    "        # Stream reasoning steps\n",
    "        reasoning_steps = [\n",
    "            \"Let me analyze this step by step...\\n\",\n",
    "            \"1. First, I'll consider the key factors\\n\",\n",
    "            \"2. Then, I'll evaluate the options\\n\",\n",
    "            \"3. Finally, I'll provide my recommendation\\n\",\n",
    "        ]\n",
    "        \n",
    "        full_reasoning = \"\"\n",
    "        for step in reasoning_steps:\n",
    "            full_reasoning += step\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                **self.create_text_delta(delta=step, item_id=\"reasoning_1\")\n",
    "            )\n",
    "            time.sleep(0.2)  # Simulate thinking\n",
    "        \n",
    "        # Yield complete reasoning item\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_reasoning_item(\n",
    "                text=full_reasoning,\n",
    "                id=\"reasoning_1\",\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Stream final answer\n",
    "        answer_chunks = [\n",
    "            \"Based on my analysis, \",\n",
    "            \"I recommend proceeding with option A \",\n",
    "            \"because it offers the best balance \",\n",
    "            \"of cost and performance.\"\n",
    "        ]\n",
    "        \n",
    "        full_answer = \"\"\n",
    "        for chunk in answer_chunks:\n",
    "            full_answer += chunk\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                **self.create_text_delta(delta=chunk, item_id=\"answer_1\")\n",
    "            )\n",
    "        \n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\n",
    "                text=full_answer,\n",
    "                id=\"answer_1\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Test reasoning streaming\n",
    "print(\"Testing reasoning stream agent...\\n\")\n",
    "reasoning_agent = ReasoningStreamAgent()\n",
    "\n",
    "print(\"Streaming with reasoning:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "current_section = None\n",
    "for event in reasoning_agent.predict_stream({\"input\": [{\"role\": \"user\", \"content\": \"Should I use Python or JavaScript?\"}]}):\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"response.output_item.done\":\n",
    "        item = event.item\n",
    "        if isinstance(item, dict):\n",
    "            item_type = item.get(\"type\")\n",
    "        else:\n",
    "            item_type = getattr(item, \"type\", \"unknown\")\n",
    "        \n",
    "        if item_type == \"reasoning\":\n",
    "            print(\"\\n\\n[ðŸ’­ Reasoning Complete]\\n\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"[ðŸ“ Now generating answer...]\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nâœ… Reasoning streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Streaming Agent Example\n",
    "\n",
    "A production-ready streaming agent combining all patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streaming_agent.py\n",
    "\"\"\"Production streaming agent with ResponsesAgent.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class StreamingChatAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Production-ready streaming chat agent.\n",
    "    \n",
    "    Features:\n",
    "    - Real-time text streaming from OpenAI\n",
    "    - Automatic format conversion\n",
    "    - Full MLflow tracing\n",
    "    - Custom output support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt or \"You are a helpful assistant.\"\n",
    "    \n",
    "    def stream_from_llm(self, messages: list) -> Generator:\n",
    "        \"\"\"Stream responses from OpenAI.\"\"\"\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            yield chunk.to_dict()\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=request.custom_inputs\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Stream response from LLM.\"\"\"\n",
    "        # Prepare messages with system prompt\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        \n",
    "        # Stream and convert\n",
    "        yield from output_to_responses_items_stream(\n",
    "            self.stream_from_llm(messages)\n",
    "        )\n",
    "\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create and set model\n",
    "agent = StreamingChatAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=\"You are a helpful AI assistant. Be concise and friendly.\"\n",
    ")\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the streaming agent\n",
    "with mlflow.start_run(run_name=\"streaming_chat_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"streaming_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Streaming agent logged!\")\n",
    "    print(f\"Model URI: {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Considerations\n",
    "\n",
    "### Streaming Best Practices:\n",
    "\n",
    "1. **Always yield `output_item.done`**: Clients need complete items\n",
    "2. **Consistent `item_id`**: All deltas for one item share the same ID\n",
    "3. **Handle disconnections**: Clean up resources on client disconnect\n",
    "4. **Timeout handling**: Set appropriate timeouts for long streams\n",
    "\n",
    "### Error Handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustStreamingAgent(ResponsesAgent):\n",
    "    \"\"\"Agent with proper error handling for streaming.\"\"\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        try:\n",
    "            # Your streaming logic here\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                **self.create_text_delta(delta=\"Processing...\", item_id=\"msg_1\")\n",
    "            )\n",
    "            \n",
    "            # Simulate work\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=self.create_text_output_item(\n",
    "                    text=\"Processing complete!\",\n",
    "                    id=\"msg_1\",\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Yield error as a proper output item\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=self.create_text_output_item(\n",
    "                    text=f\"Error occurred: {str(e)}\",\n",
    "                    id=\"error_msg\",\n",
    "                ),\n",
    "            )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs)\n",
    "\n",
    "\n",
    "print(\"âœ… Robust streaming agent pattern demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. âœ… **Why streaming matters**: Better UX and real-time visibility\n",
    "2. âœ… **Streaming interface**: Events, deltas, and output items\n",
    "3. âœ… **Basic text streaming**: Yield deltas with consistent IDs\n",
    "4. âœ… **OpenAI integration**: Using conversion utilities\n",
    "5. âœ… **Tool call streaming**: Real-time tool execution visibility\n",
    "6. âœ… **Reasoning streaming**: Chain-of-thought transparency\n",
    "7. âœ… **Error handling**: Robust production patterns\n",
    "\n",
    "### Key Helper Methods:\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `create_text_delta()` | Stream text chunks |\n",
    "| `create_text_output_item()` | Complete text output |\n",
    "| `create_function_call_item()` | Tool invocation |\n",
    "| `create_function_call_output_item()` | Tool result |\n",
    "| `create_reasoning_item()` | Chain-of-thought |\n",
    "| `create_annotation_added()` | Annotations |\n",
    "\n",
    "### Next Steps:\n",
    "- Explore OpenAI Responses API integration\n",
    "- Learn about deployment options\n",
    "- Build production streaming applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
