{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving and Deployment with MLflow ResponsesAgent\n",
    "\n",
    "This notebook covers deployment options for MLflow ResponsesAgent models.\n",
    "\n",
    "## Table of Contents\n",
    "1. Deployment Overview\n",
    "2. Local Model Serving\n",
    "3. Docker Deployment\n",
    "4. Databricks Model Serving\n",
    "5. Testing Deployed Models\n",
    "6. Production Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"Deployment_ResponseAgent\")\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deployment Overview\n",
    "\n",
    "### Deployment Options for ResponsesAgent\n",
    "\n",
    "| Option | Best For | Complexity | Scaling |\n",
    "|--------|----------|------------|--------|\n",
    "| **Local Serving** | Development, testing | Low | None |\n",
    "| **Docker** | Containerized deployments | Medium | Manual |\n",
    "| **Databricks** | Production, enterprise | Low | Auto |\n",
    "| **Kubernetes** | Custom infrastructure | High | Auto |\n",
    "\n",
    "### ResponsesAgent Deployment Flow\n",
    "\n",
    "```\n",
    "1. Create Agent → 2. Log to MLflow → 3. Choose Deployment → 4. Serve\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Models from Code**: Agent code is logged as a Python file\n",
    "2. **Auto Signature**: Input/output schemas are auto-inferred\n",
    "3. **Task Metadata**: `{\"task\": \"agent/v1/responses\"}` marks it as ResponsesAgent\n",
    "4. **Dependencies**: pip requirements are bundled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Deployable Agent\n",
    "\n",
    "Let's create a production-ready agent for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deployable_agent.py\n",
    "\"\"\"Production-ready ResponsesAgent for deployment.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class DeployableAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    A production-ready agent demonstrating best practices.\n",
    "    \n",
    "    Features:\n",
    "    - OpenAI integration\n",
    "    - Streaming support\n",
    "    - Configurable parameters\n",
    "    - Health check support\n",
    "    - Error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        system_prompt: str = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt or (\n",
    "            \"You are a helpful AI assistant. \"\n",
    "            \"Provide clear, accurate, and concise responses.\"\n",
    "        )\n",
    "    \n",
    "    def _prepare_messages(self, request: ResponsesAgentRequest) -> list:\n",
    "        \"\"\"Prepare messages with system prompt.\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        return messages\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        try:\n",
    "            messages = self._prepare_messages(request)\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            return ResponsesAgentResponse(\n",
    "                output=[\n",
    "                    self.create_text_output_item(\n",
    "                        text=response.choices[0].message.content,\n",
    "                        id=\"msg_1\",\n",
    "                    )\n",
    "                ],\n",
    "                custom_outputs={\n",
    "                    \"model\": self.model,\n",
    "                    \"usage\": {\n",
    "                        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                        \"completion_tokens\": response.usage.completion_tokens,\n",
    "                        \"total_tokens\": response.usage.total_tokens,\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ResponsesAgentResponse(\n",
    "                output=[\n",
    "                    self.create_text_output_item(\n",
    "                        text=f\"Error: {str(e)}\",\n",
    "                        id=\"error_1\",\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction.\"\"\"\n",
    "        try:\n",
    "            messages = self._prepare_messages(request)\n",
    "            \n",
    "            stream = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            yield from output_to_responses_items_stream(\n",
    "                chunk.to_dict() for chunk in stream\n",
    "            )\n",
    "        except Exception as e:\n",
    "            yield ResponsesAgentStreamEvent(\n",
    "                type=\"response.output_item.done\",\n",
    "                item=self.create_text_output_item(\n",
    "                    text=f\"Error: {str(e)}\",\n",
    "                    id=\"error_1\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create and set model\n",
    "agent = DeployableAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=\"You are a helpful assistant for technical questions.\",\n",
    ")\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the model\n",
    "with mlflow.start_run(run_name=\"deployable_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"deployable_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.0.0\",\n",
    "            \"openai>=1.0.0\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Store run ID and model URI for deployment\n",
    "    run_id = run.info.run_id\n",
    "    model_uri = model_info.model_uri\n",
    "    \n",
    "    print(f\"✅ Model logged successfully!\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Model URI: {model_uri}\")\n",
    "    print(f\"\\nModel metadata: {model_info.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Model Serving\n",
    "\n",
    "The simplest way to serve a ResponsesAgent locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the command to start local serving\n",
    "print(\"=\" * 70)\n",
    "print(\"LOCAL SERVING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nTo serve your model locally, run this command in a terminal:\\n\")\n",
    "print(f\"  mlflow models serve -m {model_uri} -p 5001\\n\")\n",
    "print(\"Or with the run ID:\")\n",
    "print(f\"  mlflow models serve -m runs:/{run_id}/agent -p 5001\\n\")\n",
    "print(\"Options:\")\n",
    "print(\"  -p, --port     : Port to serve on (default: 5000)\")\n",
    "print(\"  --host         : Host to bind to (default: 127.0.0.1)\")\n",
    "print(\"  --env-manager  : Environment manager (local, conda, virtualenv)\")\n",
    "print(\"  --no-conda     : Skip conda environment creation\")\n",
    "print(\"\\nExample with all options:\")\n",
    "print(f\"  mlflow models serve -m {model_uri} -p 5001 --host 0.0.0.0 --no-conda\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local serving (run this after starting the server)\n",
    "def test_local_server(port: int = 5001):\n",
    "    \"\"\"\n",
    "    Test the locally served model.\n",
    "    \n",
    "    Run this after starting: mlflow models serve -m <model_uri> -p 5001\n",
    "    \"\"\"\n",
    "    url = f\"http://localhost:{port}/invocations\"\n",
    "    \n",
    "    payload = {\n",
    "        \"input\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ Server response:\")\n",
    "            print(json.dumps(result, indent=2))\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Could not connect to server.\")\n",
    "        print(f\"   Make sure the server is running on port {port}\")\n",
    "        print(f\"   Command: mlflow models serve -m {model_uri} -p {port}\")\n",
    "\n",
    "\n",
    "# Uncomment to test (after starting server)\n",
    "# test_local_server(5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Docker Deployment\n",
    "\n",
    "Build a Docker image for containerized deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker deployment commands\n",
    "print(\"=\" * 70)\n",
    "print(\"DOCKER DEPLOYMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Build Docker image:\")\n",
    "print(f\"   mlflow models build-docker -m {model_uri} -n my-responses-agent\\n\")\n",
    "\n",
    "print(\"2. Run the container:\")\n",
    "print(\"   docker run -p 5001:8080 \\\\\")\n",
    "print(\"     -e OPENAI_API_KEY=$OPENAI_API_KEY \\\\\")\n",
    "print(\"     my-responses-agent\\n\")\n",
    "\n",
    "print(\"3. Test the container:\")\n",
    "print(\"   curl -X POST http://localhost:5001/invocations \\\\\")\n",
    "print(\"     -H 'Content-Type: application/json' \\\\\")\n",
    "print(\"     -d '{\\\"input\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]}'\\n\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Dockerfile (alternative approach)\n",
    "dockerfile_content = f'''# MLflow ResponsesAgent Docker Image\n",
    "# Built from: {model_uri}\n",
    "\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    mlflow>=3.0.0 \\\\\n",
    "    openai>=1.0.0 \\\\\n",
    "    pydantic>=2.0.0 \\\\\n",
    "    gunicorn\n",
    "\n",
    "# Copy model artifacts (you would need to export these first)\n",
    "# COPY model /app/model\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Environment variables\n",
    "ENV MLFLOW_MODEL_URI=/app/model\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8080/health || exit 1\n",
    "\n",
    "# Start server\n",
    "CMD [\"mlflow\", \"models\", \"serve\", \"-m\", \"/app/model\", \"-h\", \"0.0.0.0\", \"-p\", \"8080\"]\n",
    "'''\n",
    "\n",
    "print(\"Example Dockerfile for custom deployments:\")\n",
    "print(\"-\" * 50)\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Databricks Model Serving\n",
    "\n",
    "Deploy to Databricks for managed, scalable serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks deployment guide\n",
    "print(\"=\" * 70)\n",
    "print(\"DATABRICKS MODEL SERVING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Prerequisites:\n",
    "- Databricks workspace with Unity Catalog\n",
    "- Model registered in Unity Catalog\n",
    "- Appropriate permissions\n",
    "\n",
    "Step 1: Register the model in Unity Catalog\n",
    "\"\"\")\n",
    "\n",
    "databricks_register_code = f'''\n",
    "import mlflow\n",
    "\n",
    "# Set the registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Register the model\n",
    "mlflow.register_model(\n",
    "    model_uri=\"{model_uri}\",\n",
    "    name=\"catalog.schema.my_responses_agent\"\n",
    ")\n",
    "'''\n",
    "print(databricks_register_code)\n",
    "\n",
    "print(\"\\nStep 2: Create serving endpoint\")\n",
    "\n",
    "databricks_serve_code = '''\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "endpoint = client.create_endpoint(\n",
    "    name=\"my-responses-agent-endpoint\",\n",
    "    config={\n",
    "        \"served_entities\": [\n",
    "            {\n",
    "                \"name\": \"agent-entity\",\n",
    "                \"entity_name\": \"catalog.schema.my_responses_agent\",\n",
    "                \"entity_version\": \"1\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "            }\n",
    "        ],\n",
    "        \"traffic_config\": {\n",
    "            \"routes\": [\n",
    "                {\n",
    "                    \"served_model_name\": \"agent-entity-1\",\n",
    "                    \"traffic_percentage\": 100\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Endpoint created: {endpoint}\")\n",
    "'''\n",
    "print(databricks_serve_code)\n",
    "\n",
    "print(\"\\nStep 3: Query the endpoint\")\n",
    "\n",
    "databricks_query_code = '''\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "response = client.predict(\n",
    "    endpoint=\"my-responses-agent-endpoint\",\n",
    "    inputs={\n",
    "        \"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "print(databricks_query_code)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Deployed Models\n",
    "\n",
    "Various ways to test your deployed agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python client for testing\n",
    "class AgentClient:\n",
    "    \"\"\"\n",
    "    Simple client for testing deployed ResponsesAgent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "    \n",
    "    def chat(self, message: str, history: list = None) -> dict:\n",
    "        \"\"\"\n",
    "        Send a chat message to the agent.\n",
    "        \n",
    "        Args:\n",
    "            message: User message\n",
    "            history: Optional conversation history\n",
    "            \n",
    "        Returns:\n",
    "            Agent response\n",
    "        \"\"\"\n",
    "        input_messages = history or []\n",
    "        input_messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        payload = {\"input\": input_messages}\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/invocations\",\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def health_check(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the server is healthy.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{self.base_url}/health\",\n",
    "                timeout=5\n",
    "            )\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "# Usage example\n",
    "print(\"Agent Client Usage:\")\n",
    "print(\"-\" * 40)\n",
    "print('''\n",
    "# Create client\n",
    "client = AgentClient(\"http://localhost:5001\")\n",
    "\n",
    "# Check health\n",
    "if client.health_check():\n",
    "    print(\"Server is healthy!\")\n",
    "\n",
    "# Send message\n",
    "response = client.chat(\"What is Python?\")\n",
    "print(response)\n",
    "\n",
    "# Multi-turn conversation\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a programming language...\"},\n",
    "]\n",
    "response = client.chat(\"What about JavaScript?\", history=history)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cURL examples for testing\n",
    "print(\"=\" * 70)\n",
    "print(\"CURL TESTING EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "curl_examples = '''\n",
    "# Basic request\n",
    "curl -X POST http://localhost:5001/invocations \\\\\n",
    "  -H 'Content-Type: application/json' \\\\\n",
    "  -d '{\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]\n",
    "  }'\n",
    "\n",
    "# Multi-turn conversation\n",
    "curl -X POST http://localhost:5001/invocations \\\\\n",
    "  -H 'Content-Type: application/json' \\\\\n",
    "  -d '{\n",
    "    \"input\": [\n",
    "      {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Python is a programming language.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What are its main uses?\"}\n",
    "    ]\n",
    "  }'\n",
    "\n",
    "# With context and custom inputs\n",
    "curl -X POST http://localhost:5001/invocations \\\\\n",
    "  -H 'Content-Type: application/json' \\\\\n",
    "  -d '{\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    \"context\": {\"user_id\": \"123\", \"session_id\": \"abc\"},\n",
    "    \"custom_inputs\": {\"request_type\": \"greeting\"}\n",
    "  }'\n",
    "\n",
    "# Health check\n",
    "curl http://localhost:5001/health\n",
    "'''\n",
    "\n",
    "print(curl_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Best Practices\n",
    "\n",
    "### Configuration and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables for production\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION ENVIRONMENT VARIABLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "env_vars = '''\n",
    "# Required\n",
    "OPENAI_API_KEY=sk-...                    # Your API key\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI=...                  # Tracking server URI\n",
    "MLFLOW_EXPERIMENT_NAME=production        # Experiment name\n",
    "\n",
    "# Timeout Configuration (for long-running agents)\n",
    "MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT=120    # Single request timeout (seconds)\n",
    "MLFLOW_DEPLOYMENT_PREDICT_TOTAL_TIMEOUT=600  # Total retry timeout\n",
    "\n",
    "# Logging\n",
    "MLFLOW_ENABLE_TRACING=true               # Enable MLflow tracing\n",
    "LOG_LEVEL=INFO                           # Application log level\n",
    "\n",
    "# Performance\n",
    "GUNICORN_WORKERS=4                       # Number of worker processes\n",
    "GUNICORN_TIMEOUT=120                     # Worker timeout\n",
    "'''\n",
    "\n",
    "print(env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production monitoring\n",
    "print(\"=\" * 70)\n",
    "print(\"MONITORING AND OBSERVABILITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "monitoring_tips = '''\n",
    "1. MLflow Tracing\n",
    "   - Enable: mlflow.openai.autolog() or @mlflow.trace\n",
    "   - View in MLflow UI -> Traces tab\n",
    "   - Includes: latency, tokens, errors\n",
    "\n",
    "2. Health Checks\n",
    "   - Endpoint: GET /health\n",
    "   - Use with load balancers and Kubernetes\n",
    "\n",
    "3. Logging\n",
    "   - MLflow logs predictions automatically\n",
    "   - Add custom logging for business metrics\n",
    "\n",
    "4. Metrics to Monitor\n",
    "   - Request latency (p50, p95, p99)\n",
    "   - Token usage per request\n",
    "   - Error rate\n",
    "   - Requests per second\n",
    "   - Cost per request\n",
    "\n",
    "5. Alerting\n",
    "   - High error rate (>1%)\n",
    "   - Latency spikes (>10s p95)\n",
    "   - Unusual token usage patterns\n",
    "   - API key expiration\n",
    "'''\n",
    "\n",
    "print(monitoring_tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security best practices\n",
    "print(\"=\" * 70)\n",
    "print(\"SECURITY BEST PRACTICES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "security_tips = '''\n",
    "1. API Keys\n",
    "   - Never commit API keys to version control\n",
    "   - Use environment variables or secrets managers\n",
    "   - Rotate keys regularly\n",
    "   - Use separate keys for dev/staging/production\n",
    "\n",
    "2. Network Security\n",
    "   - Use HTTPS in production\n",
    "   - Implement rate limiting\n",
    "   - Use authentication for endpoints\n",
    "   - Restrict access by IP if possible\n",
    "\n",
    "3. Input Validation\n",
    "   - Validate all user inputs\n",
    "   - Limit input/output sizes\n",
    "   - Sanitize conversation history\n",
    "\n",
    "4. Data Privacy\n",
    "   - Log only necessary information\n",
    "   - Implement data retention policies\n",
    "   - Consider PII handling requirements\n",
    "\n",
    "5. Model Security\n",
    "   - Version control all model artifacts\n",
    "   - Audit model changes\n",
    "   - Test before deploying to production\n",
    "'''\n",
    "\n",
    "print(security_tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting\n",
    "\n",
    "Common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMMON ISSUES AND SOLUTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "troubleshooting = '''\n",
    "Issue: \"ModuleNotFoundError\" when serving\n",
    "Solution: Ensure all dependencies are in pip_requirements\n",
    "  mlflow.pyfunc.log_model(\n",
    "      ...,\n",
    "      pip_requirements=[\"mlflow\", \"openai\", \"pydantic>=2.0.0\"]\n",
    "  )\n",
    "\n",
    "---\n",
    "\n",
    "Issue: \"Timeout\" on long-running predictions\n",
    "Solution: Increase timeout environment variables\n",
    "  export MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT=900  # 15 minutes\n",
    "  export MLFLOW_DEPLOYMENT_PREDICT_TOTAL_TIMEOUT=1200\n",
    "\n",
    "---\n",
    "\n",
    "Issue: \"OPENAI_API_KEY not found\"\n",
    "Solution: Set environment variable before serving\n",
    "  export OPENAI_API_KEY=sk-...\n",
    "  mlflow models serve -m <model_uri>\n",
    "\n",
    "---\n",
    "\n",
    "Issue: Schema validation errors\n",
    "Solution: Ensure request matches ResponsesAgentRequest schema\n",
    "  {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"...\"}],  # Required\n",
    "    \"context\": {...},  # Optional\n",
    "    \"custom_inputs\": {...}  # Optional\n",
    "  }\n",
    "\n",
    "---\n",
    "\n",
    "Issue: Missing traces in MLflow UI\n",
    "Solution: Enable autologging in your agent code\n",
    "  mlflow.openai.autolog()\n",
    "  # or use @mlflow.trace decorator\n",
    "\n",
    "---\n",
    "\n",
    "Issue: Docker container won't start\n",
    "Solution: Check logs and ensure port is exposed\n",
    "  docker logs <container_id>\n",
    "  docker run -p 5001:8080 -e OPENAI_API_KEY=... <image>\n",
    "'''\n",
    "\n",
    "print(troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Deployment Options:\n",
    "\n",
    "| Method | Command | Best For |\n",
    "|--------|---------|----------|\n",
    "| **Local** | `mlflow models serve -m <uri>` | Development |\n",
    "| **Docker** | `mlflow models build-docker` | Containerized |\n",
    "| **Databricks** | `client.create_endpoint()` | Production |\n",
    "\n",
    "### Key Commands:\n",
    "\n",
    "```bash\n",
    "# Local serving\n",
    "mlflow models serve -m runs:/<run_id>/agent -p 5001\n",
    "\n",
    "# Build Docker image\n",
    "mlflow models build-docker -m runs:/<run_id>/agent -n my-agent\n",
    "\n",
    "# Run Docker container\n",
    "docker run -p 5001:8080 -e OPENAI_API_KEY=$OPENAI_API_KEY my-agent\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. ✅ Always test locally before deploying\n",
    "2. ✅ Use environment variables for secrets\n",
    "3. ✅ Enable tracing for observability\n",
    "4. ✅ Implement proper error handling\n",
    "5. ✅ Monitor key metrics in production\n",
    "6. ✅ Set appropriate timeouts\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy your agent to your preferred platform\n",
    "- Set up monitoring and alerting\n",
    "- Implement CI/CD for model updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
