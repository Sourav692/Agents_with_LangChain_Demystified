{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResponsesAgent with ChatCompletions API\n",
    "\n",
    "This notebook demonstrates integrating OpenAI's ChatCompletions API with MLflow ResponsesAgent.\n",
    "\n",
    "## Table of Contents\n",
    "1. ChatCompletions vs Responses API\n",
    "2. Basic ChatCompletions Integration\n",
    "3. Format Conversion Utilities\n",
    "4. Multi-turn Conversations\n",
    "5. System Prompts and Context\n",
    "6. Production Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"ChatCompletions_ResponseAgent\")\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatCompletions vs Responses API\n",
    "\n",
    "OpenAI offers two main APIs for conversational AI:\n",
    "\n",
    "### ChatCompletions API\n",
    "- **Mature and stable**: Available since GPT-3.5\n",
    "- **Wide support**: Most libraries and tools support it\n",
    "- **Simple format**: messages with role and content\n",
    "- **Streaming**: Supports token-by-token streaming\n",
    "\n",
    "```python\n",
    "# ChatCompletions format\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Responses API (Newer)\n",
    "- **Richer output**: Supports multiple output types\n",
    "- **Built-in tools**: Native tool/function support\n",
    "- **Annotations**: Link citations and references\n",
    "- **Multi-agent**: Better support for agent workflows\n",
    "\n",
    "```python\n",
    "# Responses API format\n",
    "{\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    \"output\": [{\"type\": \"message\", \"content\": [...]}]\n",
    "}\n",
    "```\n",
    "\n",
    "### MLflow ResponsesAgent\n",
    "- Uses **Responses API format** for I/O\n",
    "- Provides **conversion utilities** for ChatCompletions\n",
    "- Enables **framework flexibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic ChatCompletions Integration\n",
    "\n",
    "Let's build a ResponsesAgent that uses OpenAI's ChatCompletions API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatCompletionsAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    ResponsesAgent that uses OpenAI ChatCompletions API.\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Converting ResponsesAgent input to ChatCompletions format\n",
    "    - Calling ChatCompletions API\n",
    "    - Converting response back to ResponsesAgent format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.LLM)\n",
    "    def call_llm(self, messages: list) -> dict:\n",
    "        \"\"\"\n",
    "        Call OpenAI ChatCompletions API.\n",
    "        \n",
    "        Returns raw response for conversion.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"\n",
    "        Main prediction method.\n",
    "        \n",
    "        Steps:\n",
    "        1. Convert ResponsesAgent input to ChatCompletions format\n",
    "        2. Call ChatCompletions API\n",
    "        3. Convert response to ResponsesAgent format\n",
    "        \"\"\"\n",
    "        # Step 1: Convert input format\n",
    "        messages = to_chat_completions_input(\n",
    "            [i.model_dump() for i in request.input]\n",
    "        )\n",
    "        \n",
    "        # Step 2: Call LLM\n",
    "        response = self.call_llm(messages)\n",
    "        \n",
    "        # Step 3: Convert response to ResponsesAgent format\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=[\n",
    "                self.create_text_output_item(\n",
    "                    text=assistant_message,\n",
    "                    id=\"msg_1\",\n",
    "                )\n",
    "            ],\n",
    "            custom_outputs={\n",
    "                \"model\": self.model,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Streaming prediction using ChatCompletions stream.\n",
    "        \"\"\"\n",
    "        # Convert input format\n",
    "        messages = to_chat_completions_input(\n",
    "            [i.model_dump() for i in request.input]\n",
    "        )\n",
    "        \n",
    "        # Stream from ChatCompletions\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        # Convert stream to ResponsesAgent format\n",
    "        yield from output_to_responses_items_stream(\n",
    "            chunk.to_dict() for chunk in stream\n",
    "        )\n",
    "\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Test the agent\n",
    "print(\"Testing ChatCompletions agent...\\n\")\n",
    "agent = ChatCompletionsAgent(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = agent.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is MLflow in one sentence?\"}]\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.output[0].content[0]['text']}\")\n",
    "print(f\"\\nUsage: {response.custom_outputs['usage']}\")\n",
    "print(\"\\n✅ ChatCompletions agent working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Format Conversion Utilities\n",
    "\n",
    "MLflow provides utilities for converting between formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate to_chat_completions_input\n",
    "print(\"Format Conversion: to_chat_completions_input()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ResponsesAgent input format\n",
    "responses_input = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "]\n",
    "\n",
    "print(\"Input (ResponsesAgent format):\")\n",
    "for msg in responses_input:\n",
    "    print(f\"  {msg}\")\n",
    "\n",
    "# Convert to ChatCompletions format\n",
    "cc_messages = to_chat_completions_input(responses_input)\n",
    "\n",
    "print(\"\\nOutput (ChatCompletions format):\")\n",
    "for msg in cc_messages:\n",
    "    print(f\"  {msg}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ The formats are compatible for basic messages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate output_to_responses_items_stream\n",
    "print(\"Format Conversion: output_to_responses_items_stream()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate ChatCompletions streaming chunks\n",
    "fake_chunks = [\n",
    "    {\"choices\": [{\"delta\": {\"role\": \"assistant\"}, \"index\": 0}]},\n",
    "    {\"choices\": [{\"delta\": {\"content\": \"Hello\"}, \"index\": 0}]},\n",
    "    {\"choices\": [{\"delta\": {\"content\": \" world\"}, \"index\": 0}]},\n",
    "    {\"choices\": [{\"delta\": {\"content\": \"!\"}, \"index\": 0}]},\n",
    "    {\"choices\": [{\"delta\": {}, \"finish_reason\": \"stop\", \"index\": 0}]},\n",
    "]\n",
    "\n",
    "print(\"Converting ChatCompletions stream to ResponsesAgent events:\")\n",
    "print()\n",
    "\n",
    "for event in output_to_responses_items_stream(iter(fake_chunks)):\n",
    "    print(f\"Event type: {event.type}\")\n",
    "    if hasattr(event, 'delta') and event.delta:\n",
    "        print(f\"  Delta: '{event.delta}'\")\n",
    "    if hasattr(event, 'item') and event.item:\n",
    "        print(f\"  Item: {event.item}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Stream conversion demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-turn Conversations\n",
    "\n",
    "Handling conversation history with ChatCompletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Agent that handles multi-turn conversations.\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Conversation history in requests\n",
    "    - System prompt injection\n",
    "    - Context preservation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt or \"You are a helpful assistant. Be concise.\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        # Prepare messages with system prompt\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        \n",
    "        # Call LLM\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=[\n",
    "                self.create_text_output_item(\n",
    "                    text=response.choices[0].message.content,\n",
    "                    id=\"msg_1\",\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        \n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        yield from output_to_responses_items_stream(\n",
    "            chunk.to_dict() for chunk in stream\n",
    "        )\n",
    "\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"Testing multi-turn conversation...\\n\")\n",
    "conv_agent = ConversationalAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=\"You are a Python expert. Be concise and use code examples.\"\n",
    ")\n",
    "\n",
    "# Turn 1\n",
    "print(\"Turn 1:\")\n",
    "print(\"-\" * 40)\n",
    "response1 = conv_agent.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is a list comprehension?\"}]\n",
    "})\n",
    "print(f\"User: What is a list comprehension?\")\n",
    "print(f\"Agent: {response1.output[0].content[0]['text'][:200]}...\")\n",
    "\n",
    "# Turn 2 - with history\n",
    "print(\"\\nTurn 2:\")\n",
    "print(\"-\" * 40)\n",
    "response2 = conv_agent.predict({\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is a list comprehension?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response1.output[0].content[0]['text']},\n",
    "        {\"role\": \"user\", \"content\": \"Can you show a nested example?\"}\n",
    "    ]\n",
    "})\n",
    "print(f\"User: Can you show a nested example?\")\n",
    "print(f\"Agent: {response2.output[0].content[0]['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n✅ Multi-turn conversation working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced: Context and Custom Inputs\n",
    "\n",
    "Using context data and custom inputs with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Agent that uses context data for personalization.\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Using request.context for metadata\n",
    "    - Using request.custom_inputs for custom data\n",
    "    - Passing data through custom_outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def build_system_prompt(self, context: dict) -> str:\n",
    "        \"\"\"\n",
    "        Build dynamic system prompt based on context.\n",
    "        \"\"\"\n",
    "        base_prompt = \"You are a helpful assistant.\"\n",
    "        \n",
    "        if context:\n",
    "            user_name = context.get(\"user_name\", \"User\")\n",
    "            preferences = context.get(\"preferences\", {})\n",
    "            \n",
    "            base_prompt += f\"\\n\\nUser's name: {user_name}\"\n",
    "            \n",
    "            if preferences.get(\"concise\"):\n",
    "                base_prompt += \"\\nBe very concise in your responses.\"\n",
    "            \n",
    "            if preferences.get(\"technical_level\"):\n",
    "                level = preferences[\"technical_level\"]\n",
    "                base_prompt += f\"\\nAdjust technical depth to: {level}\"\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        # Extract context\n",
    "        context = getattr(request, 'context', {}) or {}\n",
    "        custom_inputs = getattr(request, 'custom_inputs', {}) or {}\n",
    "        \n",
    "        # Build dynamic system prompt\n",
    "        system_prompt = self.build_system_prompt(context)\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        \n",
    "        # Call LLM\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=[\n",
    "                self.create_text_output_item(\n",
    "                    text=response.choices[0].message.content,\n",
    "                    id=\"msg_1\",\n",
    "                )\n",
    "            ],\n",
    "            custom_outputs={\n",
    "                \"context_used\": context,\n",
    "                \"custom_inputs_received\": custom_inputs,\n",
    "                \"model\": self.model,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        context = getattr(request, 'context', {}) or {}\n",
    "        system_prompt = self.build_system_prompt(context)\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        \n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        yield from output_to_responses_items_stream(\n",
    "            chunk.to_dict() for chunk in stream\n",
    "        )\n",
    "\n",
    "\n",
    "# Test context-aware agent\n",
    "print(\"Testing context-aware agent...\\n\")\n",
    "context_agent = ContextAwareAgent(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Request with context\n",
    "response = context_agent.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}],\n",
    "    \"context\": {\n",
    "        \"user_name\": \"Alice\",\n",
    "        \"preferences\": {\n",
    "            \"concise\": True,\n",
    "            \"technical_level\": \"beginner\"\n",
    "        }\n",
    "    },\n",
    "    \"custom_inputs\": {\n",
    "        \"session_id\": \"abc123\",\n",
    "        \"request_type\": \"explanation\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.output[0].content[0]['text']}\")\n",
    "print(f\"\\nCustom outputs: {response.custom_outputs}\")\n",
    "print(\"\\n✅ Context-aware agent working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Production Agent\n",
    "\n",
    "A complete, deployable ChatCompletions agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile chat_completions_agent.py\n",
    "\"\"\"Production ChatCompletions agent with ResponsesAgent.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class ChatCompletionsAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Production-ready ChatCompletions agent.\n",
    "    \n",
    "    Features:\n",
    "    - OpenAI ChatCompletions integration\n",
    "    - Streaming support\n",
    "    - Context-aware system prompts\n",
    "    - Token usage tracking\n",
    "    - Full MLflow tracing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        system_prompt: str = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt or \"You are a helpful AI assistant.\"\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def _prepare_messages(self, request: ResponsesAgentRequest) -> list:\n",
    "        \"\"\"Prepare messages for ChatCompletions API.\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(\n",
    "            to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        )\n",
    "        return messages\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Non-streaming prediction.\"\"\"\n",
    "        messages = self._prepare_messages(request)\n",
    "        \n",
    "        # Build API kwargs\n",
    "        api_kwargs = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        if self.max_tokens:\n",
    "            api_kwargs[\"max_tokens\"] = self.max_tokens\n",
    "        \n",
    "        # Call API\n",
    "        response = self.client.chat.completions.create(**api_kwargs)\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=[\n",
    "                self.create_text_output_item(\n",
    "                    text=response.choices[0].message.content,\n",
    "                    id=\"msg_1\",\n",
    "                )\n",
    "            ],\n",
    "            custom_outputs={\n",
    "                \"model\": self.model,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens,\n",
    "                },\n",
    "                **getattr(request, 'custom_inputs', {})\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Streaming prediction.\"\"\"\n",
    "        messages = self._prepare_messages(request)\n",
    "        \n",
    "        api_kwargs = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "        if self.max_tokens:\n",
    "            api_kwargs[\"max_tokens\"] = self.max_tokens\n",
    "        \n",
    "        stream = self.client.chat.completions.create(**api_kwargs)\n",
    "        \n",
    "        yield from output_to_responses_items_stream(\n",
    "            chunk.to_dict() for chunk in stream\n",
    "        )\n",
    "\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create and set model\n",
    "agent = ChatCompletionsAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=\"You are a helpful AI assistant. Be concise and informative.\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the agent\n",
    "with mlflow.start_run(run_name=\"chat_completions_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"chat_completions_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"openai\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ ChatCompletions agent logged!\")\n",
    "    print(f\"Model URI: {model_info.model_uri}\")\n",
    "    print(f\"\\nTo serve locally:\")\n",
    "    print(f\"  mlflow models serve -m {model_info.model_uri} -p 5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the logged model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "response = loaded_model.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is Python?\"}]\n",
    "})\n",
    "\n",
    "print(f\"Loaded model response: {response['output'][0]['content'][0]['text'][:200]}...\")\n",
    "print(\"\\n✅ Model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. ✅ **ChatCompletions vs Responses API**: Understanding the differences\n",
    "2. ✅ **Basic integration**: Connecting ChatCompletions to ResponsesAgent\n",
    "3. ✅ **Format conversion**: Using MLflow utilities\n",
    "4. ✅ **Multi-turn conversations**: Handling history\n",
    "5. ✅ **Context awareness**: Using custom inputs and context\n",
    "6. ✅ **Production deployment**: Logging and serving\n",
    "\n",
    "### Key Utilities:\n",
    "\n",
    "| Utility | Purpose |\n",
    "|---------|--------|\n",
    "| `to_chat_completions_input()` | Convert ResponsesAgent input to ChatCompletions format |\n",
    "| `output_to_responses_items_stream()` | Convert ChatCompletions stream to ResponsesAgent events |\n",
    "\n",
    "### Next Steps:\n",
    "- Explore OpenAI Responses API integration\n",
    "- Learn about deployment options\n",
    "- Build advanced agents with tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
