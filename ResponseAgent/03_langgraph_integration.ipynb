{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow ResponseAgent with LangGraph Integration\n",
    "\n",
    "This notebook demonstrates how to wrap LangGraph agents using MLflow's ResponseAgent for production deployment.\n",
    "\n",
    "## Table of Contents\n",
    "1. Why Use ResponseAgent with LangGraph?\n",
    "2. Building a Basic LangGraph Agent\n",
    "3. Wrapping LangGraph with ResponseAgent\n",
    "4. Logging and Serving\n",
    "5. Complete Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "from typing import TypedDict, Annotated, Generator\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# MLflow ResponseAgent imports\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(\"LangGraph_ResponseAgent\")\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Use ResponseAgent with LangGraph?\n",
    "\n",
    "### LangGraph Challenges:\n",
    "- **Complex Deployment**: LangGraph graphs are not easily serializable\n",
    "- **No Standard API**: Each graph has custom input/output formats\n",
    "- **Observability**: Difficult to trace multi-step agent execution\n",
    "- **Version Control**: Hard to track and version graph changes\n",
    "\n",
    "### ResponseAgent Solutions:\n",
    "✅ **Standard Interface**: Convert LangGraph I/O to OpenAI format\n",
    "\n",
    "✅ **Easy Deployment**: Log once, deploy anywhere with MLflow\n",
    "\n",
    "✅ **Built-in Tracing**: Automatic tracking of all agent steps\n",
    "\n",
    "✅ **Version Management**: Full MLflow experiment tracking\n",
    "\n",
    "✅ **Framework Independence**: Switch between frameworks without changing deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Basic LangGraph Agent\n",
    "\n",
    "First, let's create a simple LangGraph chatbot agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our graph\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def create_simple_chatbot() -> CompiledStateGraph:\n",
    "    \"\"\"\n",
    "    Create a simple LangGraph chatbot that uses GPT-4.\n",
    "    \n",
    "    This is the basic LangGraph pattern:\n",
    "    1. Define state\n",
    "    2. Create nodes (functions that process state)\n",
    "    3. Build graph with edges\n",
    "    4. Compile graph\n",
    "    \"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    \n",
    "    # Define the chatbot node\n",
    "    def chatbot(state: State):\n",
    "        \"\"\"Process messages and return LLM response.\"\"\"\n",
    "        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "    \n",
    "    # Build the graph\n",
    "    graph_builder = StateGraph(State)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    \n",
    "    # Add edges\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "    graph_builder.add_edge(\"chatbot\", END)\n",
    "    \n",
    "    # Compile and return\n",
    "    return graph_builder.compile()\n",
    "\n",
    "\n",
    "# Test the basic LangGraph agent\n",
    "print(\"Testing basic LangGraph agent...\\n\")\n",
    "graph = create_simple_chatbot()\n",
    "\n",
    "# Test invocation\n",
    "response = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is LangGraph?\")]\n",
    "})\n",
    "\n",
    "print(f\"LangGraph Response: {response['messages'][-1].content}\")\n",
    "print(\"\\n✅ Basic LangGraph agent working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrapping LangGraph with ResponseAgent\n",
    "\n",
    "Now let's wrap our LangGraph agent in a ResponseAgent for MLflow compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities.span import SpanType\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Wrapper for LangGraph agents using ResponseAgent interface.\n",
    "    \n",
    "    This adapter:\n",
    "    1. Converts ResponsesAgentRequest to LangGraph format\n",
    "    2. Invokes the LangGraph agent\n",
    "    3. Converts LangGraph output to ResponsesAgentResponse\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        \"\"\"Initialize with a compiled LangGraph agent.\"\"\"\n",
    "        self.agent = agent\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"\n",
    "        Main prediction method - non-streaming version.\n",
    "        \n",
    "        Collects all stream events and returns final response.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        \n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=request.custom_inputs  # Pass through context\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"\n",
    "        Streaming prediction method.\n",
    "        \n",
    "        This is where the magic happens:\n",
    "        1. Convert ResponsesAgent input to ChatCompletions format\n",
    "        2. Stream through LangGraph\n",
    "        3. Convert each update to ResponsesAgent stream events\n",
    "        \"\"\"\n",
    "        # Convert input format\n",
    "        cc_msgs = to_chat_completions_input(\n",
    "            [i.model_dump() for i in request.input]\n",
    "        )\n",
    "        \n",
    "        # Stream through LangGraph\n",
    "        for _, events in self.agent.stream(\n",
    "            {\"messages\": cc_msgs}, \n",
    "            stream_mode=[\"updates\"]\n",
    "        ):\n",
    "            # Convert each node's output to ResponsesAgent format\n",
    "            for node_data in events.values():\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"✅ LangGraphResponsesAgent class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Wrapped Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and wrap the agent\n",
    "langgraph_agent = create_simple_chatbot()\n",
    "wrapped_agent = LangGraphResponsesAgent(langgraph_agent)\n",
    "\n",
    "# Test with ResponsesAgent format\n",
    "test_request = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain MLflow in one sentence.\"}\n",
    "    ],\n",
    "    \"context\": {\"user_id\": \"test_user\", \"session_id\": \"session_456\"},\n",
    "}\n",
    "\n",
    "# Get response\n",
    "response = wrapped_agent.predict(test_request)\n",
    "\n",
    "print(\"Response from wrapped LangGraph agent:\")\n",
    "print(f\"Output: {response.output[0]}\")\n",
    "print(f\"\\nCustom outputs: {response.custom_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Utility Functions\n",
    "\n",
    "Let's create helper functions for working with LangGraph + ResponseAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile langgraph_utils.py\n",
    "\"\"\"Utility functions for LangGraph + MLflow ResponseAgent integration.\"\"\"\n",
    "\n",
    "from langgraph.pregel.io import AddableValuesDict\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def _langgraph_message_to_mlflow_message(\n",
    "    langgraph_message: AddableValuesDict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert a LangGraph message to MLflow format.\n",
    "    \n",
    "    Maps:\n",
    "    - human -> user\n",
    "    - ai -> assistant\n",
    "    - system -> system\n",
    "    \"\"\"\n",
    "    langgraph_type_to_mlflow_role = {\n",
    "        \"human\": \"user\",\n",
    "        \"ai\": \"assistant\",\n",
    "        \"system\": \"system\",\n",
    "    }\n",
    "    \n",
    "    if type_clean := langgraph_type_to_mlflow_role.get(langgraph_message.type):\n",
    "        return {\"role\": type_clean, \"content\": langgraph_message.content}\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect role specified: {langgraph_message.type}\")\n",
    "\n",
    "\n",
    "def get_most_recent_message(response: AddableValuesDict) -> str:\n",
    "    \"\"\"\n",
    "    Extract the most recent message content from a LangGraph response.\n",
    "    \"\"\"\n",
    "    most_recent_message = response.get(\"messages\")[-1]\n",
    "    return _langgraph_message_to_mlflow_message(most_recent_message)[\"content\"]\n",
    "\n",
    "\n",
    "def increment_message_history(\n",
    "    response: AddableValuesDict,\n",
    "    new_message: Union[dict, AddableValuesDict]\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Add a new message to the conversation history.\n",
    "    \n",
    "    Useful for maintaining context across multiple turns.\n",
    "    \"\"\"\n",
    "    if isinstance(new_message, AddableValuesDict):\n",
    "        new_message = _langgraph_message_to_mlflow_message(new_message)\n",
    "    \n",
    "    message_history = [\n",
    "        _langgraph_message_to_mlflow_message(message)\n",
    "        for message in response.get(\"messages\")\n",
    "    ]\n",
    "    \n",
    "    return message_history + [new_message]\n",
    "\n",
    "\n",
    "print(\"✅ Utility functions created in langgraph_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Model-from-Code File\n",
    "\n",
    "Now let's create a complete, deployable agent file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile langgraph_agent.py\n",
    "\"\"\"Complete LangGraph agent wrapped in ResponseAgent for MLflow.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Generator\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities.span import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def create_chatbot_graph() -> CompiledStateGraph:\n",
    "    \"\"\"Create and compile the LangGraph chatbot.\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    \n",
    "    def chatbot(state: State):\n",
    "        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "    \n",
    "    graph_builder = StateGraph(State)\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "    graph_builder.add_edge(\"chatbot\", END)\n",
    "    \n",
    "    return graph_builder.compile()\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"ResponseAgent wrapper for LangGraph.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs,\n",
    "            custom_outputs=request.custom_inputs\n",
    "        )\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input(\n",
    "            [i.model_dump() for i in request.input]\n",
    "        )\n",
    "        \n",
    "        for _, events in self.agent.stream(\n",
    "            {\"messages\": cc_msgs},\n",
    "            stream_mode=[\"updates\"]\n",
    "        ):\n",
    "            for node_data in events.values():\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Enable auto-tracing for LangChain\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Create and set the model\n",
    "graph = create_chatbot_graph()\n",
    "agent = LangGraphResponsesAgent(graph)\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logging the Agent to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run(run_name=\"langgraph_chatbot_agent\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"langgraph_agent.py\",\n",
    "        artifact_path=\"agent\",\n",
    "        # Dependencies are auto-inferred, but you can specify:\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "            \"langgraph>=0.2.27\",\n",
    "            \"langchain>=0.3.0\",\n",
    "            \"langchain-openai>=0.2.0\",\n",
    "            \"openai\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model logged successfully!\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Model URI: {model_info.model_uri}\")\n",
    "    print(f\"\\nView in MLflow UI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading and Testing the Logged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# Test conversation\n",
    "print(\"Testing multi-turn conversation...\\n\")\n",
    "\n",
    "# Turn 1\n",
    "response1 = loaded_model.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "})\n",
    "print(f\"Turn 1:\")\n",
    "print(f\"User: What is machine learning?\")\n",
    "print(f\"Agent: {response1['output'][0]['content'][0]['text'][:100]}...\")\n",
    "\n",
    "# Turn 2 - with history\n",
    "response2 = loaded_model.predict({\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response1['output'][0]['content'][0]['text']},\n",
    "        {\"role\": \"user\", \"content\": \"Can you give me an example?\"},\n",
    "    ],\n",
    "})\n",
    "print(f\"\\nTurn 2:\")\n",
    "print(f\"User: Can you give me an example?\")\n",
    "print(f\"Agent: {response2['output'][0]['content'][0]['text'][:100]}...\")\n",
    "\n",
    "print(\"\\n✅ Multi-turn conversation working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Viewing Traces in MLflow UI\n",
    "\n",
    "To view detailed traces:\n",
    "\n",
    "1. Start MLflow UI (if not running):\n",
    "   ```bash\n",
    "   mlflow ui\n",
    "   ```\n",
    "\n",
    "2. Navigate to http://localhost:5000\n",
    "\n",
    "3. Click on your experiment: \"LangGraph_ResponseAgent\"\n",
    "\n",
    "4. Click the \"Traces\" tab\n",
    "\n",
    "5. View detailed execution traces including:\n",
    "   - Input/output for each node\n",
    "   - Token usage\n",
    "   - Execution time\n",
    "   - Error tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. ✅ Built a basic LangGraph chatbot\n",
    "2. ✅ Wrapped it with ResponseAgent for standardization\n",
    "3. ✅ Logged the agent using Models-from-Code\n",
    "4. ✅ Loaded and tested the deployed model\n",
    "5. ✅ Enabled full tracing with MLflow\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Standardization**: OpenAI-compatible API\n",
    "- **Deployment**: Easy serving with MLflow\n",
    "- **Observability**: Full execution traces\n",
    "- **Flexibility**: Switch frameworks without changing deployment\n",
    "\n",
    "### Next Steps:\n",
    "- Add tool calling to your LangGraph agent\n",
    "- Implement more complex multi-agent workflows\n",
    "- Deploy to production endpoints\n",
    "- Explore streaming responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
