{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1244e16f",
      "metadata": {},
      "source": [
        "# 2.5 Output Parsers in LangChain\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "**Output parsers** transform unstructured LLM text into structured Python objects. In this notebook, you'll learn:\n",
        "\n",
        "1. **PydanticOutputParser** - Parse LLM output into Pydantic models\n",
        "2. **JsonOutputParser** - Parse into Python dictionaries\n",
        "3. **CommaSeparatedListOutputParser** - Parse into Python lists\n",
        "\n",
        "## üí° Why Use Output Parsers?\n",
        "\n",
        "| Without Parser | With Parser |\n",
        "|---------------|-------------|\n",
        "| `\"The sentiment is positive and the topic is AI\"` | `{\"sentiment\": \"positive\", \"topic\": \"AI\"}` |\n",
        "| Manual string parsing | Automatic type validation |\n",
        "| Error-prone | Reliable and consistent |\n",
        "\n",
        "## üîë Key Concept\n",
        "\n",
        "Output parsers work in two steps:\n",
        "1. **Inject format instructions** into the prompt\n",
        "2. **Parse the LLM response** into structured data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c0fbb7",
      "metadata": {},
      "source": [
        "## üì¶ Installation (Run Once)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50858b2",
      "metadata": {},
      "source": [
        "## üîê Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0379db53",
      "metadata": {},
      "source": [
        "#### Enter your Open AI Key here\n",
        "\n",
        "You can get the key from [here](https://platform.openai.com/api-keys) after creating an account or signing in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0ecc5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from getpass import getpass\n",
        "\n",
        "# OPENAI_KEY = getpass('Please enter your Open AI API Key here: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a414b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93f6e57",
      "metadata": {},
      "source": [
        "## ü§ñ Initialize the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f3e305",
      "metadata": {},
      "source": [
        "We'll use OpenAI's GPT-4o-mini for structured output generation. More capable models generally produce better structured outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3f523105",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment variables loaded successfully!\n",
            "üìç Running on: Darwin\n",
            "ü§ñ LLM initialized: databricks-gpt-5-1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP: Load API Keys & Import Dependencies\n",
        "# ============================================================================\n",
        "# We use python-dotenv to securely load API keys from a .env file\n",
        "# This is a best practice - never hardcode API keys in your notebooks!\n",
        "# ============================================================================\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "sys.path.append(os.path.abspath(\"../..\"))\n",
        "\n",
        "# Import our LLM factory functions\n",
        "# - get_groq_llm(): Creates a Groq-hosted LLM (fast inference with open-source models)\n",
        "# - get_openai_llm(): Creates an OpenAI GPT model\n",
        "# - get_databricks_llm(): Creates a Databricks-hosted LLM\n",
        "from helpers.utils import get_groq_llm, get_openai_llm, get_databricks_llm\n",
        "\n",
        "print(\"‚úÖ Environment variables loaded successfully!\")\n",
        "print(f\"üìç Running on: {platform.system()}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize the LLM based on platform or preference\n",
        "# The choice of LLM affects tool calling capabilities and speed\n",
        "# -----------------------------------------------------------------------------\n",
        "if sys.platform == \"win32\":\n",
        "    # Windows: Use Groq for fast inference\n",
        "    llm = get_groq_llm()\n",
        "elif sys.platform == \"darwin\":\n",
        "    # macOS: Use Databricks-hosted Gemini\n",
        "    llm = get_databricks_llm(\"databricks-gpt-5-1\")  \n",
        "else:\n",
        "    # Linux: Default to Groq\n",
        "    llm = get_groq_llm()\n",
        "\n",
        "# Print which LLM we're using\n",
        "if hasattr(llm, 'model_name'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model_name}\")\n",
        "elif hasattr(llm, 'model'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model}\")\n",
        "else:\n",
        "    print(\"ü§ñ LLM initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb34d35f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Overview of Output Parsers\n",
        "\n",
        "LangChain provides several output parsers for different use cases:\n",
        "\n",
        "| Parser | Output Type | Best For |\n",
        "|--------|-------------|----------|\n",
        "| `PydanticOutputParser` | Pydantic Model | Complex schemas with validation |\n",
        "| `JsonOutputParser` | Python Dict | Simple JSON structures |\n",
        "| `CommaSeparatedListOutputParser` | Python List | Lists of items |\n",
        "| `StrOutputParser` | String | Simple text extraction |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536775d7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üî∑ Parser 1: PydanticOutputParser\n",
        "\n",
        "The most powerful parser - converts LLM output into **validated Pydantic models**.\n",
        "\n",
        "**Advantages:**\n",
        "- Type checking and validation\n",
        "- IDE autocomplete support\n",
        "- Clear schema definition\n",
        "- Automatic error messages\n",
        "\n",
        "> **Note:** Use a capable LLM (GPT-4o-mini or better) for reliable structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4bfc740c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PydanticOutputParser created\n",
            "üìã Expected fields: ['description', 'pros', 'cons', 'conclusion']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: DEFINE YOUR DATA MODEL\n",
        "# ============================================================================\n",
        "# Pydantic models define the structure of your expected output\n",
        "# Field() adds descriptions that help the LLM understand what to generate\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Define the output structure using Pydantic\n",
        "class QueryResponse(BaseModel):\n",
        "    \"\"\"Structured response for topic analysis\"\"\"\n",
        "    description: str = Field(description=\"A brief description of the topic asked by the user\")\n",
        "    pros: str = Field(description=\"3 bullet points showing the pros of the topic\")\n",
        "    cons: str = Field(description=\"3 bullet points showing the cons of the topic\")\n",
        "    conclusion: str = Field(description=\"One line conclusion of the topic\")\n",
        "\n",
        "# Create the parser from the Pydantic model\n",
        "parser = PydanticOutputParser(pydantic_object=QueryResponse)\n",
        "\n",
        "print(\"‚úÖ PydanticOutputParser created\")\n",
        "print(f\"üìã Expected fields: {list(QueryResponse.model_fields.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7db7134a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìú Format Instructions (injected into prompt):\n",
            "============================================================\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"description\": \"Structured response for topic analysis\", \"properties\": {\"description\": {\"description\": \"A brief description of the topic asked by the user\", \"title\": \"Description\", \"type\": \"string\"}, \"pros\": {\"description\": \"3 bullet points showing the pros of the topic\", \"title\": \"Pros\", \"type\": \"string\"}, \"cons\": {\"description\": \"3 bullet points showing the cons of the topic\", \"title\": \"Cons\", \"type\": \"string\"}, \"conclusion\": {\"description\": \"One line conclusion of the topic\", \"title\": \"Conclusion\", \"type\": \"string\"}}, \"required\": [\"description\", \"pros\", \"cons\", \"conclusion\"]}\n",
            "```\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: VIEW FORMAT INSTRUCTIONS\n",
        "# ============================================================================\n",
        "# The parser auto-generates instructions that tell the LLM how to format output\n",
        "# These instructions are injected into your prompt!\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üìú Format Instructions (injected into prompt):\")\n",
        "print(\"=\" * 60)\n",
        "print(parser.get_format_instructions())\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b6847ff9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Parsed Response:\n",
            "   Type: <class '__main__.QueryResponse'>\n",
            "   Description: The user is asking for the capital city of France, which is Paris....\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: CREATE THE CHAIN\n",
        "# ============================================================================\n",
        "# The chain: Prompt ‚Üí LLM ‚Üí Parser\n",
        "# - partial_variables injects the format instructions automatically\n",
        "# - The parser converts the LLM's text output into a Pydantic object\n",
        "# ============================================================================\n",
        "\n",
        "prompt_txt = \"\"\"\n",
        "Answer the user query and generate the response based on the following formatting instructions.\n",
        "\n",
        "Format Instructions:\n",
        "{format_instructions}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_txt,\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Build the LCEL chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# Test with a simple query\n",
        "response = chain.invoke({\"query\": \"What is the capital of France?\"})\n",
        "\n",
        "print(\"üéØ Parsed Response:\")\n",
        "print(f\"   Type: {type(response)}\")\n",
        "print(f\"   Description: {response.description}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510dc6a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# USING THE PARSED RESPONSE\n",
        "# ============================================================================\n",
        "\n",
        "question = \"Tell me about Commercial Real Estate\"\n",
        "response = chain.invoke({\"query\": question})\n",
        "\n",
        "print(f\"üìù Query: {question}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcdedfdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access fields as Python attributes (type-safe!)\n",
        "print(\"üìã Description:\")\n",
        "print(response.description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561deae2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n‚úÖ Pros:\")\n",
        "print(response.pros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d363f1da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to dictionary for JSON serialization\n",
        "print(\"\\nüì¶ As Dictionary:\")\n",
        "response.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ada79fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pretty print all fields\n",
        "print(\"üìã All Fields:\")\n",
        "print(\"=\" * 50)\n",
        "for k, v in response.model_dump().items():\n",
        "    print(f\"\\nüîπ {k.upper()}:\")\n",
        "    print(f\"   {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e997ff6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üî∑ Parser 2: JsonOutputParser\n",
        "\n",
        "Similar to PydanticOutputParser but returns a **Python dictionary** instead of a Pydantic model.\n",
        "\n",
        "**When to use:**\n",
        "- When you need a simple dict, not a Pydantic model\n",
        "- For dynamic schemas\n",
        "- When you want flexibility over strict typing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738ec9fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define your desired data structure - like a python data class.\n",
        "class QueryResponse(BaseModel):\n",
        "    description: str = Field(description=\"A brief description of the topic asked by the user\")\n",
        "    pros: str = Field(description=\"3 bullet points showing the pros of the topic asked by the user\")\n",
        "    cons: str = Field(description=\"3 bullet points showing the cons of the topic asked by the user\")\n",
        "    conclusion: str = Field(description=\"One line conclusion of the topic asked by the user\")\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = JsonOutputParser(pydantic_object=QueryResponse)\n",
        "parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb919022",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the final prompt with formatting instructions from the parser\n",
        "prompt_txt = \"\"\"\n",
        "             Answer the user query and generate the response based on the following formatting instructions\n",
        "\n",
        "             Format Instructions:\n",
        "             {format_instructions}\n",
        "\n",
        "             Query:\n",
        "             {query}\n",
        "            \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_txt,\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e37d28c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a simple LCEL chain to take the prompt, pass it to the LLM, enforce response format using the parser\n",
        "chain = (prompt\n",
        "              |\n",
        "            chatgpt\n",
        "              |\n",
        "            parser)\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d870122c",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_queries = [\n",
        "    \"Tell me about commercial real estate\",\n",
        "    \"Tell me about Generative AI\"\n",
        "]\n",
        "\n",
        "topic_queries_formatted = [{\"query\": topic}\n",
        "                    for topic in topic_queries]\n",
        "topic_queries_formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99083647",
      "metadata": {},
      "outputs": [],
      "source": [
        "responses = chain.map().invoke(topic_queries_formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f6a376",
      "metadata": {},
      "outputs": [],
      "source": [
        "responses[0], type(responses[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e62451",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(responses)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942c98b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "for response in responses:\n",
        "  for k,v in response.items():\n",
        "    print(f\"{k}:\\n{v}\\n\")\n",
        "  print('-----')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d927dc",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üî∑ Parser 3: CommaSeparatedListOutputParser\n",
        "\n",
        "The simplest parser - converts comma-separated text into a **Python list**.\n",
        "\n",
        "**When to use:**\n",
        "- Generating lists of items\n",
        "- Simple enumerations\n",
        "- Tags or categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7d701",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMMASEPARATEDLISTOUTPUTPARSER\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# View the format instructions\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(\"üìú Format Instructions:\")\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf57e4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# And a query intented to prompt a language model to populate the data structure.\n",
        "prompt_txt = \"\"\"\n",
        "             Create a list of 5 different ways in which Generative AI can be used\n",
        "\n",
        "             Output format instructions:\n",
        "             {format_instructions}\n",
        "             \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_txt)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_txt,\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14379eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a simple LLM Chain - more on this later\n",
        "llm_chain = (prompt\n",
        "              |\n",
        "            chatgpt\n",
        "              |\n",
        "            output_parser)\n",
        "\n",
        "# run the chain\n",
        "response = llm_chain.invoke({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fc4cde",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Parsed List:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86363e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access list items\n",
        "print(\"\\nüìù List Items:\")\n",
        "for i, item in enumerate(response, 1):\n",
        "    print(f\"  {i}. {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2eb886",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüìä Type: {type(response)}\")  # <class 'list'>\n",
        "\n",
        "# ============================================================================\n",
        "# üìù KEY TAKEAWAYS FROM THIS NOTEBOOK:\n",
        "# ============================================================================\n",
        "# 1. Output parsers convert unstructured LLM text ‚Üí structured Python objects\n",
        "# 2. PydanticOutputParser: Best for complex schemas with validation\n",
        "# 3. JsonOutputParser: Returns Python dictionaries\n",
        "# 4. CommaSeparatedListOutputParser: Returns Python lists\n",
        "# 5. Use .get_format_instructions() to see what's injected into prompts\n",
        "# 6. Chain: prompt | llm | parser for clean LCEL integration\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a779c677",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
