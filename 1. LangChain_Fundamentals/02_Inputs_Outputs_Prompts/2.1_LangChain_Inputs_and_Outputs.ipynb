{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 LangChain Inputs and Outputs\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "In this notebook, you'll learn the **fundamental ways to interact with LLMs** in LangChain:\n",
        "\n",
        "1. **Single Invocation** - Using `.invoke()` for one-off requests\n",
        "2. **Batch Processing** - Using `.batch()` and `.generate()` for multiple requests\n",
        "3. **Message Types** - Building conversations with `SystemMessage`, `HumanMessage`, and `AIMessage`\n",
        "4. **Response Handling** - Extracting content from `AIMessage` and `LLMResult` objects\n",
        "\n",
        "## üìö Prerequisites\n",
        "\n",
        "- Basic Python knowledge\n",
        "- API keys configured (OpenAI, Groq, or Databricks)\n",
        "- Completed: `1. Getting Started` notebooks\n",
        "\n",
        "## üîë Key Concepts\n",
        "\n",
        "| Method | Input | Output | Use Case |\n",
        "|--------|-------|--------|----------|\n",
        "| `.invoke()` | String or Messages | AIMessage | Single request |\n",
        "| `.batch()` | List of Strings | List of AIMessages | Multiple simple requests |\n",
        "| `.generate()` | List of Message Lists | LLMResult | Multiple conversations with metadata |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment variables loaded successfully!\n",
            "üìç Running on: Darwin\n",
            "ü§ñ LLM initialized: databricks-gemini-2-5-pro\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP: Load API Keys & Import Dependencies\n",
        "# ============================================================================\n",
        "# We use python-dotenv to securely load API keys from a .env file\n",
        "# This is a best practice - never hardcode API keys in your notebooks!\n",
        "# ============================================================================\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "sys.path.append(os.path.abspath(\"../..\"))\n",
        "\n",
        "# Import our LLM factory functions\n",
        "# - get_groq_llm(): Creates a Groq-hosted LLM (fast inference with open-source models)\n",
        "# - get_openai_llm(): Creates an OpenAI GPT model\n",
        "# - get_databricks_llm(): Creates a Databricks-hosted LLM\n",
        "from helpers.utils import get_groq_llm, get_openai_llm, get_databricks_llm\n",
        "\n",
        "print(\"‚úÖ Environment variables loaded successfully!\")\n",
        "print(f\"üìç Running on: {platform.system()}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize the LLM based on platform or preference\n",
        "# The choice of LLM affects tool calling capabilities and speed\n",
        "# -----------------------------------------------------------------------------\n",
        "if sys.platform == \"win32\":\n",
        "    # Windows: Use Groq for fast inference\n",
        "    llm = get_groq_llm()\n",
        "elif sys.platform == \"darwin\":\n",
        "    # macOS: Use Databricks-hosted Gemini\n",
        "    llm = get_databricks_llm(\"databricks-gemini-2-5-pro\")  \n",
        "else:\n",
        "    # Linux: Default to Groq\n",
        "    llm = get_groq_llm()\n",
        "\n",
        "# Print which LLM we're using\n",
        "if hasattr(llm, 'model_name'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model_name}\")\n",
        "elif hasattr(llm, 'model'):\n",
        "    print(f\"ü§ñ LLM initialized: {llm.model}\")\n",
        "else:\n",
        "    print(\"ü§ñ LLM initialized successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# BASIC LLM INVOCATION: The .invoke() Method\n",
        "# ============================================================================\n",
        "# The .invoke() method is the simplest way to interact with an LLM\n",
        "# - Input: A string prompt (or list of messages)\n",
        "# - Output: An AIMessage object containing the response\n",
        "# ============================================================================\n",
        "\n",
        "result = llm.invoke(\"Tell me a joke\")\n",
        "\n",
        "# The response is an AIMessage object - use .content to get the text\n",
        "print(\"Response type:\", type(result))\n",
        "print(\"Response content:\", result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Batch Processing with LLMs\n",
        "\n",
        "When you need to process **multiple prompts efficiently**, LangChain provides two approaches:\n",
        "\n",
        "1. **`.batch()`** - Simple and clean, accepts list of strings\n",
        "2. **`.generate()`** - More control, expects list of message lists\n",
        "\n",
        "Batch processing is more efficient than calling `.invoke()` multiple times because:\n",
        "- Reduces API overhead\n",
        "- Some providers optimize batch requests\n",
        "- Easier to manage multiple outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "BATCH PROCESSING RESULTS\n",
            "============================================================\n",
            "\n",
            "üìù Prompt 1: Tell me a joke about cows\n",
            "ü§ñ Response: Why do cows wear bells?\n",
            "\n",
            "...Because their horns don't work\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìù Prompt 2: Tell me a joke about parrots\n",
            "ü§ñ Response: Here's a classic for you:\n",
            "\n",
            "A woman gets a new parrot, but it has a terrible attitude and an even worse vocabulary. Every other word is a swear word. She tries everything to clean up the bird's language, but nothing works.\n",
            "\n",
            "Finally, in a fit of rage, she grabs the parrot and shoves it into the freezer.\n",
            "\n",
            "For a minute, she hears squawking, kicking, and screaming. Then, suddenly, it's completely silent. Horrified that she might have hurt the bird, she quickly opens the freezer door.\n",
            "\n",
            "The parrot calmly steps out, shivers, and says, \"I must apologize for my offensive language and behavior. I promise to improve my vocabulary and conduct myself as a proper gentleman from now on.\"\n",
            "\n",
            "The woman is astonished. But before she can ask what caused the sudden change, the parrot continues, \"If I may be so bold... what did the chicken do?\"\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# METHOD 1: Using .batch() - The Simple Approach\n",
        "# ============================================================================\n",
        "# .batch() accepts a list of string prompts and returns a list of AIMessage objects\n",
        "# This is the recommended approach for simple batch processing\n",
        "# ============================================================================\n",
        "\n",
        "prompts = [\n",
        "    \"Tell me a joke about cows\",\n",
        "    \"Tell me a joke about parrots\"\n",
        "]\n",
        "\n",
        "# Process multiple prompts in one call\n",
        "batch_results = llm.batch(prompts)\n",
        "\n",
        "# Extract and display each response\n",
        "print(\"=\" * 60)\n",
        "print(\"BATCH PROCESSING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "for i, result in enumerate(batch_results):\n",
        "    print(f\"\\nüìù Prompt {i+1}: {prompts[i]}\")\n",
        "    print(f\"ü§ñ Response: {result.content}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of result: <class 'langchain_core.outputs.llm_result.LLMResult'>\n",
            "\n",
            "Number of generations: 2\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# METHOD 2: Using .generate() - More Control & Metadata\n",
        "# ============================================================================\n",
        "# .generate() is more powerful but requires structured input:\n",
        "# - Input: List of message lists (each inner list is a conversation)\n",
        "# - Output: LLMResult object with generations, token usage, and metadata\n",
        "# \n",
        "# Use .generate() when you need:\n",
        "# - Access to token usage statistics\n",
        "# - Full metadata about each generation\n",
        "# - Complex multi-turn conversations in batch\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Each inner list represents a separate conversation/request\n",
        "generate_result = llm.generate([\n",
        "    [HumanMessage(content=\"Tell me a joke about cows\")],\n",
        "    [HumanMessage(content=\"Tell me a joke about parrots\")]\n",
        "])\n",
        "\n",
        "# The result contains detailed information about each generation\n",
        "print(\"Type of result:\", type(generate_result))\n",
        "print(\"\\nNumber of generations:\", len(generate_result.generations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[ChatGeneration(text='Of course!\\n\\nWhere do cows go for entertainment?\\n\\n...To the **moo**-vies', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='Of course!\\n\\nWhere do cows go for entertainment?\\n\\n...To the **moo**-vies', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 6, 'completion_tokens': 22, 'total_tokens': 1577}, 'prompt_tokens': 6, 'completion_tokens': 22, 'total_tokens': 1577, 'model': 'gemini-2.5-pro', 'model_name': 'gemini-2.5-pro', 'finish_reason': 'stop'}, id='lc_run--019c0f53-11ba-79b1-a238-70b27ef353b5-0', tool_calls=[], invalid_tool_calls=[]))],\n",
              " [ChatGeneration(text='Of course! Here\\'s a classic for you:\\n\\nA woman goes into a pet shop and sees a beautiful, colorful parrot. She asks the owner, \"How much for this magnificent bird?\"\\n\\nThe owner sighs. \"He\\'s $50, but I have to warn you, he used to live in a brothel. He says some very inappropriate things.\"\\n\\nThe woman thinks about it and decides she can handle it. \"I\\'ll take him!\"\\n\\nShe brings the parrot home. As she uncovers his cage, the parrot looks around and squawks, \"New house, new madam!\"\\n\\nThe woman is a little shocked but laughs it off. A few hours later, her two teenage daughters come home from school. The parrot sees them and squawks, \"New house, new madam, new girls!\"\\n\\nThe woman and her daughters are horrified, but before they can react, the front door opens and the woman\\'s husband, Keith, walks in.\\n\\nThe parrot looks at him, gets very quiet, and then screeches:\\n\\n\"Hi, Keith!\"', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='Of course! Here\\'s a classic for you:\\n\\nA woman goes into a pet shop and sees a beautiful, colorful parrot. She asks the owner, \"How much for this magnificent bird?\"\\n\\nThe owner sighs. \"He\\'s $50, but I have to warn you, he used to live in a brothel. He says some very inappropriate things.\"\\n\\nThe woman thinks about it and decides she can handle it. \"I\\'ll take him!\"\\n\\nShe brings the parrot home. As she uncovers his cage, the parrot looks around and squawks, \"New house, new madam!\"\\n\\nThe woman is a little shocked but laughs it off. A few hours later, her two teenage daughters come home from school. The parrot sees them and squawks, \"New house, new madam, new girls!\"\\n\\nThe woman and her daughters are horrified, but before they can react, the front door opens and the woman\\'s husband, Keith, walks in.\\n\\nThe parrot looks at him, gets very quiet, and then screeches:\\n\\n\"Hi, Keith!\"', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 6, 'completion_tokens': 222, 'total_tokens': 1724}, 'prompt_tokens': 6, 'completion_tokens': 222, 'total_tokens': 1724, 'model': 'gemini-2.5-pro', 'model_name': 'gemini-2.5-pro', 'finish_reason': 'stop'}, id='lc_run--019c0f53-11ba-79b1-a238-70c13df4d9d9-0', tool_calls=[], invalid_tool_calls=[]))]]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OPTIONAL: Inspect the full LLMResult object\n",
        "# ============================================================================\n",
        "# The generate_result variable contains the complete LLMResult object\n",
        "# with all generations and metadata. Uncomment to explore its structure.\n",
        "# ============================================================================\n",
        "\n",
        "# Uncomment to see the full structure:\n",
        "generate_result.generations  # List of ChatGeneration objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ Building Conversations with Message Types\n",
        "\n",
        "LangChain provides three core message types for building conversations:\n",
        "\n",
        "| Message Type | Purpose | Example |\n",
        "|--------------|---------|---------|\n",
        "| **SystemMessage** | Sets the AI's behavior/personality | \"You are a helpful restaurant assistant\" |\n",
        "| **HumanMessage** | User's input/questions | \"What's on the menu?\" |\n",
        "| **AIMessage** | AI's previous responses | \"We have pasta, pizza, and salads\" |\n",
        "\n",
        "**Why use message types?**\n",
        "- Gives the LLM context about who said what\n",
        "- Allows multi-turn conversations with history\n",
        "- System messages guide the AI's behavior throughout the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Switched to: databricks-gpt-5-1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SWITCHING LLM PROVIDER: Using OpenAI's ChatGPT\n",
        "# ============================================================================\n",
        "# LangChain makes it easy to switch between different LLM providers\n",
        "# Here we switch to OpenAI's GPT-4o-mini for the conversation examples\n",
        "# The API is consistent - all LLMs support .invoke(), .batch(), .generate()\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize OpenAI's GPT-4o-mini model\n",
        "# Note: Requires OPENAI_API_KEY in your .env file\n",
        "llm = get_databricks_llm(\"databricks-gpt-5-1\")  \n",
        "print(f\"‚úÖ Switched to: {llm.model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI Response: Why don‚Äôt programmers like nature?\n",
            "\n",
            "It has too many bugs.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# QUICK TEST: Verify the new LLM is working\n",
        "# ============================================================================\n",
        "# This is a simple test to confirm the OpenAI model is properly configured\n",
        "# Note: This is similar to our earlier example but uses a different provider\n",
        "# ============================================================================\n",
        "\n",
        "# Simple invocation test with OpenAI\n",
        "test_result = llm.invoke(\"Tell me a short joke\")\n",
        "print(\"‚úÖ OpenAI Response:\", test_result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Conversation Structure:\n",
            "--------------------------------------------------\n",
            "  System: You are a helpful assistant specialized in providing informa...\n",
            "   Human: What's on the menu?\n",
            "      AI: BellaVista offers a variety of Italian dishes including past...\n",
            "   Human: Do you have vegan options?\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CREATING A MULTI-TURN CONVERSATION\n",
        "# ============================================================================\n",
        "# This example simulates a restaurant chatbot conversation\n",
        "# We build the conversation history manually to show how the LLM maintains context\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage\n",
        "\n",
        "# Build a conversation with history\n",
        "# The order matters: System ‚Üí Human ‚Üí AI ‚Üí Human ‚Üí ...\n",
        "messages = [\n",
        "    # 1. System message: Sets the assistant's role and personality\n",
        "    SystemMessage(content=\"You are a helpful assistant specialized in providing information about BellaVista Italian Restaurant.\"),\n",
        "    \n",
        "    # 2. First user question\n",
        "    HumanMessage(content=\"What's on the menu?\"),\n",
        "    \n",
        "    # 3. AI's previous response (simulated history)\n",
        "    AIMessage(content=\"BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood.\"),\n",
        "    \n",
        "    # 4. Follow-up question from user\n",
        "    HumanMessage(content=\"Do you have vegan options?\")\n",
        "]\n",
        "\n",
        "# Display the conversation structure\n",
        "print(\"üìã Conversation Structure:\")\n",
        "print(\"-\" * 50)\n",
        "for msg in messages:\n",
        "    role = type(msg).__name__.replace(\"Message\", \"\")\n",
        "    print(f\"{role:>8}: {msg.content[:60]}{'...' if len(msg.content) > 60 else ''}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Raw Message Objects:\n",
            "\n",
            "[0] SystemMessage:\n",
            "    content: You are a helpful assistant specialized in providing information about BellaVista Italian Restaurant....\n",
            "\n",
            "[1] HumanMessage:\n",
            "    content: What's on the menu?...\n",
            "\n",
            "[2] AIMessage:\n",
            "    content: BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood....\n",
            "\n",
            "[3] HumanMessage:\n",
            "    content: Do you have vegan options?...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# INSPECTING MESSAGE OBJECTS\n",
        "# ============================================================================\n",
        "# Each message is a Pydantic object with:\n",
        "# - content: The actual text\n",
        "# - additional_kwargs: Extra parameters (like function calls)\n",
        "# - response_metadata: Metadata about the response (for AI messages)\n",
        "# ============================================================================\n",
        "\n",
        "# View the raw message objects (useful for debugging)\n",
        "print(\"üì¶ Raw Message Objects:\")\n",
        "for i, msg in enumerate(messages):\n",
        "    print(f\"\\n[{i}] {type(msg).__name__}:\")\n",
        "    print(f\"    content: {msg.content[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ AI Response to 'Do you have vegan options?':\n",
            "--------------------------------------------------\n",
            "Yes, BellaVista does offer vegan options. While the exact items can vary by day or season, you can typically expect:\n",
            "\n",
            "- **Salads** ‚Äì Garden or mixed green salads that can be made vegan by omitting cheese and choosing an oil‚Äëbased dressing (like vinaigrette).\n",
            "- **Bruschetta / Antipasti** ‚Äì Tomato bruschetta without cheese, grilled or marinated vegetables, olives.\n",
            "- **Pasta** ‚Äì Dried pasta (often egg‚Äëfree) with:\n",
            "  - Tomato/basil (pomodoro) sauce  \n",
            "  - Arrabbiata (spicy tomato)  \n",
            "  - Aglio e olio (garlic, olive oil, chili)  \n",
            "  Ask to confirm the pasta is egg‚Äëfree and that no butter or cheese is added.\n",
            "- **Pizza** ‚Äì Vegetable pizzas that can be made:\n",
            "  - Without cheese, or  \n",
            "  - With vegan cheese (if available at your location)  \n",
            "  Toppings like mushrooms, peppers, onions, olives, artichokes, arugula, etc.\n",
            "- **Sides** ‚Äì Roasted potatoes, grilled vegetables, saut√©ed greens prepared in olive oil instead of butter.\n",
            "- **Custom dishes** ‚Äì The kitchen can often adapt vegetarian items (e.g., hold the cheese, use olive oil instead of butter/cream).\n",
            "\n",
            "If you tell me which BellaVista location you‚Äôre visiting (city or neighborhood), I can be more specific and help you identify clearly vegan dishes and what to request when ordering.\n",
            "--------------------------------------------------\n",
            "\n",
            "üìä Token Usage: {'usage': {'prompt_tokens': 62, 'completion_tokens': 312, 'total_tokens': 374}, 'prompt_tokens': 62, 'completion_tokens': 312, 'total_tokens': 374, 'model': 'gpt-5.1-2025-11-13', 'model_name': 'gpt-5.1-2025-11-13', 'finish_reason': 'stop'}\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# INVOKING LLM WITH CONVERSATION HISTORY\n",
        "# ============================================================================\n",
        "# When you pass a list of messages, the LLM:\n",
        "# 1. Reads the SystemMessage to understand its role\n",
        "# 2. Reviews the conversation history (Human/AI exchanges)\n",
        "# 3. Generates a contextually appropriate response\n",
        "# ============================================================================\n",
        "\n",
        "# The LLM will respond to the last HumanMessage with full context\n",
        "llm_result = llm.invoke(input=messages)\n",
        "\n",
        "print(\"ü§ñ AI Response to 'Do you have vegan options?':\")\n",
        "print(\"-\" * 50)\n",
        "print(llm_result.content)\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nüìä Token Usage: {llm_result.response_metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåç Batch Processing with Conversations\n",
        "\n",
        "You can also batch process **entire conversations** (not just simple prompts). This is useful for:\n",
        "- **Parallel translations** - Same content, different system prompts\n",
        "- **A/B testing prompts** - Compare different phrasings\n",
        "- **Multi-language support** - Translate to multiple languages simultaneously\n",
        "\n",
        "The `.generate()` method is particularly useful here as it provides detailed metadata for each generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç Parallel Translation Results:\n",
            "==================================================\n",
            "üá¨üáß Original: 'Do you have vegan options?'\n",
            "üá©üá™ German:   Haben Sie vegane Optionen?\n",
            "üá™üá∏ Spanish:  ¬øTienen opciones veganas?\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# EXAMPLE: Parallel Translation with Different System Prompts\n",
        "# ============================================================================\n",
        "# Here we translate the same phrase to multiple languages simultaneously\n",
        "# Each inner list is a complete conversation with its own SystemMessage\n",
        "# ============================================================================\n",
        "\n",
        "# Create two separate conversations with different translation instructions\n",
        "batch_messages = [\n",
        "    # Conversation 1: English ‚Üí German\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to German. Only respond with the translation, nothing else.\"),\n",
        "        HumanMessage(content=\"Do you have vegan options?\")\n",
        "    ],\n",
        "    # Conversation 2: English ‚Üí Spanish\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Spanish. Only respond with the translation, nothing else.\"),\n",
        "        HumanMessage(content=\"Do you have vegan options?\")\n",
        "    ],\n",
        "]\n",
        "\n",
        "# Generate responses for both conversations in parallel\n",
        "batch_result = llm.generate(batch_messages)\n",
        "\n",
        "# Display results\n",
        "print(\"üåç Parallel Translation Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üá¨üáß Original: 'Do you have vegan options?'\")\n",
        "print(f\"üá©üá™ German:   {batch_result.generations[0][0].text}\")\n",
        "print(f\"üá™üá∏ Spanish:  {batch_result.generations[1][0].text}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Extracting Data from LLMResult\n",
        "\n",
        "The `.generate()` method returns an `LLMResult` object with rich metadata:\n",
        "- **generations**: List of generated responses (one per input conversation)\n",
        "- **llm_output**: Aggregate info like total token usage\n",
        "- **model_dump()**: Convert to dictionary for easy inspection\n",
        "\n",
        "> **Coming Up:** In later notebooks, we'll explore **Output Parsers** that automatically structure LLM outputs into Python objects!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ LLMResult Structure:\n",
            "----------------------------------------\n",
            "  ‚Ä¢ generations\n",
            "  ‚Ä¢ llm_output\n",
            "  ‚Ä¢ run\n",
            "  ‚Ä¢ type\n",
            "\n",
            "üìä Token Usage Summary:\n",
            "  ‚Ä¢ Prompt tokens:     36\n",
            "  ‚Ä¢ Completion tokens: 16\n",
            "  ‚Ä¢ Total tokens:      52\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# INSPECTING LLMResult STRUCTURE\n",
        "# ============================================================================\n",
        "# The .model_dump() method converts the Pydantic object to a dictionary\n",
        "# This is useful for:\n",
        "# - Debugging and understanding the response structure\n",
        "# - Logging responses to files/databases\n",
        "# - Extracting specific metadata\n",
        "# ============================================================================\n",
        "\n",
        "# Convert to dictionary for inspection\n",
        "result_dict = batch_result.model_dump()\n",
        "\n",
        "# Show the structure (keys at top level)\n",
        "print(\"üì¶ LLMResult Structure:\")\n",
        "print(\"-\" * 40)\n",
        "for key in result_dict.keys():\n",
        "    print(f\"  ‚Ä¢ {key}\")\n",
        "\n",
        "# Show token usage summary\n",
        "if 'generations' in result_dict and result_dict['generations']:\n",
        "    token_usage = result_dict['generations'][0][0]['message']['response_metadata'].get('usage', {})\n",
        "    print(f\"\\nüìä Token Usage Summary:\")\n",
        "    print(f\"  ‚Ä¢ Prompt tokens:     {token_usage.get('prompt_tokens', 'N/A')}\")\n",
        "    print(f\"  ‚Ä¢ Completion tokens: {token_usage.get('completion_tokens', 'N/A')}\")\n",
        "    print(f\"  ‚Ä¢ Total tokens:      {token_usage.get('total_tokens', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Extracted Translations:\n",
            "  German: Haben Sie vegane Optionen?\n",
            "  Spanish: ¬øTienen opciones veganas?\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# EXTRACTING RESPONSES FROM LLMResult\n",
        "# ============================================================================\n",
        "# The generations attribute is a nested list:\n",
        "# - Outer list: One entry per input conversation\n",
        "# - Inner list: Multiple generations if n > 1 (default is 1)\n",
        "# - Each generation has .text for the content\n",
        "# ============================================================================\n",
        "\n",
        "# Extract just the text from each generation\n",
        "translations = [generation[0].text for generation in batch_result.generations]\n",
        "\n",
        "# Display final results\n",
        "print(\"‚úÖ Extracted Translations:\")\n",
        "languages = [\"German\", \"Spanish\"]\n",
        "for lang, translation in zip(languages, translations):\n",
        "    print(f\"  {lang}: {translation}\")\n",
        "\n",
        "# ============================================================================\n",
        "# üìù KEY TAKEAWAYS FROM THIS NOTEBOOK:\n",
        "# ============================================================================\n",
        "# 1. .invoke() - Single prompt/conversation, returns AIMessage\n",
        "# 2. .batch()  - Multiple prompts, returns list of AIMessages\n",
        "# 3. .generate() - Multiple conversations with full metadata (LLMResult)\n",
        "# 4. Message Types: SystemMessage, HumanMessage, AIMessage for conversations\n",
        "# 5. LangChain provides a unified API across different LLM providers\n",
        "# ============================================================================"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
