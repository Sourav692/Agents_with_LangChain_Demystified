{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dddee9be",
      "metadata": {},
      "source": [
        "# 2.3 LLMs vs Chat Models in LangChain\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "This notebook explores the **two types of language model interfaces** in LangChain:\n",
        "\n",
        "1. **LLMs (Text Completion)** - Basic text-in, text-out models\n",
        "2. **Chat Models** - Message-based conversational models\n",
        "\n",
        "## üîë Key Differences\n",
        "\n",
        "| Feature | LLMs | Chat Models |\n",
        "|---------|------|-------------|\n",
        "| **Input** | String | List of Messages |\n",
        "| **Output** | String | AIMessage |\n",
        "| **Examples** | `gpt-3.5-turbo-instruct` | `gpt-4o-mini`, `gemini-1.5-flash` |\n",
        "| **Use Case** | Simple completions | Conversations, complex tasks |\n",
        "\n",
        "## üìö What You'll Learn\n",
        "\n",
        "- Access different LLM providers (OpenAI, Google, HuggingFace)\n",
        "- Understand the LLM vs ChatModel distinction\n",
        "- Use message types for conversational prompting\n",
        "- Build multi-turn conversations with history\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00846b9e",
      "metadata": {},
      "source": [
        "## üì¶ Install Dependencies (Run Once)\n",
        "\n",
        "Uncomment and run the cells below if you haven't installed the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3702f26a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INSTALLATION (Uncomment if needed)\n",
        "# ============================================================================\n",
        "# !pip install -qq langchain==0.3.11\n",
        "# !pip install -qq langchain-openai==0.2.12\n",
        "# !pip install -qq langchain-community==0.3.11\n",
        "# !pip install -qq huggingface_hub==0.30.2\n",
        "# !pip install -qq langchain-core==0.3.63"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88aef59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Don't run if you want to use only chatgpt\n",
        "# This is for accessing open LLMs from huggingface\n",
        "# !pip install -qq transformers==4.46.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89626e28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qq langchain_google_genai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4298ac1b",
      "metadata": {},
      "source": [
        "## üîê Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5838b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOAD API KEYS FROM .env FILE\n",
        "# ============================================================================\n",
        "import os \n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "print(\"‚úÖ Environment variables loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d5106e",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìñ Understanding Model I/O in LangChain\n",
        "\n",
        "In LangChain, the **language model** is the central component. LangChain provides a unified interface to work with different types of models.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "| Type | Definition | Input | Output |\n",
        "|------|------------|-------|--------|\n",
        "| **LLMs** | Pure text completion models | Text string | Text string |\n",
        "| **Chat Models** | Message-based conversational models | List of messages | AIMessage |\n",
        "\n",
        "> **Note:** In practice, Chat Models are preferred for most applications because they support system prompts, conversation history, and structured outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c0e54b",
      "metadata": {},
      "source": [
        "## üè¢ LLM Providers\n",
        "\n",
        "LangChain provides a **unified API** for many LLM providers:\n",
        "\n",
        "| Provider | Commercial | Chat Model Class | LLM Class |\n",
        "|----------|------------|------------------|-----------|\n",
        "| OpenAI | Yes | `ChatOpenAI` | `OpenAI` |\n",
        "| Google | Yes | `ChatGoogleGenerativeAI` | - |\n",
        "| HuggingFace | Open Source | `ChatHuggingFace` | `HuggingFaceEndpoint` |\n",
        "| Anthropic | Yes | `ChatAnthropic` | - |\n",
        "\n",
        "The consistent API means you can **swap providers with minimal code changes**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5001447c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Part 1: Accessing OpenAI Models\n",
        "\n",
        "### Option A: Using OpenAI as an LLM (Text Completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35e24fa",
      "metadata": {},
      "source": [
        "The `OpenAI` class provides access to **text completion models** (like `gpt-3.5-turbo-instruct`). These models:\n",
        "- Take a string prompt\n",
        "- Return a string response\n",
        "- Cannot maintain conversation context naturally\n",
        "\n",
        "> **Note:** This is the older API. Most new applications should use **ChatOpenAI** instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e543206",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# OPENAI LLM (Text Completion Model)\n",
        "# ============================================================================\n",
        "# The OpenAI class uses the older completion API\n",
        "# - model_name: The model to use (instruct models only)\n",
        "# - temperature: 0 = deterministic, 1 = more creative\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Initialize the LLM (text completion model)\n",
        "llm_openai = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "print(f\"‚úÖ OpenAI LLM initialized: gpt-3.5-turbo-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f954b57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# USING THE LLM\n",
        "# ============================================================================\n",
        "# Input: A string prompt\n",
        "# Output: A string response (notice: NOT an AIMessage object)\n",
        "# ============================================================================\n",
        "\n",
        "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
        "response = llm_openai.invoke(prompt)\n",
        "\n",
        "print(\"üìù Prompt:\", prompt)\n",
        "print(\"\\nü§ñ Response (type: string):\")\n",
        "print(response)\n",
        "print(f\"\\nüìä Response type: {type(response)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a41fe0c",
      "metadata": {},
      "source": [
        "### Option B: Using OpenAI as a Chat Model (Recommended)\n",
        "\n",
        "The `ChatOpenAI` class provides access to **chat models** (like `gpt-3.5-turbo`, `gpt-4o-mini`). These models:\n",
        "- Support system prompts to set behavior\n",
        "- Can maintain conversation history\n",
        "- Return structured `AIMessage` objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63dc0d6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# OPENAI CHAT MODEL (Recommended)\n",
        "# ============================================================================\n",
        "# ChatOpenAI uses the chat completions API\n",
        "# - Supports newer models like gpt-3.5-turbo, gpt-4o-mini, gpt-4o\n",
        "# - Returns AIMessage objects with metadata\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the Chat Model\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
        "response = chatgpt.invoke(prompt)\n",
        "\n",
        "print(\"üìù Prompt:\", prompt)\n",
        "print(\"\\nü§ñ Response (type: AIMessage):\")\n",
        "print(response.content)\n",
        "print(f\"\\nüìä Response type: {type(response)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d34c8b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üåê Part 2: Accessing Google Gemini\n",
        "\n",
        "Google's Gemini models are accessed via the `ChatGoogleGenerativeAI` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd382e88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GOOGLE GEMINI CHAT MODEL\n",
        "# ============================================================================\n",
        "# Requires: GOOGLE_API_KEY in your .env file\n",
        "# Available models: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "print(\"‚úÖ Google Gemini initialized: gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbaa184",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same API as ChatOpenAI - LangChain's unified interface!\n",
        "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
        "response = gemini.invoke(prompt)\n",
        "\n",
        "print(\"ü§ñ Gemini Response:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30dac2b6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üè† Part 3: Accessing Local/Open LLMs with HuggingFace\n",
        "\n",
        "HuggingFace offers **500k+ models** including **90k+ open LLMs**. You can access them in two ways:\n",
        "\n",
        "| Method | Class | Pros | Cons |\n",
        "|--------|-------|------|------|\n",
        "| **Local** | `HuggingFacePipeline` | Privacy, no API costs | Requires GPU |\n",
        "| **Remote** | `HuggingFaceEndpoint` | No GPU needed | API rate limits |\n",
        "\n",
        "> **Note:** Local inference requires `transformers` and `pytorch` packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2afec1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HUGGINGFACE LOCAL PIPELINE (Commented - requires GPU)\n",
        "# ============================================================================\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# The code below is commented because it requires significant compute resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4069e15b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gemma_params = {\n",
        "#                   \"do_sample\": False, # greedy decoding - temperature = 0\n",
        "#                   \"return_full_text\": False, # don't return input prompt\n",
        "#                   \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
        "#                 }\n",
        "\n",
        "# local_llm = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"microsoft/Phi-3.5-mini-instruct\",\n",
        "#     task=\"text-generation\",\n",
        "#     pipeline_kwargs=gemma_params,\n",
        "#     # device=0 # when running on Colab selects the GPU, you can change this if you run it on your own instance if needed\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467935d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# local_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a48a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Gemma2B when used locally expects input prompt to be formatted in a specific way\n",
        "# # check more details here: https://huggingface.co/google/gemma-1.1-2b-it#chat-template\n",
        "# gemma_prompt = \"\"\"<bos><start_of_turn>user\\n\"\"\" + prompt + \"\"\"\\n<end_of_turn>\n",
        "# <start_of_turn>model\n",
        "# \"\"\"\n",
        "# print(gemma_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f4e8d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# response = local_llm.invoke(gemma_prompt)\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf40c0b3",
      "metadata": {},
      "source": [
        "### Using HuggingFace Endpoint (Remote Inference)\n",
        "\n",
        "For users without GPUs, `HuggingFaceEndpoint` provides remote inference through HuggingFace's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ea4c17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HUGGINGFACE ENDPOINT (Remote Inference)\n",
        "# ============================================================================\n",
        "# Access HuggingFace models without local GPU\n",
        "# Requires: HUGGINGFACEHUB_API_TOKEN in your .env file\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "repo_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "phi3_params = {\n",
        "    \"wait_for_model\": True,      # Wait if model is loading\n",
        "    \"do_sample\": False,          # Greedy decoding (temperature = 0)\n",
        "    \"return_full_text\": False,   # Don't echo the input prompt\n",
        "    \"max_new_tokens\": 1000,      # Max tokens in response\n",
        "}\n",
        "\n",
        "# Note: Uncomment and add your token to use\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#     repo_id=repo_id,\n",
        "#     temperature=0.5,\n",
        "#     huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
        "#     **phi3_params\n",
        "# )\n",
        "print(f\"üìã HuggingFace Endpoint configured for: {repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f5b126",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# WRAPPING AS A CHAT MODEL\n",
        "# ============================================================================\n",
        "# ChatHuggingFace wraps a HuggingFace model to use the Chat Model interface\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "# Uncomment if you have the llm variable defined above:\n",
        "# chat_gemma = ChatHuggingFace(llm=llm, model_id='google/gemma-1.1-2b-it')\n",
        "print(\"üí° ChatHuggingFace wraps any HuggingFace model as a Chat Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bc385d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DISPLAY RESPONSE (from earlier Gemini call)\n",
        "# ============================================================================\n",
        "print(\"ü§ñ Previous Gemini response:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8055076",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üí¨ Part 4: Message Types for Conversational Prompting\n",
        "\n",
        "Chat Models process **lists of messages**, where each message has a **role** and **content**.\n",
        "\n",
        "### Message Types in LangChain\n",
        "\n",
        "| Message Type | Role | Purpose | Example |\n",
        "|--------------|------|---------|---------|\n",
        "| `SystemMessage` | System | Set AI behavior | \"You are a helpful assistant\" |\n",
        "| `HumanMessage` | User | User input | \"What is AI?\" |\n",
        "| `AIMessage` | Assistant | AI response | \"AI stands for...\" |\n",
        "\n",
        "### Message Properties\n",
        "\n",
        "- **content**: The actual text (string or list for multi-modal)\n",
        "- **additional_kwargs**: Extra info like `tool_calls` for function calling\n",
        "- **response_metadata**: Metadata about the response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73efbb5b",
      "metadata": {},
      "source": [
        "### Example: Building a Conversation with ChatGPT\n",
        "\n",
        "Let's build a multi-turn conversation using message types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034576fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INITIALIZE CHAT MODEL FOR CONVERSATION\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "print(f\"‚úÖ Chat model ready: gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de477ec6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 1: Dictionary Format (OpenAI-style)\n",
        "# ============================================================================\n",
        "# You can pass messages as dictionaries with \"role\" and \"content\" keys\n",
        "# This is familiar if you've used the OpenAI API directly\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "prompt = \"\"\"Can you explain what is Generative AI in 3 bullet points?\"\"\"\n",
        "sys_prompt = \"\"\"Act as a helpful assistant and give meaningful examples in your responses.\"\"\"\n",
        "\n",
        "# Dictionary format (OpenAI-compatible)\n",
        "message = [\n",
        "    {\"role\": \"system\", \"content\": sys_prompt},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "response = chatgpt.invoke(message)\n",
        "print(\"üìù Using dictionary format for messages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80d8d61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the message structure\n",
        "print(\"üìã Message structure (dictionary format):\")\n",
        "for i, msg in enumerate(message):\n",
        "    print(f\"  [{i}] {msg['role']}: {msg['content'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd7ce40",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ü§ñ AI Response:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cfb3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the full AIMessage object with metadata\n",
        "print(\"üìä Full AIMessage object:\")\n",
        "print(f\"  Type: {type(response)}\")\n",
        "print(f\"  Content length: {len(response.content)} chars\")\n",
        "print(f\"  Token usage: {response.usage_metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6f8b46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 2: LangChain Message Objects (Recommended)\n",
        "# ============================================================================\n",
        "# Using Message classes provides better type safety and IDE support\n",
        "# ============================================================================\n",
        "\n",
        "message = [\n",
        "    SystemMessage(content=sys_prompt),\n",
        "    HumanMessage(content=prompt)\n",
        "]\n",
        "\n",
        "response = chatgpt.invoke(message)\n",
        "print(\"üìù Using LangChain Message objects:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74c542e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View message objects\n",
        "print(\"üìã Message objects:\")\n",
        "for i, msg in enumerate(message):\n",
        "    print(f\"  [{i}] {type(msg).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f56f75",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BUILDING CONVERSATION HISTORY\n",
        "# ============================================================================\n",
        "# To maintain context, append the AI response and new user message to the list\n",
        "# This is how you build multi-turn conversations!\n",
        "# ============================================================================\n",
        "\n",
        "# Add the AI's response to the conversation history\n",
        "message.append(response)\n",
        "\n",
        "# Add a new user question\n",
        "follow_up = \"\"\"What did we discuss so far?\"\"\"\n",
        "message.append(HumanMessage(content=follow_up))\n",
        "\n",
        "print(\"üìã Conversation history:\")\n",
        "for i, msg in enumerate(message):\n",
        "    role = type(msg).__name__.replace(\"Message\", \"\")\n",
        "    content_preview = msg.content[:40] + \"...\" if len(msg.content) > 40 else msg.content\n",
        "    print(f\"  [{i}] {role}: {content_preview}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53949a5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONTINUE THE CONVERSATION\n",
        "# ============================================================================\n",
        "# The model now has full context of the conversation!\n",
        "# ============================================================================\n",
        "\n",
        "response = chatgpt.invoke(message)\n",
        "\n",
        "print(\"ü§ñ AI Response (with conversation context):\")\n",
        "print(response.content)\n",
        "\n",
        "# ============================================================================\n",
        "# üìù KEY TAKEAWAYS FROM THIS NOTEBOOK:\n",
        "# ============================================================================\n",
        "# 1. LLMs: Text completion models (string in ‚Üí string out)\n",
        "# 2. Chat Models: Message-based models (messages in ‚Üí AIMessage out)\n",
        "# 3. Chat Models are preferred for most applications\n",
        "# 4. LangChain provides a unified API across providers (OpenAI, Google, HuggingFace)\n",
        "# 5. Message types: SystemMessage, HumanMessage, AIMessage\n",
        "# 6. Build conversations by appending messages to a list\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d7b51f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
